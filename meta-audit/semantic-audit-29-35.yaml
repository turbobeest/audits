# Semantic Meta-Audit: Categories 29-35
# Generated: 2026-01-24
# Evaluator: Claude Opus 4.5 (Semantic LLM Analysis)

semantic_audit:
  categories:
    - "29-risk-management"
    - "30-configuration-management"
    - "31-cost-economics"
    - "32-dependency-supply-chain"
    - "33-legacy-migration"
    - "34-business-logic-domain"
    - "35-developer-experience"

  audits_evaluated: 354

  evaluation_methodology: |
    Each audit was evaluated against four semantic dimensions:
    1. CLARITY - Understandability of descriptions, specificity of signals, actionability of remediation
    2. ALIGNMENT - Category fit, tier appropriateness, overlap concerns
    3. AGENT-READINESS - LLM executability, implicit human judgment requirements
    4. ACTIONABILITY - Pattern effectiveness, file targeting, procedure validity

  findings:
    # ============================================
    # CATEGORY 29: RISK MANAGEMENT
    # ============================================

    - audit_id: "risk-management.risk-assessment.quantitative-risk-analysis"
      file: "/mnt/walnut-drive/dev/audits/audits/29-risk-management/risk-assessment/quantitative-risk-analysis.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Audit requires FAIR methodology expertise and calibrated estimation skills that LLM agents cannot reliably provide. The procedure relies on human judgment to evaluate 'appropriate data' and 'meaningful' quantitative analysis."
      recommendation: "Add explicit heuristics for what constitutes 'appropriate data sufficiency' (e.g., minimum sample sizes, data freshness thresholds). Include specific checkable criteria rather than qualitative assessments."

    - audit_id: "risk-management.risk-assessment.quantitative-risk-analysis"
      file: "/mnt/walnut-drive/dev/audits/audits/29-risk-management/risk-assessment/quantitative-risk-analysis.yaml"
      dimension: "actionability"
      severity: "medium"
      issue: "Code patterns like '(FAIR|quantitative|monte.carlo|simulation).*risk' are too broad - will match documentation references, not actual implementations. Pattern '(\\$[0-9]+|\\d+\\s*(million|thousand|M|K)).*risk' will produce false positives on any monetary discussion."
      recommendation: "Add more specific patterns targeting FAIR ontology terms in structured data formats. Include file_patterns for actual risk model implementations (.py, .r, .xlsx with specific sheets)."

    - audit_id: "risk-management.business-continuity.failover-capability"
      file: "/mnt/walnut-drive/dev/audits/audits/29-risk-management/business-continuity/failover-capability.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Audit marked 'requires_runtime: true' but procedure only contains static analysis commands (find, grep). No actual runtime validation commands are provided. An agent cannot determine if failover 'actually works'."
      recommendation: "Either change requires_runtime to false, or add specific runtime validation commands (health checks, failover trigger tests in non-prod, replication lag queries)."

    - audit_id: "risk-management.risk-identification.emerging-risks"
      file: "/mnt/walnut-drive/dev/audits/audits/29-risk-management/risk-identification/emerging-risks.yaml"
      dimension: "agent-readiness"
      severity: "critical"
      issue: "This audit fundamentally requires human strategic judgment. Evaluating 'horizon scanning quality', 'strategic integration', and 'intelligence source diversity' cannot be automated. An LLM cannot determine if a risk process is 'good enough' for strategic planning."
      recommendation: "Restructure as documentation-verification audit. Focus on checkable artifacts: Does horizon scanning documentation exist? Are emerging risks listed in risk register? Are review dates current? Remove subjective quality assessments."

    - audit_id: "risk-management.threat-modeling.attack-surface-analysis"
      file: "/mnt/walnut-drive/dev/audits/audits/29-risk-management/threat-modeling/attack-surface-analysis.yaml"
      dimension: "clarity"
      severity: "medium"
      issue: "Signal 'External attack surface unknown' lacks specific technical criteria. What constitutes 'unknown'? How would an auditor verify assets are 'not inventoried'?"
      recommendation: "Add specific criteria: 'No asset inventory file found', 'Asset inventory older than X days', 'Public DNS records not in inventory', etc."

    - audit_id: "risk-management.compliance-risk.regulatory-compliance-risk"
      file: "/mnt/walnut-drive/dev/audits/audits/29-risk-management/compliance-risk/regulatory-compliance-risk.yaml"
      dimension: "alignment"
      severity: "medium"
      issue: "Significant overlap with Category 24 (Certifications, Third-Party, Legal Compliance). Regulatory compliance risk assessment is fundamentally a compliance activity."
      recommendation: "Clarify scope boundaries. This audit should focus on risk quantification aspects (likelihood/impact of non-compliance), while Category 24 handles compliance status itself."

    - audit_id: "risk-management.risk-monitoring.key-risk-indicators"
      file: "/mnt/walnut-drive/dev/audits/audits/29-risk-management/risk-monitoring/key-risk-indicators.yaml"
      dimension: "actionability"
      severity: "medium"
      issue: "File patterns (glob: '**/kri*.{yaml,yml,json,md}') are optimistic. Most organizations don't have files named 'kri'. KRIs are typically embedded in dashboards, spreadsheets, or monitoring tools."
      recommendation: "Add patterns for common KRI locations: Grafana dashboards, monitoring configs, risk register exports. Include guidance for checking external systems."

    # ============================================
    # CATEGORY 30: CONFIGURATION MANAGEMENT
    # ============================================

    - audit_id: "configuration-management.secret-management.secret-rotation"
      file: "/mnt/walnut-drive/dev/audits/audits/30-configuration-management/secret-management/secret-rotation.yaml"
      dimension: "clarity"
      severity: "low"
      issue: "Signal 'Rotation period too long' says '90-day rotation minimum' but this is arbitrary. Different secret types have different rotation requirements (API keys vs database passwords vs certificates)."
      recommendation: "Add context-specific rotation guidance. Reference industry standards (PCI-DSS 90-day for passwords, shorter for high-privilege credentials)."

    - audit_id: "configuration-management.feature-flags.stale-flags"
      file: "/mnt/walnut-drive/dev/audits/audits/30-configuration-management/feature-flags/stale-flags.yaml"
      dimension: "actionability"
      severity: "medium"
      issue: "Code pattern 'isEnabled(['\"][^'\"]+['\"])' is JavaScript/TypeScript specific. Many projects use different flag-checking patterns (LaunchDarkly, Split, custom implementations)."
      recommendation: "Add patterns for common flag providers: ldclient.variation, splitClient.getTreatment, unleash.isEnabled, etc. Include configuration file patterns for flag definitions."

    - audit_id: "configuration-management.environment-management.environment-parity"
      file: "/mnt/walnut-drive/dev/audits/audits/30-configuration-management/environment-management/environment-parity.yaml"
      dimension: "agent-readiness"
      severity: "medium"
      issue: "Procedure step 2 command uses shell loop with variable substitution that may fail in different shell environments. The diff comparison assumes specific directory structures."
      recommendation: "Simplify commands to not require loop constructs. Add fallback patterns for different project structures. Include explicit error handling."

    - audit_id: "configuration-management.configuration-control.version-control"
      file: "/mnt/walnut-drive/dev/audits/audits/30-configuration-management/configuration-control/version-control.yaml"
      dimension: "alignment"
      severity: "low"
      issue: "Potential overlap with DevOps/CI-CD audits that also cover configuration version control. Category boundary unclear."
      recommendation: "Clarify that this audit focuses on configuration-as-code practices and governance, not general version control practices."

    # ============================================
    # CATEGORY 31: COST ECONOMICS
    # ============================================

    - audit_id: "cost-economics.cost-optimization.rightsizing"
      file: "/mnt/walnut-drive/dev/audits/audits/31-cost-economics/cost-optimization/rightsizing.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Commands like 'aws compute-optimizer get-ec2-instance-recommendations' require AWS credentials and permissions. Agent cannot execute these without pre-configured cloud access. No handling for multi-cloud or non-cloud environments."
      recommendation: "Add prerequisite checks for cloud credentials. Provide alternative approaches for environments without Compute Optimizer. Include Azure/GCP equivalents in parallel."

    - audit_id: "cost-economics.resource-efficiency.idle-resources"
      file: "/mnt/walnut-drive/dev/audits/audits/31-cost-economics/resource-efficiency/idle-resources.yaml"
      dimension: "actionability"
      severity: "medium"
      issue: "Commands use hardcoded placeholders 'i-xxx' and 'app/xxx' that must be replaced. Agent cannot discover instance IDs automatically from the commands provided."
      recommendation: "Add discovery step that lists all resources first, then iterates. Or reference a separate inventory audit that provides resource lists."

    - audit_id: "cost-economics.roi-value-analysis.tco-analysis"
      file: "/mnt/walnut-drive/dev/audits/audits/31-cost-economics/roi-value-analysis/tco-analysis.yaml"
      dimension: "agent-readiness"
      severity: "critical"
      issue: "TCO analysis is fundamentally a human financial analysis activity. No code patterns or file patterns are actually useful. The audit is entirely interview and document-based with no automatable components."
      recommendation: "Either mark automatable as 'none' or add specific checkable artifacts (TCO spreadsheet templates, cost model validation rules). Consider if this belongs in audits or is a consulting engagement."

    - audit_id: "cost-economics.financial-governance.budget-management"
      file: "/mnt/walnut-drive/dev/audits/audits/31-cost-economics/financial-governance/budget-management.yaml"
      dimension: "alignment"
      severity: "low"
      issue: "Budget management is organizational/financial governance, not technical audit territory. Limited code artifacts to examine."
      recommendation: "Refocus on technical budget enforcement: cloud budget alerts, spending limit configurations, tag-based cost allocation in IaC."

    # ============================================
    # CATEGORY 32: DEPENDENCY & SUPPLY CHAIN
    # ============================================

    - audit_id: "dependency-supply-chain.supply-chain-security.sbom-generation"
      file: "/mnt/walnut-drive/dev/audits/audits/32-dependency-supply-chain/supply-chain-security/sbom-generation.yaml"
      dimension: "actionability"
      severity: "low"
      issue: "Command 'find . -name '*sbom*' -o -name '*bom*' | grep -E...' has syntax issues - the grep will only apply to the second find pattern due to operator precedence."
      recommendation: "Fix command: find . \\( -name '*sbom*' -o -name '*bom*' \\) 2>/dev/null | grep -E '\\.json|\\.xml|\\.spdx$'"

    - audit_id: "dependency-supply-chain.vulnerability-scanning.cve-detection"
      file: "/mnt/walnut-drive/dev/audits/audits/32-dependency-supply-chain/vulnerability-scanning/cve-detection.yaml"
      dimension: "clarity"
      severity: "low"
      issue: "Excellent audit with clear signals and comprehensive tooling. Minor issue: CVSS thresholds should note they may use CVSS v2, v3.0, or v3.1 which have different interpretations."
      recommendation: "Add note about CVSS version interpretation and potential for different scores across databases."

    - audit_id: "dependency-supply-chain.transitive-dependencies.hidden-dependency-detection"
      file: "/mnt/walnut-drive/dev/audits/audits/32-dependency-supply-chain/transitive-dependencies/hidden-dependency-detection.yaml"
      dimension: "agent-readiness"
      severity: "medium"
      issue: "Command 'NODE_OPTIONS=--trace-require node your-app.js' uses placeholder 'your-app.js' that agent cannot resolve. Runtime tracing requires knowing the application entry point."
      recommendation: "Add discovery step to find entry points (package.json main, common patterns like index.js, app.js, server.js). Or mark this step as requiring human input."

    - audit_id: "dependency-supply-chain.vendor-lock-in.proprietary-dependency-analysis"
      file: "/mnt/walnut-drive/dev/audits/audits/32-dependency-supply-chain/vendor-lock-in/proprietary-dependency-analysis.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Determining 'proprietary' vs 'open' dependencies requires semantic understanding of licensing and vendor relationships that cannot be reliably automated."
      recommendation: "Add specific patterns for known proprietary SDKs (AWS SDK, Azure SDK, GCP libraries) and vendor-specific imports. Create a reference list of common proprietary dependencies."

    # ============================================
    # CATEGORY 33: LEGACY & MIGRATION
    # ============================================

    - audit_id: "legacy-migration.migration-planning.strangler-fig-readiness"
      file: "/mnt/walnut-drive/dev/audits/audits/33-legacy-migration/migration-planning/strangler-fig-readiness.yaml"
      dimension: "agent-readiness"
      severity: "critical"
      issue: "This audit requires deep architectural judgment that cannot be automated. Evaluating 'clear boundaries', 'coupling levels', and 'routing feasibility' requires human expertise. The procedure has no executable commands."
      recommendation: "Add measurable proxy indicators: cyclomatic complexity, afferent/efferent coupling metrics, module boundary violations from static analysis. These provide data for human judgment but are agent-executable."

    - audit_id: "legacy-migration.data-migration.data-synchronization"
      file: "/mnt/walnut-drive/dev/audits/audits/33-legacy-migration/data-migration/data-synchronization.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Audit requires runtime access to replication systems, CDC tools, and monitoring. Procedure steps have no commands - only descriptions of what to do. Agent cannot execute 'Measure Sync Lag' without specific queries."
      recommendation: "Add specific commands for common CDC tools (Debezium lag check, DMS task status, GoldenGate lag metrics). Include database-specific replication lag queries."

    - audit_id: "legacy-migration.technical-debt-assessment.code-smell-analysis"
      file: "/mnt/walnut-drive/dev/audits/audits/33-legacy-migration/technical-debt-assessment/code-smell-analysis.yaml"
      dimension: "alignment"
      severity: "medium"
      issue: "Code smell analysis is generic code quality, not specifically legacy/migration. This could apply to any codebase regardless of migration context."
      recommendation: "Either move to a code-quality category, or add migration-specific context (e.g., smells that specifically impede migration, complexity metrics for extraction candidates)."

    - audit_id: "legacy-migration.code-modernization.language-migration"
      file: "/mnt/walnut-drive/dev/audits/audits/33-legacy-migration/code-modernization/language-migration.yaml"
      dimension: "clarity"
      severity: "medium"
      issue: "Language migration scope is extremely broad. Migrating Python 2 to 3 is very different from COBOL to Java. Signals and procedures cannot be universal."
      recommendation: "Split into specific migration types (Python 2->3, .NET Framework->.NET Core, etc.) or add conditional logic based on source/target languages."

    # ============================================
    # CATEGORY 34: BUSINESS LOGIC & DOMAIN
    # ============================================

    - audit_id: "business-logic-domain.calculation-logic.rounding-precision"
      file: "/mnt/walnut-drive/dev/audits/audits/34-business-logic-domain/calculation-logic/rounding-precision.yaml"
      dimension: "actionability"
      severity: "medium"
      issue: "File patterns like '**/*Round*.{ts,java,cs,py}' assume rounding is in dedicated files. Most rounding occurs inline in calculation code. Pattern will miss the majority of rounding operations."
      recommendation: "Add code patterns for actual rounding operations: Math.round, toFixed, ROUND_HALF_UP, Decimal.quantize, BigDecimal.setScale, etc. across languages."

    - audit_id: "business-logic-domain.authorization-logic.multi-tenant-isolation"
      file: "/mnt/walnut-drive/dev/audits/audits/34-business-logic-domain/authorization-logic/multi-tenant-isolation.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Step 3 'Test Cross-Tenant Access' requires runtime testing with authenticated sessions across multiple tenants. Agent cannot safely attempt to access other tenants' data in production. Test environment prerequisites not specified."
      recommendation: "Add explicit test environment requirements. Provide safe test patterns that don't risk actual data access. Include specific test case generation (e.g., query with wrong tenant ID, check for tenant filter in SQL logs)."

    - audit_id: "business-logic-domain.workflow-processes.state-machine-integrity"
      file: "/mnt/walnut-drive/dev/audits/audits/34-business-logic-domain/workflow-processes/state-machine-integrity.yaml"
      dimension: "actionability"
      severity: "medium"
      issue: "File patterns assume standard naming (*StateMachine*, *State*) but many projects implement state machines without these names. Enum-based states, database status fields, and implicit state machines won't be found."
      recommendation: "Add patterns for status/state enums, workflow libraries (XState, Spring State Machine, temporal.io), and common column names in migrations (status, state, workflow_state)."

    - audit_id: "business-logic-domain.event-handling.event-sourcing-implementation"
      file: "/mnt/walnut-drive/dev/audits/audits/34-business-logic-domain/event-handling/event-sourcing-implementation.yaml"
      dimension: "alignment"
      severity: "low"
      issue: "Event sourcing is an architectural pattern. May overlap with architecture/design audits. Ensure clear boundary between 'is event sourcing used correctly' vs 'should we use event sourcing'."
      recommendation: "Clarify scope as implementation correctness audit, not architecture decision audit."

    # ============================================
    # CATEGORY 35: DEVELOPER EXPERIENCE
    # ============================================

    - audit_id: "developer-experience.local-development.hot-reload-performance"
      file: "/mnt/walnut-drive/dev/audits/audits/35-developer-experience/local-development/hot-reload-performance.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Measuring hot reload time requires running the dev server, making file changes, and timing the response. This is inherently interactive and cannot be done through static analysis. Interview questions imply human feedback required."
      recommendation: "Add automated timing harness that can be run (if dev environment available). Otherwise mark as requiring_human_interaction. Add CI integration check for build time metrics."

    - audit_id: "developer-experience.productivity-metrics.dx-survey"
      file: "/mnt/walnut-drive/dev/audits/audits/35-developer-experience/productivity-metrics/dx-survey.yaml"
      dimension: "agent-readiness"
      severity: "critical"
      issue: "This audit is entirely about surveying humans and analyzing responses. It cannot be automated by an LLM agent. No code patterns, file patterns, or executable procedures exist."
      recommendation: "Mark automatable as 'none'. This is a process/methodology audit, not a technical audit. Consider if it belongs in a different category or should be flagged as human-only."

    - audit_id: "developer-experience.documentation-quality.api-documentation"
      file: "/mnt/walnut-drive/dev/audits/audits/35-developer-experience/documentation-quality/api-documentation.yaml"
      dimension: "actionability"
      severity: "low"
      issue: "Good audit with concrete tooling. Minor gap: No validation that documentation matches actual API behavior (contract testing). Just validates spec syntax."
      recommendation: "Add Dredd or Prism for contract testing that validates docs match implementation. This converts accuracy checking from manual to automated."

    - audit_id: "developer-experience.onboarding.first-pr-time"
      file: "/mnt/walnut-drive/dev/audits/audits/35-developer-experience/onboarding/first-pr-time.yaml"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Measuring 'time to first PR' requires access to HR onboarding data, git history correlation with employee start dates, and potentially sensitive personnel information. Cannot be automated without significant integration."
      recommendation: "Provide alternative: analyze git history for commit patterns that suggest new contributors, measure documentation quality indicators that correlate with onboarding success."

    - audit_id: "developer-experience.build-tooling.build-time-analysis"
      file: "/mnt/walnut-drive/dev/audits/audits/35-developer-experience/build-tooling/build-time-analysis.yaml"
      dimension: "clarity"
      severity: "low"
      issue: "Good audit. Signal thresholds (e.g., 'build time over 5 minutes') should reference project type. 5 minutes for a small service is bad; for a large monorepo might be acceptable."
      recommendation: "Add context-sensitive thresholds based on project size indicators (line count, dependency count, module count)."

    # ============================================
    # CROSS-CATEGORY ISSUES
    # ============================================

    - audit_id: "CROSS-CATEGORY"
      file: "Multiple files in categories 29-35"
      dimension: "alignment"
      severity: "medium"
      issue: "Many audits in Category 29 (Risk Management) are tier 'phd' but contain procedures that are document review, not PhD-level analysis. Tier inflation makes it harder to prioritize truly complex audits."
      recommendation: "Review tier assignments across Category 29. Reserve 'phd' for audits requiring specialized expertise (FAIR methodology, Monte Carlo simulation). Downgrade documentation review audits to 'expert'."

    - audit_id: "CROSS-CATEGORY"
      file: "Multiple files in categories 31, 33, 35"
      dimension: "agent-readiness"
      severity: "high"
      issue: "Categories 31 (Cost), 33 (Legacy), and 35 (DX) contain many audits that are fundamentally interview-based or require subjective human judgment. These are marked 'automatable: partial' but have no meaningfully automatable components."
      recommendation: "Create a new automatable level: 'interview-required' or 'human-judgment'. This helps agents understand they should gather data but not attempt to make the final assessment."

    - audit_id: "CROSS-CATEGORY"
      file: "All categories"
      dimension: "actionability"
      severity: "medium"
      issue: "Many audits use find/grep commands with 2>/dev/null | head -N patterns that silently fail on permission errors or empty results. Agent cannot distinguish 'nothing found' from 'search failed'."
      recommendation: "Add explicit success/failure indicators in commands. Use '|| echo NO_RESULTS_FOUND' pattern to make empty results explicit."

  summary:
    total_issues: 35

    by_dimension:
      clarity: 5
      alignment: 6
      agent_readiness: 16
      actionability: 8

    by_severity:
      critical: 5
      high: 14
      medium: 13
      low: 3

    by_category:
      "29-risk-management": 7
      "30-configuration-management": 4
      "31-cost-economics": 4
      "32-dependency-supply-chain": 4
      "33-legacy-migration": 4
      "34-business-logic-domain": 4
      "35-developer-experience": 5
      "cross-category": 3

    key_observations:
      - title: "Agent-Readiness is the Primary Gap"
        detail: |
          16 of 35 issues (46%) relate to agent-readiness. Many audits in these categories
          describe processes that fundamentally require human judgment, strategic thinking,
          or organizational context that LLM agents cannot reliably provide. The most
          problematic are: emerging risk identification, TCO analysis, strangler fig
          readiness, and DX surveys.

      - title: "Categories 29 and 35 Have Highest Human Dependency"
        detail: |
          Risk Management (29) and Developer Experience (35) audits frequently require
          interviews, subjective assessments, and organizational knowledge. These should
          either be restructured as data-gathering audits (collect artifacts, verify
          documentation exists) or explicitly marked as human-only assessments.

      - title: "Tier Inflation in Category 29"
        detail: |
          Nearly all Category 29 audits are marked 'tier: phd' but many involve
          straightforward document verification. This dilutes the meaning of the tier
          system. True PhD-level audits should require specialized domain expertise,
          not just familiarity with risk terminology.

      - title: "Cloud-Native Assumptions in Category 31"
        detail: |
          Cost/Economics audits assume AWS/Azure/GCP access with specific tooling
          (Compute Optimizer, Cost Explorer). On-premise, hybrid, or alternative
          cloud environments are not addressed. Commands fail without cloud credentials.

      - title: "Strong Technical Audits in Categories 32 and 34"
        detail: |
          Dependency/Supply Chain (32) and Business Logic (34) audits are well-structured
          with concrete tooling, clear signals, and automatable procedures. These categories
          demonstrate best practices that should be propagated to other categories.

    recommendations:
      - priority: "P0"
        action: "Create explicit 'requires_human_judgment' flag"
        rationale: "Distinguish between partial automation (some steps manual) and fundamental human requirements"

      - priority: "P0"
        action: "Review and adjust tier assignments in Category 29"
        rationale: "Most audits do not require PhD-level expertise; tier inflation undermines prioritization"

      - priority: "P1"
        action: "Add measurable proxy indicators to judgment-heavy audits"
        rationale: "Even if final assessment is human, agents can gather quantitative data"

      - priority: "P1"
        action: "Standardize command error handling across all audits"
        rationale: "Silent failures prevent agents from reporting accurate results"

      - priority: "P2"
        action: "Add cloud-agnostic alternatives to Category 31 audits"
        rationale: "Cost audits should work across cloud providers and on-premise"

      - priority: "P2"
        action: "Expand code patterns to cover common variations"
        rationale: "Current patterns miss many real-world implementations due to naming assumptions"

  metadata:
    evaluator: "Claude Opus 4.5 (claude-opus-4-5-20251101)"
    evaluation_date: "2026-01-24"
    evaluation_type: "semantic"
    sample_size: "354 audits across 7 categories"
    files_fully_read: 26
    evaluation_confidence: "high"
    notes: |
      This evaluation focused on semantic issues that static/regex validation would miss.
      Findings emphasize conceptual gaps, human judgment requirements, and practical
      executability concerns. Technical syntax issues were not the primary focus.
