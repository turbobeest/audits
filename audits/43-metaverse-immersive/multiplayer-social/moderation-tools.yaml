# ============================================================
# AUDIT: Moderation Tools
# Category: 43 - Metaverse & Immersive
# Subcategory: multiplayer-social
# ============================================================

audit:
  id: "metaverse-immersive.multiplayer-social.moderation-tools"
  name: "Moderation Tools Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "metaverse-immersive"
  category_number: 43
  subcategory: "multiplayer-social"

  tier: "expert"
  estimated_duration: "3-5 hours"  # median: 4h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "xr-systems"

  default_profiles:
    - "full"

  blocks_phase: true
  parallelizable: true

description:
  what: |
    Evaluates moderation tools and capabilities for social VR including
    reporting systems, moderator tools, automated detection, and response
    mechanisms for handling violations.

  why_it_matters: |
    Social VR at scale requires moderation. Without tools, platforms become
    toxic. Moderation must handle the unique challenges of VR including
    spatial harassment, voice abuse, and avatar violations.

  when_to_run:
    - "Before public launch"
    - "During safety review"
    - "When scaling user base"
    - "After moderation failures"

prerequisites:
  required_artifacts:
    - type: "moderation_system"
      description: "Moderation tool implementation"
    - type: "reporting_system"
      description: "User reporting mechanism"

  access_requirements:
    - "Moderator access"
    - "Reporting system access"

discovery:
  file_patterns:
    - glob: "**/moderation/**/*.cs"
      purpose: "Find moderation code"
    - glob: "**/reporting/**/*.cs"
      purpose: "Find reporting code"

  code_patterns:
    - pattern: "Report|Moderate|Ban|Kick"
      type: "keyword"
      scope: "source"
      purpose: "Find moderation systems"

knowledge_sources:
  guides:
    - id: "moderation-best-practices"
      name: "VR Moderation Best Practices"
      url: "https://developer.oculus.com/documentation/unity/meta-moderation/"
      offline_cache: true
      priority: "required"

tooling:
  analysis_tools:
    - tool: "moderation-tester"
      purpose: "Moderation tool testing"

signals:
  critical:
    - id: "MT-CRIT-001"
      signal: "No reporting mechanism"
      evidence_indicators:
        - "Users cannot report violations"
        - "No in-VR reporting"
      explanation: |
        Without reporting, problems cannot be addressed.
        Users need easy access to reporting.
      remediation: "Implement in-VR reporting"

    - id: "MT-CRIT-002"
      signal: "No moderator tools"
      evidence_indicators:
        - "Cannot remove violators"
        - "No moderation capability"
      explanation: |
        Without tools, moderators cannot address issues.
        Moderation infrastructure is required.
      remediation: "Implement moderator tools"

  high:
    - id: "MT-HIGH-001"
      signal: "Reporting difficult in VR"
      evidence_indicators:
        - "Must leave VR to report"
        - "Complex reporting process"
      remediation: "Simplify in-VR reporting"

    - id: "MT-HIGH-002"
      signal: "No evidence capture"
      evidence_indicators:
        - "No recording capability"
        - "No context for reports"
      remediation: "Add evidence capture for reports"

  medium:
    - id: "MT-MED-001"
      signal: "No automated detection"
      remediation: "Consider automated abuse detection"

    - id: "MT-MED-002"
      signal: "Slow moderation response"
      remediation: "Improve response time"

  low:
    - id: "MT-LOW-001"
      signal: "Moderation policies undocumented"
      remediation: "Document moderation policies"

  positive:
    - id: "MT-POS-001"
      signal: "Easy in-VR reporting"
    - id: "MT-POS-002"
      signal: "Effective moderator tools"
    - id: "MT-POS-003"
      signal: "Automated detection"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Test Reporting"
      description: "Verify reporting mechanism"
      duration_estimate: "45 min"

    - id: "2"
      name: "Test Moderator Tools"
      description: "Verify moderator capabilities"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Test Response Flow"
      description: "Verify end-to-end process"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Test Evidence"
      description: "Check evidence capture"
      duration_estimate: "45 min"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "moderation_report"
      format: "prose"
      description: "Moderation assessment"

closeout_checklist:
  - id: "mt-001"
    item: "Reporting available"
    level: "CRITICAL"
    verification: "manual"

  - id: "mt-002"
    item: "Moderator tools functional"
    level: "CRITICAL"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["social-vr", "metaverse"]

  compliance_frameworks:
    - framework: "DSA"
      controls: ["Content Moderation"]

relationships:
  commonly_combined:
    - "metaverse-immersive.multiplayer-social.social-boundaries"
    - "metaverse-immersive.multiplayer-social.avatar-system"
