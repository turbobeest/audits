audit:
  id: signal-processing-data-acquisition.algorithm-verification.reference-implementation-comparison
  name: Reference Implementation Comparison Audit
  version: 1.0.0
  last_updated: '2026-01-22'
  status: active
  category: signal-processing-data-acquisition
  category_number: 40
  subcategory: algorithm-verification
  tier: expert
  estimated_duration: 2-3 hours  # median: 2h
  completeness: complete
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: software
  default_profiles:
  - full
  - signal-processing
  blocks_phase: false
  parallelizable: true
description:
  what: |
    This audit evaluates algorithm correctness by comparison with
    validated reference implementations. It examines:
    - Selection of appropriate reference implementation
    - Comparison methodology and tolerance
    - Test vector coverage and diversity
    - Statistical comparison for stochastic algorithms
    - Documentation of comparison results
    - Handling of intentional deviations
  why_it_matters: |
    Reference comparison validates implementation correctness:
    - Reference implementations are validated and trusted
    - Comparison reveals subtle implementation errors
    - Standard libraries provide reliable references
    - Numerical differences should be within tolerance
    - Intentional differences must be documented

    Reference comparison is the most reliable way to validate
    complex signal processing algorithm implementations.
  when_to_run:
  - New algorithm implementation
  - Algorithm porting
  - Optimization verification
  - Regression testing
prerequisites:
  required_artifacts:
  - type: source_code
    description: Algorithm implementation to verify
  - type: reference
    description: Validated reference implementation
  - type: test_data
    description: Test vectors for comparison
  access_requirements:
  - Access to implementation code
  - Access to reference library
  - Test infrastructure
discovery:
  code_patterns:
  - pattern: (reference|golden|expected|baseline)
    type: regex
    scope: source
    purpose: Detect reference comparison
  - pattern: (compare|verify|validate).*result
    type: regex
    scope: source
    purpose: Detect validation
  - pattern: (tolerance|epsilon|threshold).*error
    type: regex
    scope: source
    purpose: Detect tolerance handling
  file_patterns:
  - glob: '**/test*.{c,cpp,py}'
    purpose: Test files
  - glob: '**/verify*.{c,cpp,py}'
    purpose: Verification files
  - glob: '**/reference*.{py,m}'
    purpose: Reference implementation
signals:
  critical:
  - id: REF-CRIT-001
    signal: Results differ from reference beyond tolerance
    evidence_indicators:
    - Output differs from reference implementation
    - Error exceeds numerical tolerance
    - Test vectors fail comparison
    explanation: |
      Significant difference from a validated reference indicates
      an implementation error. Even small differences may indicate
      fundamental problems.
    remediation: |
      - Investigate source of difference
      - Verify reference implementation is correct
      - Check for numerical precision issues
      - Debug step by step against reference
    cwe: CWE-682
  high:
  - id: REF-HIGH-001
    signal: No reference comparison performed
    evidence_indicators:
    - Implementation not compared to reference
    - Only ad-hoc testing performed
    - No golden outputs available
    explanation: |
      Without reference comparison, implementation correctness
      cannot be verified. Ad-hoc testing may miss subtle errors.
    remediation: |
      - Identify appropriate reference (NumPy, MATLAB, etc.)
      - Create comparison test framework
      - Generate and save golden outputs
      - Document comparison results
  - id: REF-HIGH-002
    signal: Test vector coverage insufficient
    evidence_indicators:
    - Few test cases
    - Only typical inputs tested
    - Edge cases not compared
    explanation: |
      Limited test coverage may miss errors in untested cases.
      Comparison should cover typical cases, edge cases, and
      stress conditions.
    remediation: |
      - Expand test vector set
      - Include edge cases
      - Test with random inputs
      - Cover all algorithm paths
  medium:
  - id: REF-MED-001
    signal: Tolerance not properly justified
    evidence_indicators:
    - Arbitrary tolerance value
    - Tolerance too loose or too tight
    - Precision analysis not performed
    explanation: |
      Comparison tolerance should match expected numerical
      precision. Too loose masks errors; too tight causes
      false failures.
    remediation: |
      - Analyze expected numerical precision
      - Set tolerance based on precision analysis
      - Document tolerance rationale
      - Verify tolerance catches real errors
  - id: REF-MED-002
    signal: Intentional differences not documented
    evidence_indicators:
    - Known difference from reference
    - Difference not explained
    - Deviation not justified
    explanation: |
      Intentional differences from reference (for optimization,
      extension, etc.) must be documented. Otherwise they
      appear as implementation errors.
    remediation: |
      - Document all intentional differences
      - Explain rationale for deviation
      - Verify deviation doesn't affect correctness
      - Include in comparison documentation
  low:
  - id: REF-LOW-001
    signal: Comparison results not documented
    evidence_indicators:
    - Comparison performed but not recorded
    - Results not in repository
    - No comparison report
    explanation: |
      Undocumented comparison results are not verifiable.
      Results should be recorded for future reference.
    remediation: |
      - Document comparison methodology
      - Record test results
      - Include in verification documentation
      - Maintain comparison tests
  positive:
  - id: REF-POS-001
    signal: Comprehensive reference comparison
    evidence_indicators:
    - Comparison with validated reference
    - Adequate test coverage
    - Results documented
procedure:
  context:
    cognitive_mode: analytical
    ensemble_role: verification-engineer
  steps:
  - id: '1'
    name: Reference Comparison Review
    description: |
      Review reference comparison implementation.
    duration_estimate: 45 min
    commands:
    - purpose: Find comparison code
      command: |
        grep -rniE "(reference|golden|compare|verify)" \
          --include="*.c" --include="*.py" . 2>/dev/null | head -30
    - purpose: Find tolerance handling
      command: |
        grep -rniE "(tolerance|epsilon|threshold)" \
          --include="*.c" --include="*.py" . 2>/dev/null | head -20
    expected_findings:
    - Reference implementation
    - Comparison methodology
    - Tolerance values
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Comparison Analysis
    - Coverage Assessment
    - Recommendations
profiles:
  membership:
    quick:
      included: false
    full:
      included: true
      priority: 2
    signal-processing:
      included: true
      priority: 2
closeout_checklist:
- id: ref-001
  item: Reference comparison performed
  level: CRITICAL
  verification: manual
  verification_notes: Verify comparison with validated reference
  expected: Confirmed by reviewer
- id: ref-002
  item: Results match within tolerance
  level: BLOCKING
  verification: manual
  verification_notes: Verify all test vectors pass comparison
  expected: Confirmed by reviewer
- id: ref-003
  item: Comparison documented
  level: WARNING
  verification: |
    grep -rniE "(reference|golden|comparison)" \
      --include="*.md" --include="*.txt" . 2>/dev/null | \
      wc -l | xargs -I{} test {} -gt 0 && echo "PASS" || echo "FAIL"
  expected: PASS
governance:
  applicable_to:
    archetypes:
    - signal-processing
    - scientific-computing
    - safety-critical
relationships:
  commonly_combined:
  - signal-processing.algorithm-verification.algorithm-correctness
  - signal-processing.algorithm-verification.numerical-precision
  - testing.verification.golden-output-testing

# Glossary of domain-specific terms:
glossary:
  "mu (step size)": "Learning rate parameter in adaptive filters, controls convergence speed vs stability"
  "LMS": "Least Mean Squares - adaptive filter algorithm that minimizes mean squared error"
  "FFT": "Fast Fourier Transform - efficient algorithm to compute discrete Fourier transform"
