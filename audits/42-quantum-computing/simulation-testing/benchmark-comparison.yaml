# ============================================================
# AUDIT: Benchmark Comparison
# Category: 42 - Quantum Computing
# Subcategory: simulation-testing
# ============================================================

audit:
  id: "quantum-computing.simulation-testing.benchmark-comparison"
  name: "Quantum Benchmark Comparison Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "quantum-computing"
  category_number: 42
  subcategory: "simulation-testing"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "medium"
  scope: "quantum-systems"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Compares quantum algorithm performance against established benchmarks
    including standard quantum algorithms, classical baselines, and published
    results. Evaluates solution quality, resource usage, and practical
    quantum advantage.

  why_it_matters: |
    Claims of quantum advantage require rigorous benchmarking. Without
    comparison to classical methods and published results, performance
    claims are unsubstantiated. Benchmarking validates that algorithms
    deliver promised benefits.

  when_to_run:
    - "When claiming quantum advantage"
    - "During algorithm selection"
    - "When publishing results"
    - "During performance optimization"

prerequisites:
  required_artifacts:
    - type: "algorithm_implementation"
      description: "Algorithm to benchmark"
    - type: "benchmark_suite"
      description: "Standard benchmark problems"
    - type: "classical_baselines"
      description: "Classical algorithm results"

  access_requirements:
    - "Access to algorithm code"
    - "Benchmark problem sets"
    - "Classical solver access"

discovery:
  file_patterns:
    - glob: "**/benchmarks/**/*.py"
      purpose: "Find benchmark code"
    - glob: "**/baseline/**/*.py"
      purpose: "Find baseline implementations"

knowledge_sources:
  specifications:
    - id: "qasm-bench"
      name: "QASMBench Benchmark Suite"
      url: "https://github.com/pnnl/QASMBench"
      offline_cache: true
      priority: "recommended"

  papers:
    - id: "quantum-benchmarking"
      title: "Quantum Benchmarking Standards"
      url: "https://arxiv.org/abs/2005.11294"

tooling:
  analysis_tools:
    - tool: "benchmark-suite"
      purpose: "Standard benchmarks"

    - tool: "classical-solvers"
      purpose: "Classical comparison"

signals:
  critical:
    - id: "BC-CRIT-001"
      signal: "Algorithm performs worse than classical"
      evidence_threshold: "quantum_result < classical_result"
      explanation: |
        If quantum algorithm underperforms classical methods, there is
        no practical value. Resources would be better spent on classical
        approaches.
      remediation: |
        Verify implementation correctness.
        Identify performance bottlenecks.
        Consider algorithm alternatives.

    - id: "BC-CRIT-002"
      signal: "No benchmarking performed"
      evidence_indicators:
        - "No comparison to baselines"
        - "Performance claims unsubstantiated"
      remediation: "Perform systematic benchmarking"

  high:
    - id: "BC-HIGH-001"
      signal: "Benchmark methodology flawed"
      evidence_indicators:
        - "Unfair comparison conditions"
        - "Cherry-picked problems"
        - "Different resource constraints"
      remediation: "Use fair benchmarking methodology"

    - id: "BC-HIGH-002"
      signal: "Results not reproducible"
      evidence_indicators:
        - "Benchmark results vary significantly"
        - "Cannot reproduce published results"
      remediation: "Document methodology for reproducibility"

  medium:
    - id: "BC-MED-001"
      signal: "Limited benchmark coverage"
      remediation: "Expand benchmark problem set"

    - id: "BC-MED-002"
      signal: "Outdated classical baseline"
      remediation: "Update classical comparison"

  low:
    - id: "BC-LOW-001"
      signal: "Benchmark methodology not documented"
      remediation: "Document benchmarking approach"

  positive:
    - id: "BC-POS-001"
      signal: "Clear quantum advantage demonstrated"
    - id: "BC-POS-002"
      signal: "Comprehensive benchmarking performed"
    - id: "BC-POS-003"
      signal: "Results reproducible"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Select Benchmarks"
      description: "Choose appropriate benchmark problems"
      duration_estimate: "45 min"

    - id: "2"
      name: "Run Quantum Algorithm"
      description: "Execute algorithm on benchmarks"
      duration_estimate: "1.5 hours"

    - id: "3"
      name: "Run Classical Baselines"
      description: "Execute classical comparison"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Compare Results"
      description: "Analyze performance differences"
      duration_estimate: "1 hour"

    - id: "5"
      name: "Validate Methodology"
      description: "Ensure fair comparison"
      duration_estimate: "45 min"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "benchmark_report"
      format: "json"
      description: "Benchmark comparison results"

closeout_checklist:
  - id: "bc-001"
    item: "Benchmarks executed"
    level: "CRITICAL"
    verification: "automated"

  - id: "bc-002"
    item: "Classical comparison complete"
    level: "BLOCKING"
    verification: "automated"

  - id: "bc-003"
    item: "Results documented"
    level: "WARNING"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["quantum-systems", "research-systems"]

relationships:
  commonly_combined:
    - "quantum-computing.simulation-testing.simulator-accuracy"
    - "quantum-computing.quantum-algorithm.algorithm-correctness"
