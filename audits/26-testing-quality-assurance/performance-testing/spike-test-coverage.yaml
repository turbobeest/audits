# ============================================================
# AUDIT: Spike Test Coverage
# ============================================================
# Evaluates spike testing to verify system behavior under
# sudden traffic bursts.
# ============================================================

audit:
  id: "testing-quality-assurance.performance-testing.spike-test-coverage"
  name: "Spike Test Coverage"
  version: "1.0.0"
  last_updated: "2026-01-22"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 26
  subcategory: "performance-testing"

  tier: "expert"
  estimated_duration: "1-2 hours"  # median: 1h

  completeness: "complete"
  requires_runtime: true
  destructive: false

execution:
  automatable: "yes"
  severity: "medium"
  scope: "testing"

  default_profiles:
    - "full"
    - "production"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates spike testing which simulates sudden traffic bursts
    to verify system response to rapid load changes. Tests auto-scaling
    behavior, caching effectiveness, and system stability when traffic
    spikes unexpectedly.

  why_it_matters: |
    Real traffic comes in bursts: marketing campaigns, viral content,
    breaking news, or flash sales. Systems must handle sudden load
    increases without crashing. Spike testing reveals whether
    auto-scaling, caching, and load balancing respond quickly enough.

  when_to_run:
    - "Before marketing campaigns"
    - "Pre-launch validation"
    - "Auto-scaling configuration changes"
    - "Cache strategy updates"

prerequisites:
  required_artifacts:
    - type: "spike_tests"
      description: "Spike test scripts"
    - type: "scaling_config"
      description: "Auto-scaling configuration"

  access_requirements:
    - "Source code repository access"
    - "Performance test environment"
    - "Auto-scaling enabled environment"

discovery:
  file_patterns:
    - glob: "**/spike/**"
      purpose: "Spike test directory"
    - glob: "**/*spike*.{js,scala,py}"
      purpose: "Spike test files"

  code_patterns:
    - pattern: "spike|burst|sudden"
      type: "regex"
      scope: "test"
      purpose: "Spike test scenarios"
    - pattern: "rampUp.*0|instant"
      type: "regex"
      scope: "test"
      purpose: "Instant load increase"

knowledge_sources:
  guides:
    - id: "spike-testing"
      name: "Spike Testing Guide"
      url: "https://k6.io/docs/test-types/spike-testing/"
      offline_cache: true
    - id: "auto-scaling"
      name: "Auto-Scaling Testing"
      url: "https://aws.amazon.com/blogs/compute/scaling-best-practices/"
      offline_cache: true

  learning_resources:
    - id: "traffic-patterns"
      title: "Understanding Traffic Patterns"
      type: "article"
      reference: "https://grafana.com/blog/2020/10/09/traffic-patterns/"

tooling:
  infrastructure_tools:
    - tool: "k6"
      purpose: "Spike test execution"
      offline_capable: true
    - tool: "Gatling"
      purpose: "High-burst traffic simulation"
      offline_capable: true

  scripts:
    - id: "spike-test-check"
      language: "bash"
      purpose: "Check spike test configuration"
      source: "inline"
      code: |
        #!/bin/bash
        echo "=== Spike Test Coverage Analysis ==="

        echo "Spike test files:"
        find . -name '*spike*' 2>/dev/null | wc -l

        echo ""
        echo "Burst configurations:"
        grep -r -E 'spike|burst|instant|rampUp.*0' --include='*.js' . 2>/dev/null | head -10

        echo ""
        echo "Stages with sudden increase:"
        grep -r -A2 'stages' --include='*.js' . 2>/dev/null | grep -E 'target.*[0-9]{3,}' | head -10

signals:
  critical:
    - id: "SPIKE-CRIT-001"
      signal: "No spike testing performed"
      evidence_pattern: "Only gradual ramp-up load tests"
      explanation: |
        Without spike testing, system behavior under sudden load is
        unknown. Marketing events, viral content, or DDoS could
        overwhelm a system that handles gradual load fine but
        crashes under sudden bursts.
      remediation: |
        Implement spike testing:
        1. Create instant-load scenario (0 to peak in seconds)
        2. Test at 3-5x normal traffic
        3. Monitor response times during spike
        4. Verify auto-scaling response time
        5. Test recovery after spike

    - id: "SPIKE-CRIT-002"
      signal: "Auto-scaling not validated under spikes"
      evidence_pattern: "Spike tests don't verify scaling behavior"
      explanation: |
        Auto-scaling that's too slow is useless for spikes. If
        scaling takes 5 minutes but users arrive in 30 seconds,
        the system will fail before scaling helps.
      remediation: |
        Test auto-scaling response:
        - Measure time to first scale event
        - Verify scaling speed meets spike timing
        - Test with pre-warming if needed
        - Document scaling latency expectations

  high:
    - id: "SPIKE-HIGH-001"
      signal: "Spike recovery not tested"
      evidence_pattern: "Tests don't verify behavior after spike subsides"
      explanation: |
        Systems must recover after spikes. Lingering connections,
        queued requests, or overwhelmed caches may cause continued
        problems after traffic normalizes.
      remediation: |
        Test spike recovery:
        - Apply spike then reduce to normal
        - Verify response times normalize
        - Check for lingering errors
        - Verify all requests eventually complete

    - id: "SPIKE-HIGH-002"
      signal: "Cache behavior under spike not tested"
      evidence_pattern: "No cache-related spike scenarios"
      explanation: |
        Cache stampedes occur when spikes hit cold caches. Many
        requests simultaneously miss cache, overwhelming the
        database. This pattern must be tested.
      remediation: |
        Test cache under spike:
        - Cold cache spike test
        - Cache expiry during spike
        - Cache thundering herd prevention
        - Cache warming validation

  medium:
    - id: "SPIKE-MED-001"
      signal: "Spike levels don't match potential real events"
      evidence_threshold: "Spike tests at < 2x normal load"
      remediation: "Test spikes at 5-10x normal to match real-world events"

    - id: "SPIKE-MED-002"
      signal: "Error handling during spike not validated"
      evidence_pattern: "No spike + error scenario tests"
      remediation: "Test graceful degradation during traffic spikes"

  low:
    - id: "SPIKE-LOW-001"
      signal: "Spike test results not compared to baseline"
      evidence_pattern: "No comparison of spike vs normal behavior"

  positive:
    - id: "SPIKE-POS-001"
      signal: "Comprehensive spike testing"
      evidence_pattern: "Multiple spike scenarios with scaling verification"

    - id: "SPIKE-POS-002"
      signal: "System handles 5x traffic spikes"
      evidence_pattern: "Spike tests pass at 5x normal load"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Inventory spike tests"
      description: |
        Find existing spike test scenarios.
      duration_estimate: "10 min"
      commands:
        - purpose: "Find spike tests"
          command: "find . -name '*spike*' 2>/dev/null"
        - purpose: "Find burst configs"
          command: "grep -r -l 'spike\\|burst' --include='*.js' . 2>/dev/null"
      expected_findings:
        - "Spike test inventory"
        - "Scenario types"

    - id: "2"
      name: "Review spike configurations"
      description: |
        Analyze spike test parameters and load levels.
      duration_estimate: "15 min"
      commands:
        - purpose: "Extract spike configs"
          command: "grep -r -A5 'spike\\|stages' --include='*spike*' . 2>/dev/null | head -20"
      expected_findings:
        - "Spike load levels"
        - "Duration configurations"

    - id: "3"
      name: "Verify scaling integration"
      description: |
        Check if spike tests verify auto-scaling behavior.
      duration_estimate: "15 min"
      questions:
        - "Do tests monitor scaling events?"
        - "Is scaling latency measured?"
        - "Are scaling thresholds validated?"
      expected_findings:
        - "Scaling verification"
        - "Integration gaps"

    - id: "4"
      name: "Assess recovery testing"
      description: |
        Verify tests include post-spike recovery validation.
      duration_estimate: "15 min"
      commands:
        - purpose: "Find recovery scenarios"
          command: "grep -r 'recovery\\|after.*spike\\|cooldown' --include='*.js' . 2>/dev/null | head -10"
      expected_findings:
        - "Recovery testing"
        - "Missing scenarios"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "spike_report"
      format: "table"
      sections:
        - "Spike Test Inventory"
        - "Load Levels"
        - "Scaling Integration"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Spike Test Analysis"
        - "Recommendations"

  confidence_guidance:
    high: "Spike test execution with results"
    medium: "Configuration analysis"
    low: "Documentation review only"

offline:
  capability: "partial"

  cache_manifest:
    knowledge:
      - source_id: "spike-testing"
        priority: "required"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires detailed analysis"
    production:
      included: true
      priority: 4
    full:
      included: true
      priority: 7

closeout_checklist:
  - id: "spike-001"
    item: "Spike tests inventoried"
    level: "CRITICAL"
    verification: "find . -name '*spike*' 2>/dev/null | wc -l"
    expected: "Count documented"

  - id: "spike-002"
    item: "Spike levels assessed"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Document spike multipliers tested"
    expected: "Confirmed by reviewer"

  - id: "spike-003"
    item: "Scaling verification confirmed"
    level: "WARNING"
    verification: "manual"
    verification_notes: "Verify scaling is tested during spikes"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["all"]

  compliance_frameworks:
    - framework: "ISO 25010"
      controls: ["Performance Efficiency", "Reliability"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.performance-testing.load-test-coverage"
    - "testing-quality-assurance.performance-testing.stress-test-coverage"
    - "scalability-capacity.auto-scaling.auto-scaling-configuration"
