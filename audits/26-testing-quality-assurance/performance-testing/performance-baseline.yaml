# ============================================================
# AUDIT: Performance Baseline
# ============================================================
# Evaluates performance baseline establishment and regression
# detection capabilities.
# ============================================================

audit:
  id: "testing-quality-assurance.performance-testing.performance-baseline"
  name: "Performance Baseline"
  version: "1.0.0"
  last_updated: "2026-01-22"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 26
  subcategory: "performance-testing"

  tier: "expert"
  estimated_duration: "1-2 hours"  # median: 1h

  completeness: "complete"
  requires_runtime: false
  destructive: false

execution:
  automatable: "yes"
  severity: "high"
  scope: "testing"

  default_profiles:
    - "full"
    - "production"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates performance baseline establishment including documented
    baseline metrics, regression detection thresholds, historical
    comparison capabilities, and continuous performance monitoring.
    Verifies that performance changes are detected automatically.

  why_it_matters: |
    Without baselines, performance regressions go unnoticed. A change
    that adds 50ms to response time seems minor but accumulates over
    releases. Baselines provide objective criteria for detecting
    degradation and prove performance improvements.

  when_to_run:
    - "CI/CD pipeline setup"
    - "After baseline establishment"
    - "Performance review cycles"
    - "Post-release validation"

prerequisites:
  required_artifacts:
    - type: "performance_tests"
      description: "Performance test suite"
    - type: "historical_data"
      description: "Historical performance data"

  access_requirements:
    - "Source code repository access"
    - "Performance test results"
    - "Metrics storage access"

discovery:
  file_patterns:
    - glob: "**/baseline*"
      purpose: "Baseline configuration"
    - glob: "**/thresholds*"
      purpose: "Threshold configuration"

  code_patterns:
    - pattern: "baseline|threshold|p95|p99"
      type: "regex"
      scope: "test"
      purpose: "Baseline and threshold definitions"
    - pattern: "comparison|regression|trend"
      type: "regex"
      scope: "test"
      purpose: "Comparison logic"

  metrics_queries:
    - system: "Performance Dashboard"
      query: "baseline_comparison"
      purpose: "Current vs baseline comparison"
      threshold: "Within 10% of baseline"

knowledge_sources:
  guides:
    - id: "perf-baselines"
      name: "Performance Baselines Guide"
      url: "https://www.perfmatrix.com/performance-testing-baseline/"
      offline_cache: true
    - id: "regression-detection"
      name: "Detecting Performance Regressions"
      url: "https://testing.googleblog.com/2019/03/detecting-performance-regressions.html"
      offline_cache: true

  learning_resources:
    - id: "continuous-perf"
      title: "Continuous Performance Testing"
      type: "article"
      reference: "https://k6.io/blog/continuous-performance-testing/"

tooling:
  infrastructure_tools:
    - tool: "k6"
      purpose: "Performance testing with thresholds"
      offline_capable: true
    - tool: "Grafana"
      purpose: "Performance visualization"
      offline_capable: false

  scripts:
    - id: "baseline-check"
      language: "bash"
      purpose: "Check baseline configuration"
      source: "inline"
      code: |
        #!/bin/bash
        echo "=== Performance Baseline Analysis ==="

        echo "Baseline configuration files:"
        find . -name '*baseline*' 2>/dev/null | head -10

        echo ""
        echo "Threshold definitions:"
        grep -r 'threshold\|p95\|p99' --include='*.js' --include='*.yaml' . 2>/dev/null | head -15

        echo ""
        echo "Historical comparison:"
        grep -r 'compare\|baseline\|previous' --include='*.js' . 2>/dev/null | head -10

signals:
  critical:
    - id: "BASELINE-CRIT-001"
      signal: "No performance baseline established"
      evidence_pattern: "No documented baseline metrics"
      explanation: |
        Without a baseline, you can't detect regressions. Each release
        could be slower than the last without anyone noticing. Over
        time, performance degrades significantly.
      remediation: |
        Establish performance baselines:
        1. Run comprehensive performance tests
        2. Document key metrics (p50, p95, p99)
        3. Store results in accessible location
        4. Set thresholds based on baseline
        5. Compare new results to baseline

    - id: "BASELINE-CRIT-002"
      signal: "No automated regression detection"
      evidence_pattern: "Performance tests don't fail on regression"
      explanation: |
        Manual baseline comparison is unreliable. Automated regression
        detection in CI ensures every change is validated against
        baseline, preventing gradual degradation.
      remediation: |
        Automate regression detection:
        - Add thresholds to performance tests
        - Fail builds on threshold breach
        - Alert on regression detection
        - Track trends over time

  high:
    - id: "BASELINE-HIGH-001"
      signal: "Baseline not updated after legitimate changes"
      evidence_pattern: "Baseline from 6+ months ago"
      explanation: |
        Stale baselines cause false positives or hide real issues.
        After legitimate architecture changes, baselines should be
        re-established to reflect new expected behavior.
      remediation: |
        Maintain baselines:
        - Review and update quarterly
        - Re-baseline after major changes
        - Document baseline update history
        - Keep previous baselines for reference

    - id: "BASELINE-HIGH-002"
      signal: "Insufficient metrics in baseline"
      evidence_pattern: "Only average response time tracked"
      explanation: |
        Averages hide tail latency issues. A few slow requests can
        severely impact user experience while averages look fine.
        Percentiles (p95, p99) reveal true performance.
      remediation: |
        Track comprehensive metrics:
        - Response time p50, p95, p99
        - Error rate
        - Throughput (requests/second)
        - Resource utilization
        - Apdex score

  medium:
    - id: "BASELINE-MED-001"
      signal: "No trend analysis"
      evidence_pattern: "Only point-in-time comparison"
      remediation: "Track performance trends over multiple releases"

    - id: "BASELINE-MED-002"
      signal: "Thresholds too loose"
      evidence_threshold: "Regression threshold > 20%"
      remediation: "Tighten thresholds to detect smaller regressions"

  low:
    - id: "BASELINE-LOW-001"
      signal: "Baseline documentation incomplete"
      evidence_pattern: "No explanation of baseline conditions"

  positive:
    - id: "BASELINE-POS-001"
      signal: "Comprehensive baseline with automation"
      evidence_pattern: "Full metrics tracked with CI integration"

    - id: "BASELINE-POS-002"
      signal: "Historical trend tracking"
      evidence_pattern: "Performance history maintained and graphed"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Find baseline configuration"
      description: |
        Locate baseline definitions and threshold settings.
      duration_estimate: "15 min"
      commands:
        - purpose: "Find baseline files"
          command: "find . -name '*baseline*' -o -name '*threshold*' 2>/dev/null | head -10"
        - purpose: "Find threshold definitions"
          command: "grep -r 'threshold' --include='*.js' --include='*.yaml' . 2>/dev/null | head -15"
      expected_findings:
        - "Baseline configuration"
        - "Threshold definitions"

    - id: "2"
      name: "Review metric coverage"
      description: |
        Assess which metrics are included in baseline.
      duration_estimate: "15 min"
      commands:
        - purpose: "Find tracked metrics"
          command: "grep -r 'p95\\|p99\\|avg\\|median' --include='*.js' . 2>/dev/null | head -15"
      expected_findings:
        - "Tracked metrics"
        - "Missing metrics"

    - id: "3"
      name: "Verify automation"
      description: |
        Check if regression detection is automated in CI.
      duration_estimate: "15 min"
      commands:
        - purpose: "Check CI for perf gates"
          command: "grep -r 'threshold\\|performance\\|k6' .github/workflows/ 2>/dev/null | head -10"
      expected_findings:
        - "CI integration"
        - "Automation status"

    - id: "4"
      name: "Assess baseline freshness"
      description: |
        Determine when baseline was last updated.
      duration_estimate: "10 min"
      commands:
        - purpose: "Check baseline history"
          command: "git log --oneline -- '*baseline*' 2>/dev/null | head -5"
      expected_findings:
        - "Baseline age"
        - "Update frequency"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "baseline_report"
      format: "table"
      sections:
        - "Baseline Metrics"
        - "Threshold Configuration"
        - "Automation Status"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Baseline Analysis"
        - "Recommendations"

  confidence_guidance:
    high: "Full baseline review with history"
    medium: "Configuration analysis"
    low: "Documentation review only"

offline:
  capability: "full"

  cache_manifest:
    knowledge:
      - source_id: "perf-baselines"
        priority: "required"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires detailed analysis"
    production:
      included: true
      priority: 3
    full:
      included: true
      priority: 5

closeout_checklist:
  - id: "baseline-001"
    item: "Baseline configuration found"
    level: "CRITICAL"
    verification: "find . -name '*baseline*' -o -name '*threshold*' 2>/dev/null | wc -l"
    expected: "Count > 0"

  - id: "baseline-002"
    item: "Metrics coverage assessed"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Document tracked performance metrics"
    expected: "Confirmed by reviewer"

  - id: "baseline-003"
    item: "CI automation verified"
    level: "WARNING"
    verification: "grep -r 'threshold\\|k6' .github/workflows/ 2>/dev/null | wc -l"
    expected: "Count > 0"

governance:
  applicable_to:
    archetypes: ["all"]

  compliance_frameworks:
    - framework: "ISO 25010"
      controls: ["Performance Efficiency"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.performance-testing.load-test-coverage"
    - "testing-quality-assurance.test-effectiveness.test-signal-vs-noise"
    - "observability.performance-monitoring.response-time-tracking"
