# ============================================================
# AUDIT: Load Test Coverage
# ============================================================
# Evaluates load testing coverage and methodology for verifying
# system performance under normal and peak load.
# ============================================================

audit:
  id: "testing-quality-assurance.performance-testing.load-test-coverage"
  name: "Load Test Coverage"
  version: "1.0.0"
  last_updated: "2026-01-22"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 26
  subcategory: "performance-testing"

  tier: "expert"
  estimated_duration: "2-3 hours"  # median: 2h

  completeness: "complete"
  requires_runtime: true
  destructive: false

execution:
  automatable: "yes"
  severity: "high"
  scope: "testing"

  default_profiles:
    - "full"
    - "production"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates load testing coverage including identification of load
    scenarios, user simulation accuracy, endpoint coverage, and
    validation of performance under expected traffic levels. Assesses
    whether load tests adequately simulate real-world usage patterns.

  why_it_matters: |
    Systems that aren't load tested fail under real traffic. Performance
    issues like slow response times, database bottlenecks, and memory
    leaks only manifest under load. Without load testing, you discover
    these issues from angry users during peak traffic rather than in
    controlled testing.

  when_to_run:
    - "Pre-release validation"
    - "Before major traffic events"
    - "After infrastructure changes"
    - "Capacity planning"

prerequisites:
  required_artifacts:
    - type: "load_tests"
      description: "Load test scripts and scenarios"
    - type: "traffic_data"
      description: "Production traffic patterns"

  access_requirements:
    - "Source code repository access"
    - "Performance test environment"
    - "Load testing tooling"

discovery:
  file_patterns:
    - glob: "**/k6/**"
      purpose: "k6 load test scripts"
    - glob: "**/*.jmx"
      purpose: "JMeter test plans"
    - glob: "**/gatling/**"
      purpose: "Gatling simulations"
    - glob: "**/locust*"
      purpose: "Locust test files"

  code_patterns:
    - pattern: "vus|virtualUsers|threads"
      type: "regex"
      scope: "test"
      purpose: "Virtual user configuration"
    - pattern: "rampUp|stages|duration"
      type: "regex"
      scope: "test"
      purpose: "Load profile configuration"

  metrics_queries:
    - system: "Load Test Tool"
      query: "response_time_p95"
      purpose: "Response time metrics"
      threshold: "< 500ms"

knowledge_sources:
  guides:
    - id: "k6-guide"
      name: "k6 Load Testing Guide"
      url: "https://k6.io/docs/"
      offline_cache: true
    - id: "load-testing"
      name: "Load Testing Best Practices"
      url: "https://www.perfmatrix.com/load-testing-best-practices/"
      offline_cache: true

  learning_resources:
    - id: "perf-testing-book"
      title: "The Art of Application Performance Testing"
      type: "book"
      reference: "ISBN 978-0596520663"

tooling:
  infrastructure_tools:
    - tool: "k6"
      purpose: "Modern load testing"
      offline_capable: true
    - tool: "JMeter"
      purpose: "Enterprise load testing"
      offline_capable: true
    - tool: "Gatling"
      purpose: "Scala-based load testing"
      offline_capable: true
    - tool: "Locust"
      purpose: "Python load testing"
      offline_capable: true

  scripts:
    - id: "load-test-check"
      language: "bash"
      purpose: "Check load test coverage"
      source: "inline"
      code: |
        #!/bin/bash
        echo "=== Load Test Coverage Analysis ==="

        echo "k6 test files:"
        find . -name '*.js' -path '*k6*' -o -name '*load*' -name '*.js' 2>/dev/null | wc -l

        echo ""
        echo "JMeter test plans:"
        find . -name '*.jmx' 2>/dev/null | wc -l

        echo ""
        echo "Gatling simulations:"
        find . -name '*Simulation*.scala' 2>/dev/null | wc -l

        echo ""
        echo "Locust files:"
        find . -name 'locust*.py' 2>/dev/null | wc -l

        echo ""
        echo "Endpoints tested:"
        grep -r -h 'http\.[get\|post\|put\|delete]' --include='*.js' . 2>/dev/null | wc -l

signals:
  critical:
    - id: "LOADTEST-CRIT-001"
      signal: "No load tests exist"
      evidence_pattern: "Zero load test files found"
      explanation: |
        Without load tests, performance under traffic is unknown.
        The system may fail, slow down, or crash when users arrive.
        This is a critical gap for any production system.
      remediation: |
        Implement load testing:
        1. Choose tool (k6 recommended for modern workflows)
        2. Identify critical endpoints and user journeys
        3. Create baseline load scenarios
        4. Define performance thresholds
        5. Integrate into CI/CD pipeline

    - id: "LOADTEST-CRIT-002"
      signal: "Load tests don't cover critical endpoints"
      evidence_pattern: "API endpoints missing from load scenarios"
      explanation: |
        Incomplete load test coverage means some endpoints aren't
        validated under load. These untested endpoints may become
        bottlenecks that bring down the entire system.
      remediation: |
        Ensure coverage of:
        - Authentication endpoints
        - Core API endpoints
        - Database-heavy operations
        - File upload/download
        - Search functionality

  high:
    - id: "LOADTEST-HIGH-001"
      signal: "Load scenarios don't match production patterns"
      evidence_pattern: "Uniform load pattern instead of realistic traffic"
      explanation: |
        Real traffic isn't uniform. Users arrive in bursts, have
        varying behavior patterns, and hit different endpoints.
        Unrealistic load tests miss issues that occur in production.
      remediation: |
        Create realistic load profiles:
        - Analyze production traffic patterns
        - Model user think time between requests
        - Include realistic endpoint distribution
        - Simulate geographic distribution

    - id: "LOADTEST-HIGH-002"
      signal: "Performance thresholds not defined"
      evidence_pattern: "Load tests run but no pass/fail criteria"
      explanation: |
        Without thresholds, load tests don't fail on performance
        regression. Teams may accept degraded performance without
        realizing it. Thresholds create objective quality gates.
      remediation: |
        Define performance thresholds:
        - Response time p95 < 500ms
        - Error rate < 1%
        - Throughput >= expected
        - Fail builds on threshold breach

  medium:
    - id: "LOADTEST-MED-001"
      signal: "Load tests not in CI/CD pipeline"
      evidence_pattern: "Manual load test execution only"
      remediation: "Add load tests to CI with reduced scale for continuous validation"

    - id: "LOADTEST-MED-002"
      signal: "Database load not isolated"
      evidence_pattern: "Load tests affect production database"
      remediation: "Use isolated test environment with production-like data"

  low:
    - id: "LOADTEST-LOW-001"
      signal: "Load test documentation missing"
      evidence_pattern: "No explanation of scenarios or thresholds"

  positive:
    - id: "LOADTEST-POS-001"
      signal: "Comprehensive load test coverage"
      evidence_pattern: "All critical endpoints covered with thresholds"

    - id: "LOADTEST-POS-002"
      signal: "Load tests in CI pipeline"
      evidence_pattern: "Automated load testing on deployments"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Inventory load tests"
      description: |
        Find and categorize existing load test scripts.
      duration_estimate: "15 min"
      commands:
        - purpose: "Find k6 tests"
          command: "find . -name '*.js' -path '*k6*' 2>/dev/null"
        - purpose: "Find JMeter/Gatling tests"
          command: "find . -name '*.jmx' -o -name '*Simulation*.scala' 2>/dev/null"
      expected_findings:
        - "Load test inventory"
        - "Tool identification"

    - id: "2"
      name: "Analyze endpoint coverage"
      description: |
        Determine which endpoints are covered by load tests.
      duration_estimate: "25 min"
      commands:
        - purpose: "Extract endpoints from tests"
          command: "grep -r -h 'http\\.' --include='*.js' . 2>/dev/null | head -30"
      expected_findings:
        - "Covered endpoints"
        - "Coverage gaps"

    - id: "3"
      name: "Review load profiles"
      description: |
        Analyze load patterns and user simulation.
      duration_estimate: "20 min"
      commands:
        - purpose: "Find load configurations"
          command: "grep -r 'vus\\|stages\\|rampUp' --include='*.js' --include='*.jmx' . 2>/dev/null | head -20"
      expected_findings:
        - "Load profile patterns"
        - "Virtual user counts"

    - id: "4"
      name: "Verify thresholds"
      description: |
        Check for performance threshold definitions.
      duration_estimate: "15 min"
      commands:
        - purpose: "Find threshold configs"
          command: "grep -r 'thresholds\\|threshold' --include='*.js' --include='*.jmx' . 2>/dev/null | head -15"
      expected_findings:
        - "Threshold definitions"
        - "Pass/fail criteria"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "coverage_matrix"
      format: "table"
      sections:
        - "Load Test Inventory"
        - "Endpoint Coverage"
        - "Threshold Configuration"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Load Test Analysis"
        - "Recommendations"

  confidence_guidance:
    high: "Complete analysis with test execution"
    medium: "Static analysis of test files"
    low: "Pattern matching only"

offline:
  capability: "partial"

  cache_manifest:
    knowledge:
      - source_id: "k6-guide"
        priority: "required"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires detailed analysis"
    production:
      included: true
      priority: 2
    full:
      included: true
      priority: 4

closeout_checklist:
  - id: "loadtest-001"
    item: "Load tests inventoried"
    level: "CRITICAL"
    verification: "find . -name '*.js' -path '*k6*' -o -name '*.jmx' 2>/dev/null | wc -l"
    expected: "Count > 0"

  - id: "loadtest-002"
    item: "Endpoint coverage assessed"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Document covered endpoints"
    expected: "Confirmed by reviewer"

  - id: "loadtest-003"
    item: "Thresholds verified"
    level: "WARNING"
    verification: "grep -r 'threshold' --include='*.js' . 2>/dev/null | wc -l"
    expected: "Count > 0"

governance:
  applicable_to:
    archetypes: ["all"]

  compliance_frameworks:
    - framework: "ISO 25010"
      controls: ["Performance Efficiency"]
    - framework: "SOC2"
      controls: ["CC7.1"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.performance-testing.stress-test-coverage"
    - "testing-quality-assurance.performance-testing.performance-baseline"
    - "performance-efficiency.response-time.api-response-time"
