# ============================================================
# AUDIT: Unit Test Maintainability
# ============================================================
# Evaluates the maintainability and long-term sustainability
# of unit tests.
# ============================================================

audit:
  id: "testing-quality-assurance.unit-testing.unit-test-maintainability"
  name: "Unit Test Maintainability"
  version: "1.0.0"
  last_updated: "2026-01-22"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 26
  subcategory: "unit-testing"

  tier: "expert"
  estimated_duration: "1-2 hours"  # median: 1h

  completeness: "complete"
  requires_runtime: false
  destructive: false

execution:
  automatable: "partial"
  severity: "medium"
  scope: "testing"

  default_profiles:
    - "full"
    - "quality"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates the maintainability of unit tests including readability,
    naming conventions, test organization, code duplication in tests,
    assertion clarity, and ease of understanding test intent. Assesses
    whether tests serve as living documentation.

  why_it_matters: |
    Unmaintainable tests become a burden. When tests are hard to understand,
    developers fear changing them and may delete them rather than fix them.
    Well-maintained tests serve as documentation, catch regressions, and
    enable confident refactoring. Test code deserves the same quality
    standards as production code.

  when_to_run:
    - "Test suite code review"
    - "Test debt assessment"
    - "Onboarding review"
    - "Sprint retrospectives"

prerequisites:
  required_artifacts:
    - type: "source_code"
      description: "Source code with unit tests"

  access_requirements:
    - "Source code repository access"
    - "Test file analysis"

discovery:
  file_patterns:
    - glob: "**/*.test.{js,ts}"
      purpose: "JavaScript test files"
    - glob: "**/test_*.py"
      purpose: "Python test files"
    - glob: "**/*Test.java"
      purpose: "Java test files"

  code_patterns:
    - pattern: "it\\(['\"]should|test.*should|def test_.*_should"
      type: "regex"
      scope: "test"
      purpose: "Descriptive test names"
    - pattern: "describe\\(['\"][A-Z]"
      type: "regex"
      scope: "test"
      purpose: "Test organization"
    - pattern: "expect\\(.*\\)\\.toBe\\(true\\)|assert.*True|assertEqual\\(True"
      type: "regex"
      scope: "test"
      purpose: "Boolean assertions (often unclear)"

knowledge_sources:
  guides:
    - id: "test-naming"
      name: "Test Naming Conventions"
      url: "https://osherove.com/blog/2005/4/3/naming-standards-for-unit-tests.html"
      offline_cache: true
    - id: "test-readability"
      name: "Writing Readable Tests"
      url: "https://mtlynch.io/good-developers-bad-tests/"
      offline_cache: true

  learning_resources:
    - id: "clean-code"
      title: "Clean Code - Chapter 9: Unit Tests"
      type: "book"
      reference: "ISBN 978-0132350884"
    - id: "art-of-testing"
      title: "The Art of Unit Testing"
      type: "book"
      reference: "ISBN 978-1617290893"

tooling:
  static_analysis:
    - tool: "eslint-plugin-jest"
      purpose: "Jest best practices"
      offline_capable: true
    - tool: "pytest-lint"
      purpose: "Python test linting"
      offline_capable: true
    - tool: "SonarQube"
      purpose: "Test code quality"
      offline_capable: false

  scripts:
    - id: "maintainability-check"
      language: "bash"
      purpose: "Check test maintainability indicators"
      source: "inline"
      code: |
        #!/bin/bash
        echo "=== Test Maintainability Analysis ==="

        echo "Tests with descriptive names (should/when/given):"
        grep -r -c 'should\|when\|given' --include='*.test.*' . 2>/dev/null | awk -F: '{sum+=$2} END {print sum}'

        echo ""
        echo "Large test files (>500 lines):"
        find . -name '*.test.*' -exec wc -l {} \; 2>/dev/null | awk '$1 > 500 {print}' | wc -l

        echo ""
        echo "Test helper/utility functions:"
        grep -r -l 'function.*test\|def.*helper\|setup\|factory' --include='*.test.*' . 2>/dev/null | wc -l

        echo ""
        echo "Boolean-only assertions (potential code smell):"
        grep -r -c '\.toBe(true)\|\.toBe(false)\|assertTrue\|assertFalse' --include='*.test.*' . 2>/dev/null | awk -F: '{sum+=$2} END {print sum}'

signals:
  critical:
    - id: "TESTMAINT-CRIT-001"
      signal: "Tests without descriptive names"
      evidence_pattern: "Generic test names like 'test1', 'it works', 'should work'"
      explanation: |
        Non-descriptive test names make it impossible to understand what
        broke when tests fail. Developers waste time investigating failures
        that could be immediately understood from a good name.
      remediation: |
        Use descriptive test names following patterns:
        - 'should [expected behavior] when [condition]'
        - 'given [context], when [action], then [result]'
        - '[method]_[scenario]_[expected result]'

    - id: "TESTMAINT-CRIT-002"
      signal: "Massive test files with no organization"
      evidence_pattern: "Single test file > 1000 lines without describe/context blocks"
      explanation: |
        Large unorganized test files are impossible to navigate. Finding
        relevant tests becomes a search problem, and adding tests in the
        right place is guesswork.
      remediation: |
        Organize tests:
        - Group related tests in describe/context blocks
        - One test file per module/class
        - Use nested describes for method-level grouping
        - Extract shared setup to before blocks

  high:
    - id: "TESTMAINT-HIGH-001"
      signal: "Significant code duplication in tests"
      evidence_pattern: "Same setup code repeated across many tests"
      explanation: |
        Duplicated test code multiplies maintenance burden. When the
        setup changes, every copy must be updated. DRY applies to
        test code too.
      remediation: |
        Extract common code:
        - Use beforeEach for shared setup
        - Create test fixtures and factories
        - Extract helper functions for assertions
        - Use parameterized tests for variations

    - id: "TESTMAINT-HIGH-002"
      signal: "Magic numbers and unexplained values"
      evidence_pattern: "Hard-coded values without constants or comments"
      explanation: |
        Tests with unexplained magic values are cryptic. When they fail,
        it's unclear what the values represent or why they should be
        those specific values.
      remediation: |
        Use meaningful constants:
        - Define constants with descriptive names
        - Document why specific values are chosen
        - Use builder patterns for test data

  medium:
    - id: "TESTMAINT-MED-001"
      signal: "Tests verify implementation details"
      evidence_pattern: "Tests that break when refactoring without behavior change"
      remediation: "Focus tests on public API and observable behavior"

    - id: "TESTMAINT-MED-002"
      signal: "Unclear assertions"
      evidence_pattern: "toBe(true) instead of specific matchers"
      remediation: "Use specific matchers like toContain, toHaveProperty, etc."

  low:
    - id: "TESTMAINT-LOW-001"
      signal: "Tests lack comments explaining complex scenarios"
      evidence_pattern: "Complex test logic without explanation"

  positive:
    - id: "TESTMAINT-POS-001"
      signal: "Tests serve as documentation"
      evidence_pattern: "Reading tests clearly explains system behavior"

    - id: "TESTMAINT-POS-002"
      signal: "Well-organized test hierarchy"
      evidence_pattern: "Clear describe/context nesting structure"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Analyze test naming"
      description: |
        Evaluate test naming conventions and descriptiveness.
      duration_estimate: "20 min"
      commands:
        - purpose: "Sample test names"
          command: "grep -r -h \"it(\\|test(\\|def test_\" --include='*.test.*' . 2>/dev/null | head -30"
        - purpose: "Count descriptive names"
          command: "grep -r -c 'should\\|when\\|given' --include='*.test.*' . 2>/dev/null | awk -F: '{sum+=$2} END {print sum}'"
      expected_findings:
        - "Test naming patterns"
        - "Descriptive name percentage"

    - id: "2"
      name: "Check test organization"
      description: |
        Evaluate how tests are organized and grouped.
      duration_estimate: "20 min"
      commands:
        - purpose: "Count describe blocks"
          command: "grep -r -c 'describe(' --include='*.test.*' . 2>/dev/null | awk -F: '{sum+=$2} END {print sum}'"
        - purpose: "Find large test files"
          command: "find . -name '*.test.*' -exec wc -l {} \\; 2>/dev/null | sort -rn | head -10"
      expected_findings:
        - "Test organization patterns"
        - "Large file identification"

    - id: "3"
      name: "Identify code duplication"
      description: |
        Look for repeated patterns indicating duplication.
      duration_estimate: "25 min"
      commands:
        - purpose: "Find repeated setup"
          command: "grep -r -h 'beforeEach\\|setUp' --include='*.test.*' . 2>/dev/null | sort | uniq -c | sort -rn | head -10"
      expected_findings:
        - "Duplication patterns"
        - "Shared setup usage"

    - id: "4"
      name: "Review assertion clarity"
      description: |
        Assess whether assertions are clear and specific.
      duration_estimate: "15 min"
      commands:
        - purpose: "Count boolean assertions"
          command: "grep -r -c 'toBe(true)\\|toBe(false)' --include='*.test.*' . 2>/dev/null | awk -F: '{sum+=$2} END {print sum}'"
        - purpose: "Count specific matchers"
          command: "grep -r -c 'toEqual\\|toContain\\|toHaveProperty' --include='*.test.*' . 2>/dev/null | awk -F: '{sum+=$2} END {print sum}'"
      expected_findings:
        - "Assertion specificity"
        - "Matcher usage patterns"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "maintainability_report"
      format: "table"
      sections:
        - "Naming Quality"
        - "Organization"
        - "Duplication"
        - "Assertion Clarity"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Maintainability Assessment"
        - "Improvement Recommendations"

  confidence_guidance:
    high: "Comprehensive pattern analysis"
    medium: "Sample-based review"
    low: "Quick spot check"

offline:
  capability: "full"

  cache_manifest:
    knowledge:
      - source_id: "test-naming"
        priority: "required"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires detailed analysis"
    full:
      included: true
      priority: 10

closeout_checklist:
  - id: "testmaint-001"
    item: "Test naming quality assessed"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Document percentage of descriptive test names"
    expected: "Confirmed by reviewer"

  - id: "testmaint-002"
    item: "Test organization reviewed"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Describe organization structure"
    expected: "Confirmed by reviewer"

  - id: "testmaint-003"
    item: "Code duplication identified"
    level: "WARNING"
    verification: "manual"
    verification_notes: "List areas with significant duplication"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["all"]

  compliance_frameworks:
    - framework: "ISO 25010"
      controls: ["Maintainability"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.unit-testing.mock-stub-strategy"
    - "testing-quality-assurance.test-effectiveness.test-maintenance-burden"
    - "code-quality-maintainability.code-readability.code-documentation"
