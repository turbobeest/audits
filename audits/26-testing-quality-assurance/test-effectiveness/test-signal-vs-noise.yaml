audit:
  id: testing-quality-assurance.test-effectiveness.test-signal-vs-noise
  name: Test Signal vs Noise
  version: 1.0.0
  last_updated: '2026-01-22'
  status: active
  category: testing-quality-assurance
  category_number: 26
  subcategory: test-effectiveness
  tier: expert
  estimated_duration: 30-45 min
  completeness: complete
  requires_runtime: false
  destructive: false
execution:
  automatable: partial
  severity: medium
  scope: quality
  default_profiles:
  - full
  - quality
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the signal-to-noise ratio of the test suite - how well
    test failures communicate real problems versus generating false
    alarms, unclear failures, or irrelevant information. Assesses
    test clarity, failure messages, and actionability.
  why_it_matters: |
    Noisy tests train developers to ignore failures. When most failures
    are false positives or unclear, real issues get dismissed. Good tests
    communicate clearly: what failed, why, and what to do about it.
    High signal enables fast debugging.
  when_to_run:
  - Test quality assessment
  - Developer productivity review
  - Test suite maintenance
  - Quality improvement
prerequisites:
  required_artifacts:
  - type: test_files
    description: Test code
  - type: test_results
    description: Recent test failures
  access_requirements:
  - Test source code
  - Test execution results
discovery:
  file_patterns:
  - glob: '**/*.test.{js,ts}'
    purpose: Test files
  - glob: '**/*.spec.{js,ts}'
    purpose: Spec files
  code_patterns:
  - pattern: expect\(|assert\.|should\.
    type: regex
    scope: test
    purpose: Assertion patterns
knowledge_sources:
  guides:
  - id: test-assertions
    name: Effective Test Assertions
    url: https://jestjs.io/docs/expect
    offline_cache: true
  - id: test-naming
    name: Test Naming Conventions
    url: https://osherove.com/blog/2005/4/3/naming-standards-for-unit-tests.html
    offline_cache: true
  learning_resources:
  - id: assertion-patterns
    title: Assertion Best Practices
    type: article
    reference: Testing Best Practices
tooling:
  static_analysis:
  - tool: Custom analysis
    purpose: Assertion pattern detection
    offline_capable: true
  scripts:
  - id: signal-check
    language: bash
    purpose: Analyze test signal quality
    source: inline
    code: |
      #!/bin/bash
      echo "=== Test Signal vs Noise Analysis ==="

      echo "Assertion patterns:"
      grep -r -c 'expect\|assert\|should' --include='*.test.*' --include='*.spec.*' . 2>/dev/null | grep -v ':0$' | head -10

      echo ""
      echo "Tests with descriptive names (should/describe):"
      grep -r -c 'should\|describe\|it(' --include='*.test.*' --include='*.spec.*' . 2>/dev/null | head -10

      echo ""
      echo "Custom error messages in assertions:"
      grep -r -n "expect.*\..*('.*')" --include='*.test.*' . 2>/dev/null | head -10

      echo ""
      echo "Generic toBe/toEqual usage (potential low-signal):"
      grep -r -c 'toBe\|toEqual' --include='*.test.*' . 2>/dev/null | grep -v ':0$' | head -10

      echo ""
      echo "Specific matchers (high-signal):"
      grep -r -c 'toHaveLength\|toThrow\|toContain\|toMatch' --include='*.test.*' . 2>/dev/null | grep -v ':0$' | head -10
signals:
  critical:
  - id: SIGNAL-CRIT-001
    signal: High false positive rate
    evidence_pattern: '> 20% of failures are false positives'
    explanation: |
      When most failures don't indicate real problems, developers
      stop trusting the test suite. They assume failures are noise
      and may miss real issues. Tests must be reliable indicators.
    remediation: |
      Reduce false positives:
      1. Track false positive causes
      2. Fix or remove noisy tests
      3. Improve test stability
      4. Verify all failures indicate real issues
      5. Monitor false positive rate
  - id: SIGNAL-CRIT-002
    signal: Failures don't indicate what's wrong
    evidence_pattern: Debugging requires significant investigation
    explanation: |
      Unclear failure messages waste debugging time. Good tests tell
      you exactly what failed and often why. Developers shouldn't
      need to read test code to understand failures.
    remediation: |
      Improve failure messages:
      - Add custom assertion messages
      - Use specific matchers
      - Include expected vs actual
      - Name tests descriptively
      - Log relevant context
  high:
  - id: SIGNAL-HIGH-001
    signal: Too many assertions per test
    evidence_pattern: Tests with > 10 assertions
    explanation: |
      Many assertions in one test create confusion when failures occur.
      Which assertion failed? What does it mean? Single-assertion tests
      (or focused tests) communicate clearly.
    remediation: |
      Focus tests:
      - One logical assertion per test
      - Split large tests
      - Use descriptive test names
      - Group related tests in describe blocks
  - id: SIGNAL-HIGH-002
    signal: Unclear test names
    evidence_pattern: test1, test2, or generic names
    explanation: |
      Test names should describe the behavior being tested. Generic
      names like 'test1' or 'works' don't communicate what failed.
      Good names document expected behavior.
    remediation: |
      Improve test naming:
      - Use 'should' convention
      - Describe behavior, not implementation
      - Include context in name
      - Follow team conventions
  medium:
  - id: SIGNAL-MED-001
    signal: No custom assertion messages
    evidence_pattern: Default assertion messages only
    remediation: Add context to assertions with custom messages
  - id: SIGNAL-MED-002
    signal: Excessive test output
    evidence_pattern: Console spam during tests
    remediation: Reduce verbose logging, mock console in tests
  low:
  - id: SIGNAL-LOW-001
    signal: Inconsistent test style
    evidence_pattern: Mixed conventions across tests
  positive:
  - id: SIGNAL-POS-001
    signal: Clear failure messages
    evidence_pattern: Failures immediately actionable
  - id: SIGNAL-POS-002
    signal: Descriptive test names
    evidence_pattern: Tests document behavior
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Analyze assertion patterns
    description: |
      Review assertion usage and quality.
    duration_estimate: 10 min
    commands:
    - purpose: Count assertions
      command: grep -r -c 'expect\|assert' --include='*.test.*' . 2>/dev/null | grep -v ':0$' | head -10
    - purpose: Check matcher variety
      command: grep -r -h 'expect.*\.' --include='*.test.*' . 2>/dev/null | grep -oE '\.[a-zA-Z]+\(' |
        sort | uniq -c | sort -rn | head -10
    expected_findings:
    - Assertion patterns
    - Matcher usage
  - id: '2'
    name: Review test naming
    description: |
      Assess test name quality.
    duration_estimate: 10 min
    commands:
    - purpose: Check test naming
      command: grep -r -h 'it(\|test(' --include='*.test.*' . 2>/dev/null | head -20
    expected_findings:
    - Naming quality
    - Consistency
  - id: '3'
    name: Examine failure output
    description: |
      Review how failures communicate.
    duration_estimate: 10 min
    questions:
    - Do failures explain what's wrong?
    - Is debugging straightforward?
    - Are error messages helpful?
    expected_findings:
    - Message quality
    - Actionability
  - id: '4'
    name: Assess false positive rate
    description: |
      Estimate noise level in failures.
    duration_estimate: 10 min
    questions:
    - What % of recent failures were real issues?
    - How often do tests fail without code changes?
    - Do developers trust test results?
    expected_findings:
    - False positive rate
    - Trust level
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: signal_report
    format: table
    sections:
    - Assertion Quality
    - Naming Assessment
    - Signal-to-Noise Ratio
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Signal Analysis
    - Improvement Recommendations
  confidence_guidance:
    high: Full failure analysis with metrics
    medium: Pattern analysis
    low: Code review only
offline:
  capability: full
  cache_manifest:
    knowledge:
    - source_id: test-assertions
      priority: recommended
profiles:
  membership:
    quick:
      included: false
      reason: Requires quality analysis
    quality:
      included: true
      priority: 3
    full:
      included: true
      priority: 4
closeout_checklist:
- id: signal-001
  item: Assertion patterns reviewed
  level: CRITICAL
  verification: grep -r 'expect\|assert' --include='*.test.*' . 2>/dev/null | wc -l
  expected: Count documented
- id: signal-002
  item: Test naming assessed
  level: WARNING
  verification: manual
  verification_notes: Review sample of test names
  expected: Quality documented
- id: signal-003
  item: False positive rate estimated
  level: WARNING
  verification: manual
  verification_notes: Estimate from recent failures
  expected: Rate documented
governance:
  applicable_to:
    archetypes:
    - all
  compliance_frameworks:
  - framework: ISO 25010
    controls:
    - Maintainability
    - Usability
relationships:
  commonly_combined:
  - testing-quality-assurance.test-effectiveness.test-flakiness
  - testing-quality-assurance.test-effectiveness.defect-detection-rate
  - testing-quality-assurance.test-effectiveness.false-positive-rate
