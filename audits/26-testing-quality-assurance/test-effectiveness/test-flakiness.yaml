audit:
  id: testing-quality-assurance.test-effectiveness.test-flakiness
  name: Test Flakiness
  version: 1.0.0
  last_updated: '2026-01-22'
  status: active
  category: testing-quality-assurance
  category_number: 26
  subcategory: test-effectiveness
  tier: expert
  estimated_duration: 30-45 min
  completeness: complete
  requires_runtime: false
  destructive: false
execution:
  automatable: 'yes'
  severity: high
  scope: quality
  default_profiles:
  - full
  - quality
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates test flakiness - tests that sometimes pass and sometimes
    fail without code changes. Identifies flaky tests, measures
    flakiness rate, and assesses impact on development velocity
    and test trustworthiness.
  why_it_matters: |
    Flaky tests destroy confidence in the test suite. Developers learn
    to ignore failures, retry until green, or skip testing entirely.
    A single flaky test can waste hours of debugging time across a team.
    Test reliability is essential for testing value.
  when_to_run:
  - CI/CD reliability issues
  - Developer productivity assessment
  - Test suite maintenance
  - Quality improvement
prerequisites:
  required_artifacts:
  - type: test_results
    description: Historical test execution results
  - type: ci_logs
    description: CI pipeline logs
  access_requirements:
  - Test result history
  - CI/CD logs access
discovery:
  file_patterns:
  - glob: '**/*.test.{js,ts}'
    purpose: Test files
  - glob: '**/*.spec.{js,ts}'
    purpose: Spec files
  - glob: '**/test-results/**'
    purpose: Test result artifacts
  code_patterns:
  - pattern: retry|flaky|skip|pending
    type: regex
    scope: test
    purpose: Flakiness indicators
knowledge_sources:
  guides:
  - id: flaky-tests
    name: Dealing with Flaky Tests
    url: https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html
    offline_cache: true
  - id: test-reliability
    name: Test Reliability Engineering
    url: https://martinfowler.com/articles/nonDeterminism.html
    offline_cache: true
  learning_resources:
  - id: flakiness-patterns
    title: Common Causes of Flaky Tests
    type: article
    reference: Testing Anti-Patterns
tooling:
  static_analysis:
  - tool: Jest
    purpose: Test detection and analysis
    offline_capable: true
  - tool: CI flakiness detection
    purpose: Track test reliability
    offline_capable: false
  scripts:
  - id: flakiness-check
    language: bash
    purpose: Analyze test flakiness
    source: inline
    code: |
      #!/bin/bash
      echo "=== Test Flakiness Analysis ==="

      echo "Tests marked as skip/pending:"
      grep -r -c 'skip\|pending\|\.only' --include='*.test.*' --include='*.spec.*' . 2>/dev/null | grep -v ':0$' | head -10

      echo ""
      echo "Retry configurations:"
      grep -r 'retry\|retries\|flaky' --include='*.json' --include='*.js' --include='*.ts' . 2>/dev/null | head -10

      echo ""
      echo "Timeout configurations:"
      grep -r 'timeout\|setTimeout' --include='*.test.*' --include='*.spec.*' . 2>/dev/null | head -10

      echo ""
      echo "Async/await patterns (potential flakiness):"
      grep -r -c 'async\|await\|Promise' --include='*.test.*' . 2>/dev/null | sort -t: -k2 -rn | head -10

      echo ""
      echo "Sleep/wait patterns (flakiness indicator):"
      grep -r -n 'sleep\|\.wait\|waitFor' --include='*.test.*' --include='*.spec.*' . 2>/dev/null | head -10
signals:
  critical:
  - id: FLAKY-CRIT-001
    signal: High flakiness rate
    evidence_pattern: '> 5% of tests are flaky'
    explanation: |
      High flakiness rates make the test suite unreliable. Developers
      can't trust any failure, so they ignore or retry everything.
      CI becomes a slot machine rather than a quality gate.
    remediation: |
      Reduce flakiness systematically:
      1. Identify top flaky tests
      2. Quarantine while fixing
      3. Fix root causes (timing, state, resources)
      4. Add monitoring for regression
      5. Establish flakiness SLA
  - id: FLAKY-CRIT-002
    signal: Critical path tests are flaky
    evidence_pattern: Important tests have inconsistent results
    explanation: |
      Flaky critical path tests block deployments or create risk.
      Either teams wait through retries or deploy despite failures.
      Critical tests must be deterministic.
    remediation: |
      Stabilize critical tests:
      - Prioritize by impact
      - Add deterministic waiting
      - Fix resource dependencies
      - Consider test isolation
      - Add retry analysis
  high:
  - id: FLAKY-HIGH-001
    signal: Retry-until-green culture
    evidence_pattern: Team re-runs CI until it passes
    explanation: |
      Re-running until green hides flakiness and real failures.
      Each retry wastes CI resources and time. Real issues get
      ignored as assumed flakiness.
    remediation: |
      Change the culture:
      - Make flakiness visible
      - Track retry rates
      - Investigate all failures
      - Fix rather than retry
      - Quarantine known flakes
  - id: FLAKY-HIGH-002
    signal: Many tests skipped due to flakiness
    evidence_pattern: Tests marked skip/pending long-term
    explanation: |
      Skipped tests provide no value. Long-term skips indicate
      abandonment. The test was worth writing but not maintaining.
      Either fix or remove.
    remediation: |
      Address skipped tests:
      - Audit all skipped tests
      - Fix fixable tests
      - Remove truly obsolete tests
      - Document skip reasons
      - Set fix deadlines
  medium:
  - id: FLAKY-MED-001
    signal: Time-dependent tests
    evidence_pattern: Tests that fail at certain times
    remediation: Use fixed timestamps or proper time mocking
  - id: FLAKY-MED-002
    signal: Order-dependent tests
    evidence_pattern: Tests fail when run individually
    remediation: Ensure each test is independent
  low:
  - id: FLAKY-LOW-001
    signal: Flakiness not tracked
    evidence_pattern: No flakiness metrics
  positive:
  - id: FLAKY-POS-001
    signal: Very low flakiness rate
    evidence_pattern: < 1% flaky tests
  - id: FLAKY-POS-002
    signal: Flakiness actively monitored
    evidence_pattern: Dashboard tracking test reliability
procedure:
  context:
    cognitive_mode: analytical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Identify flaky indicators
    description: |
      Find patterns that indicate flakiness.
    duration_estimate: 10 min
    commands:
    - purpose: Find skipped tests
      command: grep -r -c 'skip\|pending\|.only' --include='*.test.*' . 2>/dev/null | grep -v ':0$' |
        head -10
    - purpose: Find retry configs
      command: grep -r 'retry\|retries' --include='*.json' . 2>/dev/null | head -5
    expected_findings:
    - Skip/pending counts
    - Retry configuration
  - id: '2'
    name: Analyze timing patterns
    description: |
      Look for timing-related flakiness causes.
    duration_estimate: 10 min
    commands:
    - purpose: Find sleep/wait
      command: grep -r -n 'sleep\|.wait\|setTimeout' --include='*.test.*' . 2>/dev/null | head -10
    expected_findings:
    - Timing patterns
    - Wait usage
  - id: '3'
    name: Review CI results
    description: |
      Examine CI failure patterns.
    duration_estimate: 10 min
    questions:
    - Which tests fail most often?
    - Do failures cluster by time or resource?
    - How often is CI retried?
    expected_findings:
    - Failure frequency
    - Retry rate
  - id: '4'
    name: Calculate flakiness rate
    description: |
      Determine overall test reliability.
    duration_estimate: 10 min
    questions:
    - What percentage of tests are flaky?
    - How is flakiness trending?
    - What's the impact on velocity?
    expected_findings:
    - Flakiness rate
    - Trend analysis
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: flakiness_report
    format: table
    sections:
    - Flaky Tests
    - Flakiness Patterns
    - Impact Assessment
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Flakiness Analysis
    - Remediation Plan
  confidence_guidance:
    high: Full CI history with statistical analysis
    medium: Pattern analysis and sampling
    low: Code pattern detection only
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: flaky-tests
      priority: required
profiles:
  membership:
    quick:
      included: true
      reason: Quick pattern detection possible
    quality:
      included: true
      priority: 1
    full:
      included: true
      priority: 2
closeout_checklist:
- id: flaky-001
  item: Flakiness indicators identified
  level: CRITICAL
  verification: grep -r 'skip\|retry' --include='*.test.*' . 2>/dev/null | wc -l
  expected: Count documented
- id: flaky-002
  item: Flaky tests listed
  level: BLOCKING
  verification: manual
  verification_notes: Document known flaky tests
  expected: List created
- id: flaky-003
  item: Flakiness rate calculated
  level: WARNING
  verification: manual
  verification_notes: Calculate from CI data if available
  expected: Rate documented
governance:
  applicable_to:
    archetypes:
    - all
  compliance_frameworks:
  - framework: ISO 25010
    controls:
    - Reliability
    - Maintainability
relationships:
  commonly_combined:
  - testing-quality-assurance.test-data-management.test-data-isolation
  - testing-quality-assurance.test-environment.environment-stability
  - testing-quality-assurance.test-effectiveness.test-maintenance-burden
