audit:
  id: testing-quality-assurance.test-data-management.test-data-strategy
  name: Test Data Strategy
  version: 1.0.0
  last_updated: '2026-01-22'
  status: active
  category: testing-quality-assurance
  category_number: 26
  subcategory: test-data-management
  tier: expert
  estimated_duration: 45-60 min
  completeness: complete
  requires_runtime: false
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: quality
  default_profiles:
  - full
  - quality
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the test data strategy including documentation, tooling,
    generation approaches, maintenance procedures, and governance.
    Assesses whether test data practices support reliable, repeatable
    testing across all environments.
  why_it_matters: |
    Poor test data management causes flaky tests, false positives, and
    gaps in coverage. Without a clear strategy, teams waste time debugging
    data issues instead of actual defects. Good test data enables confident,
    deterministic testing.
  when_to_run:
  - Testing process assessment
  - Quality improvement initiative
  - New project setup
  - Environment scaling
prerequisites:
  required_artifacts:
  - type: test_configuration
    description: Test setup and configuration files
  - type: documentation
    description: Test data documentation if available
  access_requirements:
  - Test configuration access
  - Data management tool access
discovery:
  file_patterns:
  - glob: '**/fixtures/**'
    purpose: Test fixtures
  - glob: '**/factories/**'
    purpose: Data factories
  - glob: '**/seeds/**'
    purpose: Seed data
  - glob: '**/__fixtures__/**'
    purpose: Jest fixtures
  code_patterns:
  - pattern: Factory|Faker|fixture|seed
    type: regex
    scope: test
    purpose: Data generation patterns
knowledge_sources:
  guides:
  - id: test-data-guide
    name: Test Data Management Best Practices
    url: https://martinfowler.com/articles/testing-data.html
    offline_cache: true
  - id: synthetic-data
    name: Synthetic Data Generation
    url: https://faker.readthedocs.io/
    offline_cache: true
  learning_resources:
  - id: data-management
    title: Test Data Management
    type: article
    reference: Software Testing Best Practices
tooling:
  static_analysis:
  - tool: Custom analysis
    purpose: Test data pattern detection
    offline_capable: true
  scripts:
  - id: test-data-check
    language: bash
    purpose: Analyze test data patterns
    source: inline
    code: |
      #!/bin/bash
      echo "=== Test Data Strategy Analysis ==="

      echo "Fixture directories:"
      find . -type d -name 'fixtures' -o -name '__fixtures__' -o -name 'factories' 2>/dev/null | head -10

      echo ""
      echo "Seed files:"
      find . -name '*seed*' -o -name '*fixture*' 2>/dev/null | grep -E '\.(js|ts|py|json)$' | head -15

      echo ""
      echo "Factory patterns:"
      grep -r -l 'Factory\|faker\|Faker' --include='*.js' --include='*.ts' --include='*.py' . 2>/dev/null | head -10

      echo ""
      echo "Data generation libraries:"
      grep -E '(faker|factory_boy|fishery|chance)' package.json requirements.txt Gemfile 2>/dev/null
signals:
  critical:
  - id: TESTDATA-CRIT-001
    signal: No test data strategy
    evidence_pattern: Ad-hoc data creation with no consistency
    explanation: |
      Without a test data strategy, each test creates data differently.
      This leads to inconsistent test behavior, maintenance burden, and
      difficulty understanding what data tests need.
    remediation: |
      Establish test data strategy:
      1. Document data requirements by test type
      2. Choose generation approach (fixtures, factories, builders)
      3. Define data lifecycle management
      4. Create shared data utilities
      5. Establish naming conventions
  - id: TESTDATA-CRIT-002
    signal: Production data in test files
    evidence_pattern: Real customer data committed to repository
    explanation: |
      Real production data in tests creates privacy and compliance risks.
      Personal information should never be in source control. This can
      lead to data breaches and regulatory violations.
    remediation: |
      Remove production data:
      1. Scan repository for PII patterns
      2. Remove and replace with synthetic data
      3. Use data anonymization for realistic data
      4. Add pre-commit hooks to prevent PII commits
      5. Document data sourcing requirements
  high:
  - id: TESTDATA-HIGH-001
    signal: Hardcoded test data throughout codebase
    evidence_pattern: Magic values scattered in test files
    explanation: |
      Hardcoded test data makes tests brittle and hard to maintain.
      When data changes, many tests break. Centralized data management
      reduces maintenance burden.
    remediation: |
      Centralize test data:
      - Create shared fixture files
      - Use factory patterns for object creation
      - Define test data constants
      - Reference data by identifier, not value
  - id: TESTDATA-HIGH-002
    signal: No data cleanup between tests
    evidence_pattern: Tests depend on previous test state
    explanation: |
      Tests that don't clean up data create dependencies between tests.
      Test order matters, parallel execution fails, and failures cascade.
      Each test should start with a known state.
    remediation: |
      Implement data cleanup:
      - Use setup/teardown hooks
      - Transaction rollback for database tests
      - Reset fixtures between tests
      - Verify test isolation
  medium:
  - id: TESTDATA-MED-001
    signal: Fixture files not version controlled
    evidence_pattern: Test data stored externally
    remediation: Version control all test data with code
  - id: TESTDATA-MED-002
    signal: No documentation of test data requirements
    evidence_pattern: Unclear what data each test needs
    remediation: Document data requirements for test suites
  low:
  - id: TESTDATA-LOW-001
    signal: Test data not representative
    evidence_pattern: Overly simple or unrealistic data
  positive:
  - id: TESTDATA-POS-001
    signal: Comprehensive test data strategy
    evidence_pattern: Documented approach with factories and fixtures
  - id: TESTDATA-POS-002
    signal: Automated data generation
    evidence_pattern: Factories/builders used consistently
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Identify data patterns
    description: |
      Find test data management patterns in the codebase.
    duration_estimate: 15 min
    commands:
    - purpose: Find fixtures
      command: find . -type d \( -name 'fixtures' -o -name 'factories' \) 2>/dev/null | head -10
    - purpose: Find factory usage
      command: grep -r -l 'Factory\|faker' --include='*.js' --include='*.ts' . 2>/dev/null | head -10
    expected_findings:
    - Data management approach
    - Tool usage
  - id: '2'
    name: Review data organization
    description: |
      Assess how test data is organized and maintained.
    duration_estimate: 15 min
    commands:
    - purpose: Count fixture files
      command: find . -name '*fixture*' -o -name '*seed*' 2>/dev/null | wc -l
    expected_findings:
    - Data organization
    - Consistency level
  - id: '3'
    name: Check for sensitive data
    description: |
      Scan for potential PII or production data.
    duration_estimate: 10 min
    commands:
    - purpose: Check for email patterns
      command: grep -r -E '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' --include='*.json' . 2>/dev/null
        | grep -v 'example\|test\|fake' | head -10
    expected_findings:
    - PII presence
    - Data sourcing
  - id: '4'
    name: Evaluate strategy documentation
    description: |
      Review documentation of test data approach.
    duration_estimate: 10 min
    questions:
    - Is test data strategy documented?
    - Are data requirements defined per test type?
    - Is data lifecycle managed?
    expected_findings:
    - Documentation status
    - Governance level
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: strategy_report
    format: table
    sections:
    - Data Patterns
    - Organization
    - Recommendations
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Strategy Assessment
    - Improvement Plan
  confidence_guidance:
    high: Full strategy review with documentation
    medium: Code pattern analysis
    low: Limited file sampling
offline:
  capability: full
  cache_manifest:
    knowledge:
    - source_id: test-data-guide
      priority: required
profiles:
  membership:
    quick:
      included: false
      reason: Requires detailed analysis
    quality:
      included: true
      priority: 4
    full:
      included: true
      priority: 5
closeout_checklist:
- id: testdata-001
  item: Data patterns identified
  level: CRITICAL
  verification: find . -type d -name 'fixtures' -o -name 'factories' 2>/dev/null | wc -l
  expected: Count documented
- id: testdata-002
  item: PII scan completed
  level: BLOCKING
  verification: manual
  verification_notes: Confirm no production data in tests
  expected: No PII found
- id: testdata-003
  item: Strategy documented
  level: WARNING
  verification: manual
  verification_notes: Verify test data approach is documented
  expected: Documentation exists
governance:
  applicable_to:
    archetypes:
    - all
  compliance_frameworks:
  - framework: GDPR
    controls:
    - Art. 32
  - framework: SOC2
    controls:
    - CC6.1
relationships:
  commonly_combined:
  - testing-quality-assurance.test-data-management.test-data-generation
  - testing-quality-assurance.test-data-management.test-data-privacy
  - testing-quality-assurance.test-environment.environment-data
