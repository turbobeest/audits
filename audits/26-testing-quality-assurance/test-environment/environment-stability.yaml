audit:
  id: testing-quality-assurance.test-environment.environment-stability
  name: Environment Stability
  version: 1.0.0
  last_updated: '2026-01-22'
  status: active
  category: testing-quality-assurance
  category_number: 26
  subcategory: test-environment
  tier: expert
  estimated_duration: 30-45 min
  completeness: complete
  requires_runtime: false
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: infrastructure
  default_profiles:
  - full
  - quality
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the stability and reliability of test environments including
    uptime, resource availability, configuration consistency, and recovery
    from failures. Assesses whether environments provide stable testing
    conditions.
  why_it_matters: |
    Unstable test environments cause flaky tests and wasted debugging time.
    When environments crash, tests fail for infrastructure reasons, not
    code issues. Stable environments enable reliable test results and
    developer confidence.
  when_to_run:
  - Test flakiness investigation
  - Environment health check
  - Post-incident review
  - Quality assessment
prerequisites:
  required_artifacts:
  - type: monitoring_config
    description: Environment monitoring setup
  - type: incident_logs
    description: Environment incident history
  access_requirements:
  - Monitoring dashboard access
  - Environment logs access
discovery:
  file_patterns:
  - glob: '**/monitoring/**'
    purpose: Monitoring configuration
  - glob: '**/healthcheck*'
    purpose: Health check definitions
  code_patterns:
  - pattern: healthcheck|liveness|readiness
    type: regex
    scope: config
    purpose: Health check patterns
knowledge_sources:
  guides:
  - id: container-health
    name: Container Health Checks
    url: https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck
    offline_cache: true
  - id: k8s-probes
    name: Kubernetes Probes
    url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
    offline_cache: true
  learning_resources:
  - id: site-reliability
    title: Site Reliability Engineering
    type: book
    reference: ISBN 978-1491929124
tooling:
  infrastructure_tools:
  - tool: Docker Compose Health Checks
    purpose: Container health monitoring
    offline_capable: true
  - tool: Prometheus
    purpose: Metrics collection
    offline_capable: true
  scripts:
  - id: stability-check
    language: bash
    purpose: Analyze environment stability
    source: inline
    code: |
      #!/bin/bash
      echo "=== Environment Stability Analysis ==="

      echo "Health check configurations:"
      grep -r 'healthcheck\|health_check' --include='*.yml' --include='*.yaml' . 2>/dev/null | head -10

      echo ""
      echo "Liveness/readiness probes:"
      grep -r 'livenessProbe\|readinessProbe' --include='*.yaml' . 2>/dev/null | head -10

      echo ""
      echo "Restart policies:"
      grep -r 'restart:' --include='*.yml' . 2>/dev/null | head -10

      echo ""
      echo "Resource limits:"
      grep -r -A3 'resources:\|limits:' --include='*.yml' --include='*.yaml' . 2>/dev/null | head -15

      echo ""
      echo "Retry configurations:"
      grep -r 'retry\|backoff' --include='*.yml' . 2>/dev/null | head -5
signals:
  critical:
  - id: STABILITY-CRIT-001
    signal: Frequent environment crashes
    evidence_pattern: Environment down multiple times per week
    explanation: |
      Frequently crashing environments block testing and erode trust
      in test results. Developers assume test failures are environment
      issues rather than real bugs. Stability is foundational.
    remediation: |
      Stabilize environments:
      1. Identify crash root causes
      2. Add resource limits
      3. Implement health checks
      4. Add automatic recovery
      5. Monitor and alert on issues
  - id: STABILITY-CRIT-002
    signal: No health monitoring
    evidence_pattern: No health checks or monitoring
    explanation: |
      Without monitoring, environment issues go undetected until tests
      fail. Proactive health checks identify problems before they
      impact testing. Monitoring enables quick recovery.
    remediation: |
      Add health monitoring:
      - Implement health check endpoints
      - Add container health checks
      - Set up monitoring dashboards
      - Configure alerts for issues
  high:
  - id: STABILITY-HIGH-001
    signal: No automatic recovery
    evidence_pattern: Manual intervention required for recovery
    explanation: |
      Manual recovery creates bottlenecks and delays. When issues occur
      outside business hours, environments stay down. Automatic recovery
      minimizes downtime and maintains testing capability.
    remediation: |
      Enable automatic recovery:
      - Configure restart policies
      - Add readiness/liveness probes
      - Implement circuit breakers
      - Automate common fixes
  - id: STABILITY-HIGH-002
    signal: Resource exhaustion issues
    evidence_pattern: OOM kills or disk space issues
    explanation: |
      Resource exhaustion causes unpredictable failures. Tests may pass
      or fail depending on resource availability. Proper resource
      management ensures consistent behavior.
    remediation: |
      Manage resources:
      - Set resource limits
      - Monitor resource usage
      - Clean up unused resources
      - Add resource alerts
  medium:
  - id: STABILITY-MED-001
    signal: Long environment startup time
    evidence_pattern: Environment takes > 10 minutes to stabilize
    remediation: Optimize startup, add readiness checks
  - id: STABILITY-MED-002
    signal: Intermittent connectivity issues
    evidence_pattern: Random network failures in tests
    remediation: Improve network configuration, add retry logic
  low:
  - id: STABILITY-LOW-001
    signal: No incident history tracking
    evidence_pattern: Past issues not documented
  positive:
  - id: STABILITY-POS-001
    signal: High environment uptime
    evidence_pattern: '> 99% uptime for test environments'
  - id: STABILITY-POS-002
    signal: Automatic healing enabled
    evidence_pattern: Self-healing infrastructure
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Check health configurations
    description: |
      Find health check and monitoring setups.
    duration_estimate: 10 min
    commands:
    - purpose: Find health checks
      command: grep -r 'healthcheck\|liveness' --include='*.yml' . 2>/dev/null | head -10
    - purpose: Find restart policies
      command: grep -r 'restart:' --include='*.yml' . 2>/dev/null | head -10
    expected_findings:
    - Health check coverage
    - Recovery configuration
  - id: '2'
    name: Review resource configuration
    description: |
      Check resource limits and management.
    duration_estimate: 10 min
    commands:
    - purpose: Find resource limits
      command: grep -r -A3 'resources:\|limits:' --include='*.yml' . 2>/dev/null | head -15
    expected_findings:
    - Resource limits
    - Constraint configuration
  - id: '3'
    name: Assess incident history
    description: |
      Review past environment issues.
    duration_estimate: 10 min
    questions:
    - How often do environments fail?
    - What are common failure modes?
    - How long does recovery take?
    expected_findings:
    - Incident frequency
    - Common issues
  - id: '4'
    name: Verify monitoring
    description: |
      Check monitoring and alerting setup.
    duration_estimate: 10 min
    questions:
    - Is environment health monitored?
    - Are alerts configured?
    - What metrics are tracked?
    expected_findings:
    - Monitoring coverage
    - Alert configuration
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: stability_report
    format: table
    sections:
    - Health Configuration
    - Resource Management
    - Incident Summary
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Stability Analysis
    - Recommendations
  confidence_guidance:
    high: Full monitoring review with incident history
    medium: Configuration analysis
    low: Pattern detection only
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: container-health
      priority: recommended
profiles:
  membership:
    quick:
      included: false
      reason: Requires operational data
    quality:
      included: true
      priority: 3
    full:
      included: true
      priority: 4
closeout_checklist:
- id: stability-001
  item: Health checks identified
  level: CRITICAL
  verification: grep -r 'healthcheck\|liveness' --include='*.yml' . 2>/dev/null | wc -l
  expected: Count > 0
- id: stability-002
  item: Resource limits verified
  level: WARNING
  verification: grep -r 'limits:' --include='*.yml' . 2>/dev/null | wc -l
  expected: Count documented
- id: stability-003
  item: Incident history reviewed
  level: WARNING
  verification: manual
  verification_notes: Document recent environment issues
  expected: Issues documented
governance:
  applicable_to:
    archetypes:
    - all
  compliance_frameworks:
  - framework: ISO 25010
    controls:
    - Reliability
  - framework: SOC2
    controls:
    - CC7.1
relationships:
  commonly_combined:
  - testing-quality-assurance.test-environment.environment-parity
  - testing-quality-assurance.test-effectiveness.test-flakiness
  - observability-monitoring.application-monitoring.health-checks
