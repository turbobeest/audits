audit:
  id: documentation-knowledge.architecture-docs.data-flow-documentation
  name: Data Flow Documentation Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: documentation-knowledge
  category_number: 27
  subcategory: architecture-docs
  tier: expert
  estimated_duration: 3-5 hours  # median: 4h
  completeness: complete
  requires_runtime: false
  destructive: false
execution:
  automatable: partial
  severity: medium
  scope: documentation
  default_profiles:
  - full
  blocks_phase: false
  parallelizable: true
description:
  what: |
    This audit evaluates documentation of how data flows through the system,
    including data sources, transformations, storage, and destinations.
    The audit examines data flow diagrams, data lineage documentation,
    and data lifecycle documentation.
  why_it_matters: |
    Understanding data flow is essential for debugging, security analysis,
    compliance, and system evolution. Without data flow documentation,
    teams struggle to trace data issues, assess privacy impact, or
    understand how changes affect downstream systems.
  when_to_run:
  - During data architecture reviews
  - When preparing for privacy compliance
  - After adding new data integrations
  - When investigating data quality issues
prerequisites:
  required_artifacts:
  - type: data_flow_docs
    description: Access to data flow documentation
  - type: system_architecture
    description: Understanding of system components
  access_requirements:
  - Read access to architecture documentation
  - Read access to data schemas
discovery:
  file_patterns:
  - glob: '**/docs/data/**/*.md'
    purpose: Find data documentation
  - glob: '**/data-flow*.md'
    purpose: Find data flow docs
  - glob: '**/docs/architecture/*data*'
    purpose: Find data architecture docs
  - glob: '**/diagrams/*data*'
    purpose: Find data flow diagrams
  - glob: '**/schemas/**/*.json'
    purpose: Find data schemas
knowledge_sources:
  guides:
  - id: data-flow-diagrams
    name: Data Flow Diagram Notation
    relevance: Standard for documenting data flows
  - id: data-lineage
    name: Data Lineage Best Practices
    relevance: Tracking data origin and transformations
tooling:
  analysis_commands:
  - purpose: Find data documentation
    command: find . -name '*data*' | grep -E '\.(md|txt|puml)$' | head -15
  - purpose: Find data schemas
    command: find . -name '*.schema.json' -o -name '*schema*.yaml' | head -10
  - purpose: Find ETL documentation
    command: find . -name '*etl*' -o -name '*pipeline*' | grep -E '\.(md|txt)$'
signals:
  critical:
  - id: DATAFLOW-CRIT-001
    signal: No data flow documentation exists
    evidence_indicators:
    - No data flow diagrams
    - Data movement undocumented
    - Teams don't know where data goes
    explanation: |
      Without data flow documentation, it's impossible to understand
      how data moves through the system, impacting debugging, security,
      and compliance.
    remediation: Create data flow documentation for major data paths
  - id: DATAFLOW-CRIT-002
    signal: PII/sensitive data flows undocumented
    evidence_indicators:
    - Sensitive data paths unknown
    - Privacy impact assessment impossible
    - Compliance risk unassessed
    explanation: |
      Undocumented sensitive data flows create significant compliance
      and security risks.
    remediation: Document all PII and sensitive data flows immediately
  high:
  - id: DATAFLOW-HIGH-001
    signal: Data sources not documented
    evidence_indicators:
    - Origin of data unknown
    - Data quality unmeasurable
    - Upstream dependencies unclear
    explanation: |
      Without documented data sources, it's difficult to assess
      data quality or understand upstream impacts.
    remediation: Document all data sources and their characteristics
  - id: DATAFLOW-HIGH-002
    signal: Data transformations undocumented
    evidence_indicators:
    - Business logic in transformations unknown
    - Data derivation unclear
    - Calculation logic undocumented
    explanation: |
      Undocumented transformations make it impossible to understand
      how derived data is calculated.
    remediation: Document data transformation logic
  - id: DATAFLOW-HIGH-003
    signal: Data storage locations not mapped
    evidence_indicators:
    - Data residence unknown
    - Storage systems not documented
    - Retention unclear
    explanation: |
      Without storage documentation, compliance with data residency
      and retention requirements is impossible to verify.
    remediation: Map all data storage locations and retention
  medium:
  - id: DATAFLOW-MED-001
    signal: Data flow diagrams outdated
    explanation: |
      Outdated diagrams lead to incorrect assumptions about data
      movement and dependencies.
    remediation: Update data flow diagrams regularly
  - id: DATAFLOW-MED-002
    signal: Data schemas not documented
    remediation: Document data schemas with field descriptions
  - id: DATAFLOW-MED-003
    signal: Data quality expectations not documented
    remediation: Document data quality requirements and validation
  low:
  - id: DATAFLOW-LOW-001
    signal: Data lineage tooling not used
    remediation: Consider data lineage tools for automated tracking
  - id: DATAFLOW-LOW-002
    signal: Data dictionary not maintained
    remediation: Create and maintain data dictionary
  positive:
  - id: DATAFLOW-POS-001
    signal: Comprehensive data flow documentation
  - id: DATAFLOW-POS-002
    signal: Automated data lineage tracking
  - id: DATAFLOW-POS-003
    signal: PII data flows clearly documented
procedure:
  context:
    cognitive_mode: analytical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Inventory Data Documentation
    description: |
      Find all data flow and data architecture documentation.
    duration_estimate: 30 min
    commands:
    - purpose: Find data docs
      command: find . -name '*data*' | grep -E '\.(md|txt)$'
    - purpose: Find data diagrams
      command: find . -name '*data*' | grep -E '\.(puml|drawio|svg|png)$'
    expected_findings:
    - Documentation inventory
    - Diagram inventory
  - id: '2'
    name: Assess Data Flow Coverage
    description: |
      Evaluate whether major data flows are documented.
    duration_estimate: 45 min
    coverage_checklist:
    - User data collection
    - Internal data processing
    - External integrations
    - Reporting and analytics
    - Data archival and deletion
    expected_findings:
    - Flow coverage assessment
    - Undocumented flows
  - id: '3'
    name: Review Diagram Quality
    description: |
      Evaluate data flow diagram quality and completeness.
    duration_estimate: 45 min
    quality_checklist:
    - Sources clearly identified
    - Destinations documented
    - Transformations shown
    - Data types indicated
    - Security boundaries marked
    expected_findings:
    - Diagram quality assessment
    - Missing information
  - id: '4'
    name: Verify Sensitive Data Documentation
    description: |
      Check that sensitive data flows are documented.
    duration_estimate: 45 min
    sensitive_data_checks:
    - PII flows identified
    - Financial data flows documented
    - Healthcare data flows if applicable
    - Data classification applied
    expected_findings:
    - Sensitive data documentation status
    - Compliance gaps
  - id: '5'
    name: Assess Accuracy
    description: |
      Verify documentation matches actual data flows.
    duration_estimate: 45 min
    expected_findings:
    - Accuracy assessment
    - Discrepancies identified
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Data Flow Coverage
    - Sensitive Data Review
    - Accuracy Assessment
    - Recommendations
  confidence_guidance:
    high: Documentation validated against actual data flows
    medium: Documentation reviewed with data team
    low: Documentation existence review
closeout_checklist:
- id: dataflow-001
  item: Data flow documentation exists
  level: CRITICAL
  verification: find . -name '*data-flow*' -o -name '*dataflow*' | head -1
  expected: Documentation present
- id: dataflow-002
  item: PII data flows documented
  level: CRITICAL
  verification: grep -ri 'pii\|personal\|sensitive' docs/data/ 2>/dev/null | head -1
  expected: PII flows documented
- id: dataflow-003
  item: Data sources documented
  level: BLOCKING
  verification: grep -i 'source\|origin' docs/data/*.md 2>/dev/null | head -1
  expected: Sources documented
- id: dataflow-004
  item: Data storage locations mapped
  level: WARNING
  verification: manual
  verification_notes: Check for storage documentation
  expected: Storage locations documented
governance:
  applicable_to:
    archetypes:
    - data-processing
    - all-systems
  compliance_mappings:
  - framework: GDPR
    control: Article 30
    description: Records of processing activities
  - framework: SOC 2
    control: CC6.1
    description: Data flow documentation
relationships:
  commonly_combined:
  - documentation-knowledge.architecture-docs.system-architecture-docs
  - data-governance.data-lineage
  feeds_into:
  - privacy.dpia
  - security.data-classification
