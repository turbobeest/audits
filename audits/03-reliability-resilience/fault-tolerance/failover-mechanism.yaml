audit:
  id: reliability-resilience.fault-tolerance.failover-mechanism
  name: Failover Mechanism Audit
  version: 1.0.0
  last_updated: '2026-01-18'
  status: active
  category: reliability-resilience
  category_number: 3
  subcategory: fault-tolerance
  tier: phd
  estimated_duration: 3 hours
  completeness: complete
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: critical
  scope: infrastructure
  default_profiles:
  - full
  - production
  blocks_phase: false
  parallelizable: true
description:
  what: |
    This audit evaluates failover mechanisms across all critical system
    components. It examines health check configurations, failure detection
    timing, failover triggers, promotion procedures, traffic rerouting,
    data consistency during failover, and recovery time objectives. Both
    automatic and manual failover procedures are assessed.
  why_it_matters: |
    Failover mechanisms determine how quickly and cleanly a system recovers
    from component failures. Poorly configured failover causes extended
    outages (slow detection), data loss (incomplete replication), or
    cascading failures (thundering herd). The difference between 30-second
    and 30-minute recovery directly impacts revenue, user experience, and
    SLA compliance. Untested failover mechanisms frequently fail when needed.
  when_to_run:
  - During disaster recovery planning
  - Before production deployment
  - After infrastructure topology changes
  - Quarterly reliability assessments
  - Post-incident reviews involving failover
prerequisites:
  required_artifacts:
  - type: infrastructure-config
    description: Load balancer and health check configurations
  - type: database-config
    description: Database replication and failover settings
  - type: runbooks
    description: Failover procedures and runbooks
  access_requirements:
  - Read access to health check configurations
  - Read access to database cluster settings
  - Access to failover test results and logs
  - Read access to monitoring and alerting rules
discovery:
  code_patterns:
  - pattern: healthCheck|health_check|livenessProbe|readinessProbe
    type: regex
    scope: config
    purpose: Health check configurations
  - pattern: failover|FailoverPolicy|failover_priority
    type: regex
    scope: config
    purpose: Failover policy settings
  - pattern: promotion_tier|PromotionTier|priority.*failover
    type: regex
    scope: config
    purpose: Failover priority configurations
  - pattern: deregistration_delay|connection_draining
    type: regex
    scope: config
    purpose: Graceful failover settings
  file_patterns:
  - glob: '**/terraform/**/*.tf'
    purpose: Infrastructure failover configurations
  - glob: '**/k8s/**/*.yaml'
    purpose: Kubernetes health and readiness probes
  - glob: '**/runbooks/**/*.md'
    purpose: Failover procedures documentation
  metrics_queries:
  - system: Prometheus
    query: probe_success
    purpose: Health check success rate
    threshold: 100% in steady state
  - system: CloudWatch
    query: AWS/RDS FreeStorageSpace by ReplicaLag
    purpose: Replica readiness for failover
    threshold: ReplicaLag < threshold for promotion
knowledge_sources:
  specifications:
  - id: aws-rds-failover
    name: Amazon RDS Multi-AZ Failover
    url: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html
    offline_cache: true
    priority: required
  guides:
  - id: k8s-probes
    name: Kubernetes Probes
    url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
    offline_cache: true
  - id: postgres-failover
    name: PostgreSQL Automatic Failover
    url: https://www.postgresql.org/docs/current/warm-standby-failover.html
    offline_cache: true
  - id: patroni
    name: Patroni HA PostgreSQL
    url: https://patroni.readthedocs.io/en/latest/
    offline_cache: true
  - id: release-it
    name: Michael Nygard - Release It!
    url: https://pragprog.com/titles/mnee2/release-it-second-edition/
    offline_cache: true
  - id: sre-book
    name: Site Reliability Engineering
    url: https://sre.google/sre-book/table-of-contents/
    offline_cache: true
  learning_resources:
  - id: database-reliability
    title: Database Reliability Engineering
    type: book
    reference: ISBN 978-1491925942
tooling:
  infrastructure_tools:
  - tool: kubectl
    purpose: Check probe configurations
    command: 'kubectl get pods -A -o json | jq ''.items[] | {name: .metadata.name, probes: {liveness:
      .spec.containers[0].livenessProbe, readiness: .spec.containers[0].readinessProbe}}'''
  - tool: aws
    purpose: Check target group health
    command: aws elbv2 describe-target-health --target-group-arn ARN
  - tool: aws
    purpose: Check RDS failover events
    command: aws rds describe-events --source-type db-instance --event-categories failover
  monitoring_queries:
  - system: Prometheus
    query: |
      rate(kube_pod_container_status_restarts_total[1h]) > 0
    purpose: Container restarts indicating health check failures
  - system: CloudWatch
    query: HealthyHostCount, UnHealthyHostCount
    purpose: Load balancer target health status
signals:
  critical:
  - id: FAIL-CRIT-001
    signal: No automatic failover configured for database
    evidence_pattern: automatic_failover.*false|failover.*manual
    explanation: |
      Manual database failover requires human intervention, extending
      outage duration from seconds to minutes or hours. Production
      databases must have automatic failover with < 60 second detection.
      RDS Multi-AZ, Aurora, or Patroni provide automatic failover.
    remediation: Enable automatic failover using RDS Multi-AZ, Aurora, or Patroni-style HA
  - id: FAIL-CRIT-002
    signal: Health checks missing or improperly configured
    evidence_pattern: livenessProbe.*null|healthCheck.*missing
    explanation: |
      Without health checks, failed components are not detected and
      continue receiving traffic. Improperly configured health checks
      (wrong endpoint, too lenient thresholds) mask failures. Every
      production service must have appropriate health checks.
    remediation: Configure liveness and readiness probes for all containers with appropriate thresholds
  - id: FAIL-CRIT-003
    signal: Failover causes data loss
    evidence_pattern: async_replication.*failover|lag.*exceeded
    explanation: |
      Asynchronous replication with failover can lose data that hasn't
      replicated. The replication lag at failover time represents data
      loss. For critical data, synchronous replication or two-phase
      commit is required.
    remediation: Use synchronous replication or implement compensating transactions for data consistency
  high:
  - id: FAIL-HIGH-001
    signal: Failover detection time exceeds RTO
    evidence_pattern: interval.*60s|timeout.*120s
    explanation: |
      Slow health checks delay failure detection. If health check interval
      is 30s and threshold is 3 failures, detection takes 90+ seconds.
      This must align with Recovery Time Objective (RTO) requirements.
    remediation: Tune health check intervals and thresholds to meet RTO
  - id: FAIL-HIGH-002
    signal: No connection draining during failover
    evidence_pattern: deregistration_delay.*0|connection_draining.*disabled
    explanation: |
      Immediate connection termination during failover causes in-flight
      requests to fail. Connection draining allows graceful completion
      of existing requests before removing the target.
    remediation: Configure appropriate deregistration delay (30-300 seconds based on request duration)
  - id: FAIL-HIGH-003
    signal: Failover not tested in production-like environment
    evidence_pattern: last_failover_test.*null|failover_test.*never
    explanation: |
      Untested failover mechanisms frequently fail when needed. Regular
      testing in production-like environments validates configuration
      and reveals unexpected issues. Netflix-style chaos engineering
      provides continuous validation.
    remediation: Implement regular failover testing, ideally automated chaos engineering
  medium:
  - id: FAIL-MED-001
    signal: Failover runbooks incomplete or outdated
    explanation: |
      Manual failover steps require accurate, tested runbooks. Incomplete
      or outdated runbooks cause delays and errors during incidents.
    remediation: Update and test failover runbooks, include in regular drills
  - id: FAIL-MED-002
    signal: DNS TTL too high for failover
    explanation: |
      High DNS TTL values cause clients to continue connecting to failed
      endpoints. TTL should be low enough to support failover timing
      requirements (typically 60-300 seconds).
    remediation: Reduce DNS TTL to support failover timing requirements
  - id: FAIL-MED-003
    signal: Failover alerts not configured or tested
    explanation: |
      Teams must be notified of failover events for investigation and
      potential intervention. Missing or broken alerting delays response.
    remediation: Configure and test failover alerts for all critical components
  low:
  - id: FAIL-LOW-001
    signal: Failover metrics not captured
    explanation: |
      Tracking failover frequency, duration, and cause helps identify
      patterns and improvement opportunities.
  positive:
  - id: FAIL-POS-001
    signal: Automatic failover with < 30 second detection
  - id: FAIL-POS-002
    signal: Regular failover testing documented and passing
  - id: FAIL-POS-003
    signal: Zero data loss during failover verified
procedure:
  context:
    cognitive_mode: critical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Inventory failover configurations
    description: |
      Document all components with failover capabilities and their
      current configurations.
    duration_estimate: 30 min
    commands:
    - purpose: List health check configurations
      command: |
        kubectl get deployments -A -o json | jq '.items[] | {name: .metadata.name, probes: .spec.template.spec.containers[].livenessProbe}'
    - purpose: Check ALB health check settings
      command: |
        aws elbv2 describe-target-groups --query 'TargetGroups[*].{Name:TargetGroupName,HealthCheck:HealthCheckPath,Interval:HealthCheckIntervalSeconds,Threshold:UnhealthyThresholdCount}'
    expected_findings:
    - Health check configurations
    - Failover policy settings
  - id: '2'
    name: Verify failure detection timing
    description: |
      Calculate actual failure detection time based on health check
      intervals, thresholds, and timeout settings.
    duration_estimate: 30 min
    commands:
    - purpose: Calculate detection time
      command: |
        echo "Detection Time = (interval * threshold) + timeout"
    questions:
    - What is the health check interval?
    - How many failed checks trigger failover?
    - What is the health check timeout?
    - Does total time meet RTO?
    expected_findings:
    - Calculated detection times
    - RTO compliance status
  - id: '3'
    name: Assess database failover
    description: |
      Evaluate database failover configuration including replication,
      promotion, and data consistency.
    duration_estimate: 45 min
    commands:
    - purpose: Check RDS Multi-AZ status
      command: |
        aws rds describe-db-instances --query 'DBInstances[*].{Name:DBInstanceIdentifier,MultiAZ:MultiAZ,SecondaryAZ:SecondaryAvailabilityZone}'
    - purpose: Check PostgreSQL replication
      command: |
        psql -c "SELECT client_addr, state, sync_state FROM pg_stat_replication"
    expected_findings:
    - Database failover capability
    - Replication synchronization status
  - id: '4'
    name: Review failover testing history
    description: |
      Examine records of failover tests to verify mechanisms work
      as expected.
    duration_estimate: 30 min
    questions:
    - When was the last failover test?
    - What was the actual failover time?
    - Were there any issues during testing?
    - Is testing automated or manual?
    expected_findings:
    - Failover test history
    - Issues and remediations
  - id: '5'
    name: Evaluate failover impact
    description: |
      Assess what happens during failover: connection handling,
      data consistency, and service continuity.
    duration_estimate: 45 min
    questions:
    - How are in-flight requests handled?
    - Is there potential for data loss?
    - Do clients automatically reconnect?
    - Are sessions preserved?
    expected_findings:
    - Failover impact analysis
    - Data consistency assessment
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Failover Configuration Inventory
    - Detection Timing Analysis
    - Data Consistency Assessment
    - Testing Recommendations
  confidence_guidance:
    high: Failover tested with observed metrics
    medium: Configuration review with historical test data
    low: Configuration review only
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: aws-rds-failover
      priority: required
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive infrastructure analysis
    full:
      included: true
      priority: 1
    production:
      included: true
      priority: 1
closeout_checklist:
- id: fail-001
  item: Automatic failover configured for all stateful services
  level: CRITICAL
  verification: manual
  verification_notes: Verify databases, caches have automatic failover enabled
  expected: Confirmed by reviewer
- id: fail-002
  item: Health checks configured for all services
  level: CRITICAL
  verification: |
    kubectl get deployments -A -o json | jq '[.items[] | select(.spec.template.spec.containers[0].livenessProbe == null)] | length'
  expected: '0'
- id: fail-003
  item: Failover detection time meets RTO
  level: BLOCKING
  verification: manual
  verification_notes: Verify (interval * threshold) + timeout < RTO
  expected: Confirmed by reviewer
- id: fail-004
  item: Failover tested within last 90 days
  level: WARNING
  verification: manual
  verification_notes: Check failover test logs and runbooks
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - all
  compliance_frameworks:
  - framework: SOC 2
    controls:
    - CC7.2
    - CC7.4
  - framework: ISO 27001
    controls:
    - A.17.1.2
relationships:
  commonly_combined:
  - reliability-resilience.fault-tolerance.redundancy
  - reliability-resilience.fault-tolerance.graceful-degradation
  - reliability-resilience.disaster-recovery.rto-rpo
