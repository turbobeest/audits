# ============================================================
# AUDIT: ETL Pipeline Review
# Category: 33 - Legacy & Migration
# Subcategory: data-migration
# ============================================================

audit:
  id: "legacy-migration.data-migration.etl-pipeline-review"
  name: "ETL Pipeline Review Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "legacy-migration"
  category_number: 33
  subcategory: "data-migration"

  tier: "expert"
  estimated_duration: "6-8 hours"  # median: 7h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "data"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Reviews ETL (Extract, Transform, Load) pipelines used for data migration
    including extraction logic, transformation rules, load processes, error
    handling, and performance characteristics. This audit ensures pipelines
    correctly move and transform data while handling edge cases and errors
    appropriately.

  why_it_matters: |
    ETL pipeline defects are a primary cause of data migration failures. Bugs
    in transformation logic, inadequate error handling, or performance issues
    can corrupt data, cause incomplete migrations, or extend migration windows
    beyond acceptable limits.

  when_to_run:
    - "After ETL development"
    - "Before migration execution"
    - "When pipeline issues detected"
    - "During migration rehearsals"

prerequisites:
  required_artifacts:
    - type: "etl_code"
      description: "ETL pipeline source code"
    - type: "transformation_specs"
      description: "Data transformation requirements"
    - type: "test_data"
      description: "Representative test datasets"

  access_requirements:
    - "ETL code repository access"
    - "ETL execution environment"
    - "Source and target data access"
    - "Pipeline monitoring"

discovery:
  file_patterns:
    - glob: "**/etl/**"
      purpose: "Find ETL code"
    - glob: "**/*transform*.{py,sql,scala}"
      purpose: "Find transformation code"
    - glob: "**/airflow/**"
      purpose: "Find Airflow DAGs"
    - glob: "**/dbt/**"
      purpose: "Find dbt transformations"

  interview_questions:
    - role: "Data Engineer"
      questions:
        - "How does the pipeline handle errors?"
        - "What monitoring exists for the pipeline?"
        - "How is the pipeline tested?"
    - role: "ETL Developer"
      questions:
        - "What are the most complex transformations?"
        - "What edge cases have been considered?"
        - "How is idempotency handled?"

knowledge_sources:
  guides:
    - id: "etl-best-practices"
      name: "ETL Design Patterns"
      url: "https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl"
    - id: "dbt-docs"
      name: "dbt Documentation"
      url: "https://docs.getdbt.com/"

  standards:
    - id: "data-engineering"
      name: "Data Engineering Best Practices"
      relevance: "Pipeline design standards"

tooling:
  analysis_tools:
    - tool: "dbt"
      purpose: "Data transformation and testing"
      command: "dbt run && dbt test"
    - tool: "Apache Airflow"
      purpose: "Pipeline orchestration"
    - tool: "Great Expectations"
      purpose: "Data quality validation"
    - tool: "DataGrip"
      purpose: "SQL analysis and debugging"
    - tool: "Apache Spark"
      purpose: "Large-scale ETL processing"

signals:
  critical:
    - id: "EPR-CRIT-001"
      signal: "Pipeline has no error handling"
      evidence_indicators:
        - "Failures not caught or logged"
        - "Partial loads without rollback"
        - "Silent data corruption possible"
      explanation: |
        Without error handling, pipeline failures may go undetected or cause
        partial, inconsistent data states.
      remediation: "Implement comprehensive error handling and alerting"

    - id: "EPR-CRIT-002"
      signal: "Transformation logic produces incorrect results"
      evidence_indicators:
        - "Unit tests failing"
        - "Output doesn't match specification"
        - "Business rules not implemented correctly"
      explanation: |
        Incorrect transformation logic will systematically corrupt migrated data.
      remediation: "Fix transformation logic and verify with comprehensive tests"

  high:
    - id: "EPR-HIGH-001"
      signal: "Pipeline cannot complete within migration window"
      remediation: "Optimize performance or extend window"

    - id: "EPR-HIGH-002"
      signal: "Pipeline not idempotent"
      remediation: "Implement idempotent operations for retry safety"

    - id: "EPR-HIGH-003"
      signal: "Insufficient test coverage for transformations"
      remediation: "Add unit and integration tests"

  medium:
    - id: "EPR-MED-001"
      signal: "Monitoring insufficient for production"
      remediation: "Implement comprehensive monitoring and alerting"

    - id: "EPR-MED-002"
      signal: "Documentation incomplete"
      remediation: "Document pipeline architecture and operations"

  low:
    - id: "EPR-LOW-001"
      signal: "Code style inconsistencies"
      remediation: "Apply consistent coding standards"

    - id: "EPR-LOW-002"
      signal: "Minor optimization opportunities"
      remediation: "Optimize in subsequent iterations"

  positive:
    - id: "EPR-POS-001"
      signal: "Pipeline well-tested with comprehensive coverage"

    - id: "EPR-POS-002"
      signal: "Robust error handling and monitoring in place"

procedure:
  context:
    cognitive_mode: "analytical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Pipeline Architecture"
      description: "Understand overall pipeline design and flow"
      duration_estimate: "1 hour"

    - id: "2"
      name: "Analyze Extraction Logic"
      description: "Review data extraction implementation"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Review Transformations"
      description: "Verify transformation logic against requirements"
      duration_estimate: "2 hours"

    - id: "4"
      name: "Evaluate Error Handling"
      description: "Test error scenarios and recovery"
      duration_estimate: "1 hour"

    - id: "5"
      name: "Assess Performance"
      description: "Validate performance characteristics"
      duration_estimate: "1 hour"

    - id: "6"
      name: "Review Tests"
      description: "Evaluate test coverage and quality"
      duration_estimate: "1 hour"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Architecture Review"
        - "Transformation Analysis"
        - "Error Handling Assessment"
        - "Performance Evaluation"
        - "Recommendations"

closeout_checklist:
  - id: "epr-001"
    item: "All transformations verified"
    level: "WARNING"
    verification: "tool_output"
  - id: "epr-002"
    item: "Error handling tested"
    level: "WARNING"
    verification: "manual"
  - id: "epr-003"
    item: "Performance acceptable"
    level: "WARNING"
    verification: "tool_output"

governance:
  applicable_to:
    archetypes: ["data-migrations", "etl-heavy-projects"]

  compliance_mappings:
    - framework: "SOX"
      control: "Data Processing Controls"
      description: "Data transformation accuracy"
    - framework: "ISO 8000"
      control: "Data Quality"
      description: "Data processing quality"

relationships:
  commonly_combined:
    - "legacy-migration.data-migration.data-validation"
    - "data-quality.data-profiling"
  depends_on:
    - "legacy-migration.data-migration.schema-compatibility"
  feeds_into:
    - "legacy-migration.data-migration.migration-verification"
