audit:
  id: legacy-migration.data-migration.data-quality-assessment
  name: Pre-Migration Data Quality Assessment Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: legacy-migration
  category_number: 33
  subcategory: data-migration
  tier: expert
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: full
  severity: high
  scope: data
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Assesses the quality of source data before migration to identify issues
    that could affect migration success including duplicates, inconsistencies,
    invalid values, missing data, and orphaned records. This audit profiles
    source data to understand quality issues that must be addressed during
    or before migration.
  why_it_matters: |
    Migrating poor quality data just moves the problem to the new system.
    Understanding data quality before migration enables decisions about
    cleansing during migration, helps set realistic expectations, and prevents
    surprises when issues surface post-migration.
  when_to_run:
  - Before migration planning
  - During scope definition
  - When quality issues are suspected
  - As part of data inventory
prerequisites:
  required_artifacts:
  - type: source_data_access
    description: Query access to source data
  - type: data_dictionary
    description: Expected data definitions
  - type: business_rules
    description: Data validation rules
  access_requirements:
  - Read access to source database
  - Query execution privileges
  - Data profiling tool access
discovery:
  file_patterns:
  - glob: '**/data-quality*'
    purpose: Find data quality rules
  - glob: '**/validation-rules*'
    purpose: Find validation definitions
  - glob: '**/data-dictionary*'
    purpose: Find data definitions
  interview_questions:
  - role: Data Steward
    questions:
    - What are the known data quality issues?
    - What validation rules should data satisfy?
    - What data cleansing has been attempted before?
  - role: Business Analyst
    questions:
    - What data quality impacts business operations?
    - What data accuracy is expected?
    - Are there duplicate handling requirements?
knowledge_sources:
  guides:
  - id: data-profiling
    name: Data Profiling Best Practices
    url: https://www.informatica.com/resources/articles/what-is-data-profiling.html
  - id: data-quality-dimensions
    name: Data Quality Dimensions
    url: https://www.dama.org/cpages/dama-dmbok
  standards:
  - id: iso-8000
    name: ISO 8000 Data Quality
    relevance: Data quality standards
  - id: dama-dmbok
    name: DAMA Data Management Body of Knowledge
    relevance: Data quality management
tooling:
  analysis_tools:
  - tool: Great Expectations
    purpose: Data quality profiling
    command: great_expectations profile source_data
  - tool: Apache Griffin
    purpose: Data quality measurement
  - tool: Ataccama
    purpose: Enterprise data quality
  - tool: pandas-profiling
    purpose: Python data profiling
    command: ydata-profiling report
  - tool: OpenRefine
    purpose: Data cleansing and transformation
signals:
  critical:
  - id: DQA-CRIT-001
    signal: Critical data fields have >10% invalid values
    evidence_indicators:
    - Primary key violations
    - Required fields with NULL values
    - Foreign key orphans
    explanation: |
      High rates of invalid data in critical fields will cause migration
      failures or result in a migrated system with severe data quality issues.
    remediation: Plan data cleansing before or during migration
  - id: DQA-CRIT-002
    signal: Duplicate primary business entities
    evidence_indicators:
    - Duplicate customer records
    - Duplicate transaction entries
    - Multiple records for same entity
    explanation: |
      Duplicates will be migrated and continue causing business problems
      unless addressed during migration.
    remediation: Implement deduplication strategy for migration
  high:
  - id: DQA-HIGH-001
    signal: Significant data inconsistencies between tables
    remediation: Resolve inconsistencies before migration
  - id: DQA-HIGH-002
    signal: Date/time data quality issues
    remediation: Define date handling strategy for migration
  - id: DQA-HIGH-003
    signal: Character encoding issues in source data
    remediation: Plan encoding normalization during migration
  medium:
  - id: DQA-MED-001
    signal: Data format inconsistencies
    remediation: Normalize formats during transformation
  - id: DQA-MED-002
    signal: Historical data quality lower than recent
    remediation: Consider differential handling of historical data
  low:
  - id: DQA-LOW-001
    signal: Minor data quality issues in non-critical fields
    remediation: Address as part of ongoing data quality program
  - id: DQA-LOW-002
    signal: Metadata inconsistencies
    remediation: Update metadata during migration
  positive:
  - id: DQA-POS-001
    signal: Source data quality high with few issues
  - id: DQA-POS-002
    signal: Known issues have documented remediation plans
procedure:
  context:
    cognitive_mode: analytical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Profile Data Structure
    description: Analyze table and column characteristics
    duration_estimate: 1 hour
  - id: '2'
    name: Check Completeness
    description: Identify NULL and missing values
    duration_estimate: 1 hour
  - id: '3'
    name: Validate Consistency
    description: Check cross-table consistency
    duration_estimate: 1 hour
  - id: '4'
    name: Identify Duplicates
    description: Find duplicate records
    duration_estimate: 1 hour
  - id: '5'
    name: Verify Business Rules
    description: Check data against business rules
    duration_estimate: 1 hour
  - id: '6'
    name: Document Findings
    description: Create data quality report
    duration_estimate: 1 hour
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Data Profile Summary
    - Quality Metrics
    - Issue Inventory
    - Remediation Recommendations
closeout_checklist:
- id: dqa-001
  item: All critical tables profiled
  level: WARNING
  verification: tool_output
- id: dqa-002
  item: Quality issues catalogued
  level: WARNING
  verification: manual
- id: dqa-003
  item: Remediation strategies defined
  level: WARNING
  verification: manual
governance:
  applicable_to:
    archetypes:
    - data-migrations
    - data-intensive-systems
  compliance_mappings:
  - framework: ISO 8000
    control: Data Quality
    description: Data quality assessment
  - framework: GDPR
    control: Article 5
    description: Data accuracy
relationships:
  commonly_combined:
  - data-quality.data-profiling
  - legacy-migration.data-migration.schema-compatibility
  depends_on:
  - inventory.data-inventory
  feeds_into:
  - legacy-migration.data-migration.etl-pipeline-review
  - legacy-migration.migration-planning.scope-definition
