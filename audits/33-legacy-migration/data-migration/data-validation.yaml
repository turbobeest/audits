audit:
  id: legacy-migration.data-migration.data-validation
  name: Data Migration Validation Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: legacy-migration
  category_number: 33
  subcategory: data-migration
  tier: phd
  estimated_duration: 6-10 hours  # median: 8h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: critical
  scope: data
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Validates that migrated data matches source data through row counts,
    checksums, sample comparisons, and business rule verification. This audit
    ensures data integrity, completeness, and correctness after migration by
    systematically comparing source and target data at multiple levels.
  why_it_matters: |
    Data corruption and loss during migration often goes undetected until it
    causes business problems. Thorough validation catches issues early when
    they can be corrected. Without validation, organizations operate on
    potentially corrupt data with no awareness of the problem.
  when_to_run:
  - After each migration iteration
  - Before production cutover
  - After data synchronization
  - During migration acceptance testing
prerequisites:
  required_artifacts:
  - type: source_data_access
    description: Query access to source database
  - type: target_data_access
    description: Query access to target database
  - type: validation_criteria
    description: Business rules for data validation
  access_requirements:
  - Read access to source database
  - Read access to target database
  - Sufficient query privileges
  - Access to validation tools
discovery:
  file_patterns:
  - glob: '**/validation*'
    purpose: Find validation scripts
  - glob: '**/data-quality*'
    purpose: Find data quality rules
  - glob: '**/test-data*'
    purpose: Find test data definitions
  interview_questions:
  - role: Data Engineer
    questions:
    - What validation checks are already in place?
    - What are the known data quality issues?
    - What business rules must data satisfy?
  - role: Business Analyst
    questions:
    - What data accuracy is critical for business operations?
    - How would you verify the data looks correct?
    - What reports should produce identical results?
knowledge_sources:
  guides:
  - id: data-validation
    name: Data Migration Validation Strategies
    url: https://www.informatica.com/blogs/data-migration-validation.html
  - id: great-expectations
    name: Great Expectations Documentation
    url: https://docs.greatexpectations.io/
  standards:
  - id: iso-8000
    name: ISO 8000 Data Quality
    relevance: Data quality verification
tooling:
  analysis_tools:
  - tool: Great Expectations
    purpose: Data validation framework
    command: great_expectations checkpoint run migration_validation
  - tool: dbt tests
    purpose: Data transformation testing
    command: dbt test --select migration_checks
  - tool: dbForge Data Compare
    purpose: Database comparison
  - tool: SQL Data Compare
    purpose: Row-level data comparison
  - tool: Apache Griffin
    purpose: Data quality monitoring
signals:
  critical:
  - id: DMV-CRIT-001
    signal: Row count mismatch between source and target
    evidence_indicators:
    - Target has fewer rows than source
    - Unexpected orphan records
    - Missing critical records
    explanation: |
      Row count differences indicate data loss or duplication, both of
      which compromise data integrity and business operations.
    remediation: Investigate missing/extra records and re-migrate
  - id: DMV-CRIT-002
    signal: Data corruption detected in migrated data
    evidence_indicators:
    - Checksum mismatches on critical tables
    - Character encoding errors
    - Truncated or altered values
    explanation: |
      Corrupted data will cause application errors and incorrect business
      decisions.
    remediation: Identify corruption source and re-migrate affected data
  high:
  - id: DMV-HIGH-001
    signal: Referential integrity violations in target
    remediation: Fix foreign key relationships or adjust migration order
  - id: DMV-HIGH-002
    signal: Business rule violations in migrated data
    remediation: Apply data cleansing or transformation rules
  - id: DMV-HIGH-003
    signal: NULL values where data expected
    remediation: Investigate data transformation or mapping issues
  medium:
  - id: DMV-MED-001
    signal: Minor data format differences
    remediation: Normalize formats or update applications
  - id: DMV-MED-002
    signal: Historical data differences acceptable per policy
    remediation: Document and accept with approval
  low:
  - id: DMV-LOW-001
    signal: Metadata differences between systems
    remediation: Update metadata in target system
  - id: DMV-LOW-002
    signal: Audit trail differences
    remediation: Plan audit data reconciliation
  positive:
  - id: DMV-POS-001
    signal: All validation checks pass
  - id: DMV-POS-002
    signal: Business reports produce matching results
procedure:
  context:
    cognitive_mode: analytical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Verify Row Counts
    description: Compare row counts between source and target
    duration_estimate: 1 hour
  - id: '2'
    name: Calculate Checksums
    description: Compare checksums for critical tables
    duration_estimate: 2 hours
  - id: '3'
    name: Sample Data Comparison
    description: Compare random samples row by row
    duration_estimate: 2 hours
  - id: '4'
    name: Validate Referential Integrity
    description: Verify foreign key relationships
    duration_estimate: 1 hour
  - id: '5'
    name: Apply Business Rules
    description: Validate against business data rules
    duration_estimate: 2 hours
  - id: '6'
    name: Execute Report Comparison
    description: Compare key business reports
    duration_estimate: 2 hours
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Row Count Analysis
    - Checksum Results
    - Sample Comparison Results
    - Integrity Validation
    - Business Rule Compliance
closeout_checklist:
- id: dmv-001
  item: Row counts validated
  level: WARNING
  verification: tool_output
- id: dmv-002
  item: Checksums verified
  level: WARNING
  verification: tool_output
- id: dmv-003
  item: Business rules validated
  level: WARNING
  verification: manual
governance:
  applicable_to:
    archetypes:
    - data-migrations
    - database-changes
  compliance_mappings:
  - framework: SOX
    control: Data Integrity
    description: Financial data accuracy
  - framework: GDPR
    control: Article 5
    description: Data accuracy
relationships:
  commonly_combined:
  - legacy-migration.data-migration.schema-compatibility
  - data-quality.data-profiling
  depends_on:
  - legacy-migration.data-migration.migration-execution
  feeds_into:
  - legacy-migration.data-migration.migration-verification
  - operations.production-cutover
