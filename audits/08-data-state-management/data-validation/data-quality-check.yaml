audit:
  id: data-state-management.data-validation.data-quality-check
  name: Data Quality Check Audit
  version: 1.0.0
  last_updated: '2026-01-18'
  status: active
  category: data-state-management
  category_number: 8
  subcategory: data-validation
  tier: phd
  estimated_duration: 120 minutes
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: data
  default_profiles:
  - full
  - data
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the mechanisms for ensuring data quality including completeness,
    accuracy, consistency, timeliness, and validity checks. Examines data
    quality rules, monitoring dashboards, automated quality gates, and
    remediation processes for data quality issues.
  why_it_matters: |
    Poor data quality leads to incorrect business decisions, customer complaints,
    regulatory fines, and system failures. Data quality issues compound over time
    - bad data begets more bad data. Proactive quality monitoring catches issues
    before they propagate through downstream systems and analytics.
  when_to_run:
  - Data pipeline reviews
  - Before major data migrations
  - Analytics audit preparation
  - After data quality incidents
prerequisites:
  required_artifacts:
  - type: source_code
    description: Data pipeline and ETL code
  - type: data_access
    description: Read access to production or production-like data
  - type: documentation
    description: Data quality requirements and SLAs
  access_requirements:
  - Read access to data pipeline code
  - Read access to data stores
  - Access to data quality monitoring dashboards
discovery:
  code_patterns:
  - pattern: data_quality|quality_check|dq_|validate_data
    type: regex
    scope: source
    purpose: Detect data quality functions
  - pattern: great_expectations|deequ|soda|pandera
    type: regex
    scope: source
    purpose: Detect data quality frameworks
  - pattern: null_count|duplicate_check|uniqueness|freshness
    type: regex
    scope: source
    purpose: Detect quality dimension checks
  - pattern: anomaly|outlier|threshold|expectation
    type: regex
    scope: source
    purpose: Detect anomaly detection
  file_patterns:
  - glob: '**/quality/**'
    purpose: Data quality modules
  - glob: '**/expectations/**'
    purpose: Great Expectations suites
  - glob: '**/dq/**'
    purpose: Data quality rules
  - glob: '**/validation/**'
    purpose: Validation pipelines
knowledge_sources:
  guides:
  - id: great-expectations
    name: Great Expectations Documentation
    url: https://docs.greatexpectations.io/
    offline_cache: true
  - id: data-quality-dimensions
    name: Data Quality Dimensions
    url: https://www.dama.org/cpages/body-of-knowledge
    offline_cache: true
  learning_resources:
  - id: dq-fundamentals
    title: 'Data Quality: The Accuracy Dimension'
    type: book
    reference: 'Jack Olson, ISBN: 978-1558608917'
tooling:
  static_analysis:
  - tool: great_expectations
    purpose: Data validation and documentation
    offline_capable: true
  - tool: deequ
    purpose: Data quality constraints on Spark
    offline_capable: true
  - tool: soda-core
    purpose: Data quality testing
    offline_capable: true
  scripts:
  - id: data-quality-scan
    language: bash
    purpose: Scan for data quality patterns
    source: inline
    code: |
      echo "=== Data Quality Analysis ==="
      echo "--- Data quality frameworks ---"
      grep -rn "great_expectations\|deequ\|soda\|pandera\|cerberus" \
        --include="*.py" --include="*.scala" --include="*.yaml" . 2>/dev/null | head -30

      echo "--- Quality check functions ---"
      grep -rn "quality\|validate_data\|check_data\|assert_" \
        --include="*.py" --include="*.scala" . 2>/dev/null | head -30
signals:
  critical:
  - id: DQC-CRIT-001
    signal: No data quality checks in data pipelines
    evidence_pattern: ETL code without validation steps
    explanation: |
      Data pipelines without quality gates propagate bad data throughout
      the organization. Once corrupt data enters downstream systems and
      reports, remediation becomes exponentially more expensive.
    remediation: Implement quality gates at each pipeline stage
  - id: DQC-CRIT-002
    signal: Critical tables have no completeness checks
    evidence_pattern: No null checks on required fields
    explanation: |
      Missing completeness checks allow null values in critical fields,
      breaking downstream processes and creating unreliable analytics.
    remediation: Add completeness expectations for all required fields
  high:
  - id: DQC-HIGH-001
    signal: No duplicate detection in data loads
    evidence_pattern: Insert without deduplication logic
    explanation: |
      Without duplicate detection, data loads can create redundant records,
      inflating metrics and corrupting aggregations.
    remediation: Implement deduplication at ingestion and periodic audits
  - id: DQC-HIGH-002
    signal: No freshness monitoring for data sources
    evidence_pattern: No timestamp validation on incoming data
    explanation: |
      Stale data presented as current leads to incorrect decisions.
      Users must know the age of data they're working with.
    remediation: Add freshness checks and expose data timestamps
  - id: DQC-HIGH-003
    signal: Referential integrity violations possible
    evidence_pattern: No cross-table consistency checks
    explanation: |
      Data across related tables can become inconsistent without
      referential integrity checks, especially in eventual consistency systems.
    remediation: Add cross-table consistency validations
  medium:
  - id: DQC-MED-001
    signal: No anomaly detection for numeric fields
    evidence_pattern: Accepted values without range validation
    remediation: Add statistical anomaly detection for numeric fields
  - id: DQC-MED-002
    signal: Data quality metrics not monitored
    evidence_pattern: No dashboards for data quality KPIs
    remediation: Create data quality dashboards with alerting
  - id: DQC-MED-003
    signal: No data profiling documentation
    evidence_pattern: Schema without field statistics or descriptions
    remediation: Generate and maintain data profiling reports
  low:
  - id: DQC-LOW-001
    signal: Data quality rules not version controlled
    evidence_pattern: Rules defined in UI without code backup
    remediation: Store quality rules as code in version control
  - id: DQC-LOW-002
    signal: No data quality SLAs defined
    evidence_pattern: Missing quality expectations documentation
    remediation: Define and document data quality SLAs per dataset
  positive:
  - id: DQC-POS-001
    signal: Comprehensive data quality framework in place
    evidence_pattern: Great Expectations or equivalent configured
  - id: DQC-POS-002
    signal: Quality gates block bad data from propagating
    evidence_pattern: Pipeline fails on quality check failure
  - id: DQC-POS-003
    signal: Data quality metrics tracked over time
    evidence_pattern: Historical quality trends visible
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Identify Data Quality Framework
    description: |
      Determine what data quality tooling is in use.
    duration_estimate: 20 min
    commands:
    - purpose: Find quality framework usage
      command: |
        grep -rn "great_expectations\|deequ\|soda\|pandera\|pydantic" \
          --include="*.py" --include="*.scala" --include="*.yaml" . 2>/dev/null | head -30
    - purpose: Find requirements with quality libraries
      command: |
        grep -rn "great-expectations\|pydeequ\|soda-core\|pandera" \
          --include="requirements*.txt" --include="pyproject.toml" --include="*.gradle" . 2>/dev/null
    expected_findings:
    - Quality framework inventory
    - Framework coverage
  - id: '2'
    name: Review Quality Checks
    description: |
      Examine what quality dimensions are being checked.
    duration_estimate: 30 min
    commands:
    - purpose: Find validation functions
      command: |
        grep -rn "expect_\|assert_\|validate_\|check_" \
          --include="*.py" --include="*.scala" . 2>/dev/null | \
          grep -v test | head -40
    - purpose: Find quality expectations
      command: |
        find . -name "*.json" -path "*expectations*" 2>/dev/null | head -20
    expected_findings:
    - Quality check coverage
    - Dimensions validated
  - id: '3'
    name: Analyze Quality Gates
    description: |
      Check if quality failures block data propagation.
    duration_estimate: 25 min
    commands:
    - purpose: Find quality gate logic
      command: |
        grep -rn "fail_pipeline\|stop_on_error\|quality_gate\|checkpoint" \
          --include="*.py" --include="*.scala" --include="*.yaml" . 2>/dev/null | head -30
    expected_findings:
    - Quality gate presence
    - Failure handling
  - id: '4'
    name: Review Monitoring and Alerting
    description: |
      Check for data quality observability.
    duration_estimate: 25 min
    commands:
    - purpose: Find quality metrics emission
      command: |
        grep -rn "metric\|dashboard\|alert\|monitor" \
          --include="*.py" --include="*.yaml" . 2>/dev/null | \
          grep -i "quality\|dq" | head -30
    expected_findings:
    - Monitoring coverage
    - Alert configuration
  - id: '5'
    name: Assess Remediation Processes
    description: |
      Verify processes exist for handling quality failures.
    duration_estimate: 20 min
    questions:
    - What happens when quality checks fail?
    - Is there a quarantine process for bad data?
    - How are quality issues escalated?
    expected_findings:
    - Remediation workflow
    - Escalation procedures
output:
  deliverables:
  - type: finding_list
    format: structured
    content:
    - framework_assessment
    - coverage_gaps
    - monitoring_status
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Quality Framework Assessment
    - Coverage Analysis
    - Monitoring and Alerting
    - Recommendations
  confidence_guidance:
    high: Clear gap in quality checks or missing framework
    medium: Quality checks exist but coverage incomplete
    low: Requires data profiling to confirm quality
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: great-expectations
      priority: required
    - source_id: data-quality-dimensions
      priority: required
profiles:
  membership:
    quick:
      included: false
      reason: Requires deep data pipeline analysis
    full:
      included: true
      priority: 1
    data:
      included: true
      priority: 1
closeout_checklist:
- id: dqc-001
  item: Data quality framework is in place
  level: CRITICAL
  verification: |
    grep -rn "great_expectations\|deequ\|soda\|pandera" \
      --include="*.py" --include="*.yaml" . 2>/dev/null | wc -l | \
      xargs -I {} test {} -gt 0 && echo "PASS" || echo "FAIL"
  expected: PASS
- id: dqc-002
  item: Quality gates exist in data pipelines
  level: CRITICAL
  verification: |
    grep -rn "validate\|expect_\|checkpoint" --include="*.py" . 2>/dev/null | \
      wc -l | xargs -I {} test {} -gt 0 && echo "PASS" || echo "FAIL"
  expected: PASS
- id: dqc-003
  item: Data quality metrics are monitored
  level: BLOCKING
  verification: manual
  verification_notes: Verify dashboards exist for data quality KPIs
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - data-intensive
    - analytics
    - enterprise
  compliance_frameworks:
  - framework: SOX
    controls:
    - Data Integrity Controls
  - framework: GDPR
    controls:
    - Data Accuracy
  - framework: BCBS 239
    controls:
    - Data Quality
relationships:
  commonly_combined:
  - data-state-management.data-validation.input-validation
  - data-state-management.data-lineage-provenance.data-lineage-tracking
  - data-state-management.data-lineage-provenance.audit-trail
