# ============================================================
# AUDIT: Data Source Documentation
# ============================================================

audit:
  id: "data-state-management.data-lineage-provenance.data-source-documentation"
  name: "Data Source Documentation Audit"
  version: "1.0.0"
  last_updated: "2026-01-18"
  status: "active"

  category: "data-state-management"
  category_number: 8
  subcategory: "data-lineage-provenance"

  tier: "expert"
  estimated_duration: "90 minutes"

  completeness: "requires_discovery"
  requires_runtime: false
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "documentation"

  default_profiles:
    - "full"
    - "data"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates the documentation of data sources including their origin,
    ownership, update frequency, quality characteristics, schema documentation,
    and integration details. Analyzes whether data consumers can understand
    what data is available, where it comes from, and how to use it correctly.

  why_it_matters: |
    Undocumented data sources lead to misuse, incorrect analysis, and
    integration failures. When developers don't know the meaning of fields,
    update frequency, or quality expectations, they make assumptions that
    lead to bugs. Data documentation is the foundation of data-driven
    decision making.

  when_to_run:
    - "New data integration planning"
    - "Data governance assessments"
    - "Onboarding new team members"
    - "Data catalog reviews"

prerequisites:
  required_artifacts:
    - type: "source_code"
      description: "Data integration code and schemas"
    - type: "documentation"
      description: "Existing data documentation"
    - type: "data_catalog"
      description: "Data catalog if it exists"

  access_requirements:
    - "Read access to data schemas"
    - "Access to data documentation"
    - "Access to data catalog"

discovery:
  code_patterns:
    - pattern: "datasource|DataSource|data_source|source_system"
      type: "regex"
      scope: "source"
      purpose: "Detect data source definitions"

    - pattern: "schema|Schema|field|column|attribute"
      type: "regex"
      scope: "source"
      purpose: "Detect schema definitions"

    - pattern: "description|comment|doc|documentation"
      type: "regex"
      scope: "source"
      purpose: "Detect inline documentation"

  file_patterns:
    - glob: "**/schemas/**"
      purpose: "Schema definitions"
    - glob: "**/*.schema.json"
      purpose: "JSON schemas"
    - glob: "**/data-dictionary*"
      purpose: "Data dictionaries"
    - glob: "**/catalog/**"
      purpose: "Data catalog files"

knowledge_sources:
  guides:
    - id: "data-catalog"
      name: "Data Catalog Best Practices"
      url: "https://www.collibra.com/us/en/blog/what-is-a-data-catalog"
      offline_cache: true

    - id: "schema-registry"
      name: "Schema Registry Documentation"
      url: "https://docs.confluent.io/platform/current/schema-registry/"
      offline_cache: true

  learning_resources:
    - id: "dmbok"
      title: "DAMA-DMBOK: Data Management Body of Knowledge"
      type: "book"
      reference: "DAMA International, ISBN: 978-1634622349"

tooling:
  static_analysis:
    - tool: "schema-doc-generator"
      purpose: "Generate documentation from schemas"
      offline_capable: true

    - tool: "openapi-generator"
      purpose: "Generate API documentation"
      offline_capable: true

  scripts:
    - id: "data-source-doc-scan"
      language: "bash"
      purpose: "Analyze data source documentation"
      source: "inline"
      code: |
        echo "=== Data Source Documentation Analysis ==="
        echo "--- Schema files ---"
        find . -type f \( -name "*.schema.json" -o -name "*schema*.yaml" \) \
          -not -path "*/node_modules/*" 2>/dev/null | head -30

        echo "--- Schema documentation ---"
        grep -rn "description\|@description\|comment\|doc:" \
          --include="*.json" --include="*.yaml" --include="*.ts" . 2>/dev/null | \
          grep -i "schema\|field\|column" | head -30

        echo "--- Data dictionary files ---"
        find . -type f \( -name "*dictionary*" -o -name "*glossary*" -o -name "*catalog*" \) \
          -not -path "*/node_modules/*" 2>/dev/null | head -20

signals:
  critical:
    - id: "DSD-CRIT-001"
      signal: "Data sources undocumented"
      evidence_pattern: "External data integrations without documentation"
      explanation: |
        Undocumented data sources are black boxes. No one knows what the
        data means, when it updates, or what quality guarantees exist.
        This leads to incorrect usage and broken integrations.
      remediation: "Document all data sources with owner, schema, and quality info"

    - id: "DSD-CRIT-002"
      signal: "No data dictionary for shared data"
      evidence_pattern: "Central database with no field documentation"
      explanation: |
        When multiple teams use shared data, they need a common understanding
        of what each field means. Without a data dictionary, teams interpret
        fields differently, leading to inconsistent analysis.
      remediation: "Create and maintain data dictionary for all shared datasets"

  high:
    - id: "DSD-HIGH-001"
      signal: "Schema fields without descriptions"
      evidence_pattern: "JSON schema with only types, no descriptions"
      explanation: |
        Schema types alone don't explain meaning. A field named "status"
        could mean many things. Descriptions explain what values mean
        and how to interpret them.
      remediation: "Add descriptions to all schema fields"

    - id: "DSD-HIGH-002"
      signal: "No data owner identified"
      evidence_pattern: "Data source without responsible party"
      explanation: |
        Every data source needs an owner who can answer questions, fix
        issues, and approve changes. Without ownership, data issues
        have no resolution path.
      remediation: "Assign and document owner for each data source"

    - id: "DSD-HIGH-003"
      signal: "Data freshness not documented"
      evidence_pattern: "No documentation of update frequency or SLAs"
      explanation: |
        Consumers need to know when data updates to use it correctly.
        Using hourly data as if it were real-time, or monthly data as
        if it were daily, leads to incorrect decisions.
      remediation: "Document update frequency and freshness SLAs"

  medium:
    - id: "DSD-MED-001"
      signal: "Documentation not co-located with schema"
      evidence_pattern: "Schema in code, documentation in wiki"
      remediation: "Keep documentation with schema (descriptions in schema file)"

    - id: "DSD-MED-002"
      signal: "No sample data or examples"
      evidence_pattern: "Schema without example values"
      remediation: "Include example values in schema documentation"

    - id: "DSD-MED-003"
      signal: "Data quality characteristics undocumented"
      evidence_pattern: "No documentation of nullability, uniqueness, ranges"
      remediation: "Document data quality expectations per field"

  low:
    - id: "DSD-LOW-001"
      signal: "Documentation not versioned with schema"
      evidence_pattern: "Schema changes without documentation updates"
      remediation: "Version documentation alongside schema changes"

    - id: "DSD-LOW-002"
      signal: "No integration examples"
      evidence_pattern: "Schema without code samples for integration"
      remediation: "Provide integration code examples"

  positive:
    - id: "DSD-POS-001"
      signal: "Comprehensive data catalog"
      evidence_pattern: "Searchable catalog with all data sources"

    - id: "DSD-POS-002"
      signal: "Schema documentation in schema files"
      evidence_pattern: "Descriptions embedded in JSON Schema/OpenAPI"

    - id: "DSD-POS-003"
      signal: "Data quality SLAs documented"
      evidence_pattern: "Explicit freshness, completeness, accuracy expectations"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Inventory Data Sources"
      description: |
        Catalog all data sources in the system.
      duration_estimate: "25 min"
      commands:
        - purpose: "Find external data sources"
          command: |
            grep -rn "datasource\|DataSource\|source.*url\|connection.*string" \
              --include="*.yaml" --include="*.json" --include="*.ts" . 2>/dev/null | \
              grep -v node_modules | head -30
        - purpose: "Find data integrations"
          command: |
            grep -rn "import.*data\|fetch.*api\|extract.*from" \
              --include="*.ts" --include="*.py" --include="*.java" . 2>/dev/null | \
              grep -v node_modules | head -30

      expected_findings:
        - "Data source inventory"
        - "Integration points"

    - id: "2"
      name: "Review Schema Documentation"
      description: |
        Examine schema documentation quality.
      duration_estimate: "25 min"
      commands:
        - purpose: "Find schema files"
          command: |
            find . -type f \( -name "*.schema.json" -o -name "*schema*.yaml" -o -name "*.avsc" \) \
              -not -path "*/node_modules/*" 2>/dev/null
        - purpose: "Check for descriptions"
          command: |
            grep -rn "description" --include="*.schema.json" --include="*.yaml" . 2>/dev/null | \
              grep -v node_modules | head -30

      expected_findings:
        - "Schema file inventory"
        - "Description coverage"

    - id: "3"
      name: "Check Data Dictionary"
      description: |
        Review data dictionary or catalog existence.
      duration_estimate: "20 min"
      commands:
        - purpose: "Find data dictionary files"
          command: |
            find . -type f \( -name "*dictionary*" -o -name "*glossary*" -o -name "*catalog*" \) \
              -not -path "*/node_modules/*" 2>/dev/null
        - purpose: "Find README in data directories"
          command: |
            find . -type f -name "README*" -path "*/data*" -not -path "*/node_modules/*" 2>/dev/null

      expected_findings:
        - "Data dictionary status"
        - "Documentation locations"

    - id: "4"
      name: "Assess Documentation Quality"
      description: |
        Evaluate completeness and usefulness of documentation.
      duration_estimate: "20 min"
      questions:
        - "Can a new developer understand each data source?"
        - "Are data owners identified?"
        - "Is update frequency documented?"
        - "Are quality expectations clear?"
      expected_findings:
        - "Documentation quality assessment"
        - "Gap identification"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"
      content:
        - "data_source_inventory"
        - "documentation_coverage"
        - "quality_gaps"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Data Source Inventory"
        - "Documentation Coverage"
        - "Quality Assessment"
        - "Recommendations"

  confidence_guidance:
    high: "Clear absence of documentation for major data sources"
    medium: "Documentation exists but incomplete"
    low: "Requires domain knowledge to assess completeness"

offline:
  capability: "full"

  cache_manifest:
    knowledge:
      - source_id: "data-catalog"
        priority: "required"
      - source_id: "dmbok"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires documentation review"
    full:
      included: true
      priority: 1
    data:
      included: true
      priority: 1

closeout_checklist:
  - id: "dsd-001"
    item: "All data sources documented"
    level: "CRITICAL"
    verification: |
      find . -type f -name "*schema*" -not -path "*/node_modules/*" 2>/dev/null | wc -l | \
        xargs -I {} test {} -gt 0 && echo "PASS" || echo "FAIL"
    expected: "PASS"

  - id: "dsd-002"
    item: "Schema fields have descriptions"
    level: "CRITICAL"
    verification: |
      grep -rn "description" --include="*.schema.json" --include="*.yaml" . 2>/dev/null | \
        grep -v node_modules | wc -l | xargs -I {} test {} -gt 0 && echo "PASS" || echo "FAIL"
    expected: "PASS"

  - id: "dsd-003"
    item: "Data owners identified"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Verify each data source has assigned owner"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["data-intensive", "enterprise", "analytics"]

  compliance_frameworks:
    - framework: "GDPR"
      controls: ["Article 30"]
    - framework: "BCBS 239"
      controls: ["Data Quality"]
    - framework: "ISO 8000"
      controls: ["Data Quality"]

relationships:
  commonly_combined:
    - "data-state-management.data-lineage-provenance.data-lineage-tracking"
    - "data-state-management.data-validation.data-quality-check"
    - "code-quality.documentation.api-documentation"
