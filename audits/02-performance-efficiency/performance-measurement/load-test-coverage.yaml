# ============================================================
# AUDIT: Load Test Coverage Audit
# ============================================================

audit:
  id: "performance-efficiency.performance-measurement.load-test-coverage"
  name: "Load Test Coverage Audit"
  version: "1.0.0"
  last_updated: "2026-01-18"
  status: "active"

  category: "performance-efficiency"
  category_number: 2
  subcategory: "performance-measurement"

  tier: "expert"
  estimated_duration: "2.5 hours"

  completeness: "complete"
  requires_runtime: false
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "testing"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates load testing practices including test coverage of critical paths,
    realistic traffic modeling, environment parity with production, result
    analysis methodology, and integration with release processes. Reviews
    load test tools, scripts, and execution patterns.

  why_it_matters: |
    Systems that aren't load tested will fail under load. Capacity planning
    without load testing is guesswork. Amazon found that 100ms of latency
    costs 1% in sales. Netflix attributes their resilience to extensive load
    testing. Organizations that load test before releases experience 70% fewer
    production incidents related to capacity.

  when_to_run:
    - "Before major releases"
    - "After infrastructure changes"
    - "Capacity planning reviews"
    - "When scaling expectations change"

prerequisites:
  required_artifacts:
    - type: "testing_framework"
      description: "Load test scripts and configuration"
    - type: "documentation"
      description: "Load testing procedures"

  access_requirements:
    - "Load test scripts"
    - "Test environment access"
    - "Historical load test results"

discovery:
  file_patterns:
    - glob: "**/*k6*"
      purpose: "k6 load test files"
    - glob: "**/*gatling*"
      purpose: "Gatling test files"
    - glob: "**/*locust*"
      purpose: "Locust test files"
    - glob: "**/*artillery*"
      purpose: "Artillery test files"
    - glob: "**/*jmeter*"
      purpose: "JMeter test files"

  code_patterns:
    - pattern: "http\\.get|http\\.post|load.*test|stress.*test"
      type: "keyword"
      scope: "source"
      purpose: "Find load test definitions"
    - pattern: "vus|virtual.*user|concurrent"
      type: "keyword"
      scope: "config"
      purpose: "Find concurrency configuration"

knowledge_sources:
  guides:
    - id: "k6-docs"
      name: "k6 Documentation"
      url: "https://k6.io/docs/"
      offline_cache: true
    - id: "locust-docs"
      name: "Locust Documentation"
      url: "https://docs.locust.io/"
      offline_cache: true
    - id: "gatling-docs"
      name: "Gatling Documentation"
      url: "https://gatling.io/docs/"
      offline_cache: true

  learning_resources:
    - id: "load-testing-book"
      title: "The Art of Capacity Planning"
      type: "book"
      reference: "John Allspaw"

tooling:
  static_analysis:
    - tool: "k6"
      purpose: "Modern load testing"
      offline_capable: true
    - tool: "locust"
      purpose: "Python-based load testing"
      offline_capable: true
    - tool: "gatling"
      purpose: "Scala-based load testing"
      offline_capable: true

  scripts:
    - id: "load-test-runner"
      language: "bash"
      purpose: "Run load test with standard output"
      source: "inline"
      code: |
        #!/bin/bash
        echo "=== Load Test Execution ==="

        # k6 example
        if [ -f "load-test.js" ]; then
          k6 run --out json=results.json \
            --vus 50 \
            --duration 5m \
            load-test.js
        fi

        # Analyze results
        if [ -f "results.json" ]; then
          echo -e "\n--- Results Summary ---"
          jq '.metrics.http_req_duration' results.json
        fi

signals:
  critical:
    - id: "LOAD-CRIT-001"
      signal: "No load tests exist for the application"
      evidence_pattern: "No k6, Locust, Gatling, or JMeter files found"
      explanation: |
        Without load testing, capacity limits are unknown until breached in
        production. This leads to outages during traffic spikes, failed
        launches, and embarrassing public incidents. Load testing is essential
        for any production system with variable traffic.
      remediation: "Implement load tests for critical user journeys using k6, Locust, or similar"

    - id: "LOAD-CRIT-002"
      signal: "Load tests don't cover critical business paths"
      evidence_pattern: "Tests only hit health check, not login, checkout, search"
      explanation: |
        Testing only infrastructure endpoints (health checks) misses the
        actual user experience. Critical paths like authentication, payment,
        and search often have the most complex code and database queries.
        These are exactly the paths that fail under load.
      remediation: "Map critical user journeys and ensure load tests cover them"

  high:
    - id: "LOAD-HIGH-001"
      signal: "Load tests run in environment significantly different from production"
      evidence_pattern: "Single server test env vs multi-server production"
      explanation: |
        Load test results only predict production behavior if the environment
        is representative. Testing on a single server when production has 10
        will miss distributed system issues. Database size differences affect
        query performance. Network topology matters.
      remediation: "Use production-like environment for load testing or document differences"

    - id: "LOAD-HIGH-002"
      signal: "Traffic patterns don't reflect realistic usage"
      evidence_pattern: "Uniform load instead of spikes, no think time"
      explanation: |
        Real users don't hit endpoints continuously without pauses. Realistic
        load tests include think time between requests, traffic ramp-up, and
        spike scenarios. Tests without these patterns produce misleading results.
      remediation: "Model realistic traffic: think time, ramp-up, spikes, and diurnal patterns"

  medium:
    - id: "LOAD-MED-001"
      signal: "Load tests not integrated into release process"
      evidence_pattern: "Load tests run manually, not in CI/CD"
      remediation: "Integrate load testing into pre-release pipeline"

    - id: "LOAD-MED-002"
      signal: "No baseline comparison in load test results"
      evidence_pattern: "Results reported without comparison to previous runs"
      remediation: "Compare each load test to baseline to detect regressions"

  low:
    - id: "LOAD-LOW-001"
      signal: "Load test data not representative of production"

  positive:
    - id: "LOAD-POS-001"
      signal: "Comprehensive load tests for all critical paths"
    - id: "LOAD-POS-002"
      signal: "Realistic traffic modeling with think time and spikes"
    - id: "LOAD-POS-003"
      signal: "Production-like test environment"
    - id: "LOAD-POS-004"
      signal: "Load tests in CI/CD with pass/fail criteria"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Locate Load Test Assets"
      description: |
        Find load test scripts, configurations, and results
        across the codebase.
      duration_estimate: "20 min"

      commands:
        - purpose: "Find load test files"
          command: "find . -name '*k6*' -o -name '*locust*' -o -name '*gatling*' -o -name '*artillery*' -o -name '*jmeter*' | head -20"
        - purpose: "Search for load test references"
          command: "grep -rn 'load.*test\\|stress.*test\\|performance.*test' --include='*.md' --include='*.yaml' ."
        - purpose: "Check package.json for load test tools"
          command: "grep -E 'k6|locust|artillery|gatling' package.json 2>/dev/null"

      expected_findings:
        - "Load test scripts"
        - "Test configuration"

    - id: "2"
      name: "Review Test Coverage"
      description: |
        Analyze load test scripts to determine which endpoints
        and user journeys are covered.
      duration_estimate: "30 min"

      commands:
        - purpose: "Extract tested endpoints from k6"
          command: "grep -rn 'http\\.get\\|http\\.post\\|http\\.put' --include='*.js' tests/ | grep -oE '/[a-z/]+' | sort -u"
        - purpose: "List all API endpoints"
          command: "grep -rn 'router\\.get\\|router\\.post\\|app\\.get\\|app\\.post' --include='*.ts' --include='*.js' src/ | head -20"
        - purpose: "Calculate coverage"
          command: "echo 'Manual: Compare tested endpoints vs total endpoints'"

      expected_findings:
        - "List of endpoints covered by load tests"
        - "Critical paths (login, checkout) included"

    - id: "3"
      name: "Evaluate Traffic Modeling"
      description: |
        Review load test configuration for realistic traffic
        patterns including think time and ramp-up.
      duration_estimate: "25 min"

      commands:
        - purpose: "Check for think time"
          command: "grep -rn 'sleep\\|think\\|pause\\|delay' --include='*.js' --include='*.py' tests/"
        - purpose: "Check ramp-up configuration"
          command: "grep -rn 'stages\\|ramp\\|duration' --include='*.js' --include='*.yaml' tests/"
        - purpose: "Check spike scenarios"
          command: "grep -rn 'spike\\|burst\\|peak' --include='*.js' --include='*.yaml' tests/"

      expected_findings:
        - "Think time between requests"
        - "Gradual ramp-up configured"
        - "Spike scenarios defined"

    - id: "4"
      name: "Review CI/CD Integration"
      description: |
        Verify load tests are integrated into release process
        with appropriate pass/fail criteria.
      duration_estimate: "20 min"

      commands:
        - purpose: "Check CI for load test jobs"
          command: "grep -rn 'load.*test\\|k6\\|locust' .github/workflows/*.yaml 2>/dev/null"
        - purpose: "Check thresholds in load tests"
          command: "grep -rn 'thresholds\\|assert\\|check' --include='*.js' tests/"
        - purpose: "Check for test environment config"
          command: "grep -rn 'LOAD_TEST_ENV\\|TEST_URL' .github/workflows/*.yaml tests/"

      expected_findings:
        - "Load test job in CI"
        - "Pass/fail thresholds defined"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Test Coverage Analysis"
        - "Traffic Modeling Review"
        - "CI/CD Integration Status"
        - "Recommendations"

  confidence_guidance:
    high: "Direct load test script review"
    medium: "Configuration analysis"
    low: "Documentation-based assessment"

offline:
  capability: "full"

  cache_manifest:
    knowledge:
      - source_id: "k6-docs"
        priority: "required"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires comprehensive test analysis"
    full:
      included: true
      priority: 1

closeout_checklist:
  - id: "loadtest-001"
    item: "Load tests exist for the application"
    level: "CRITICAL"
    verification: |
      TESTS=$(find . -name '*k6*' -o -name '*locust*' -o -name '*gatling*' 2>/dev/null | wc -l)
      if [ "$TESTS" -gt 0 ]; then echo "PASS"; else echo "FAIL: No load tests found"; fi
    expected: "PASS"

  - id: "loadtest-002"
    item: "Critical user journeys covered"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Verify login, checkout, search, and other critical paths are tested"
    expected: "Confirmed by reviewer"

  - id: "loadtest-003"
    item: "Realistic traffic modeling (think time, ramp-up)"
    level: "BLOCKING"
    verification: |
      THINK=$(grep -rn 'sleep\|think\|pause' --include='*.js' --include='*.py' tests/ 2>/dev/null | wc -l)
      if [ "$THINK" -gt 0 ]; then echo "PASS"; else echo "FAIL: No think time"; fi
    expected: "PASS"

  - id: "loadtest-004"
    item: "Load tests in CI/CD pipeline"
    level: "BLOCKING"
    verification: |
      CI_LOAD=$(grep -rn 'load\|k6\|locust' .github/workflows/*.yaml 2>/dev/null | wc -l)
      if [ "$CI_LOAD" -gt 0 ]; then echo "PASS"; else echo "FAIL: Not in CI"; fi
    expected: "PASS"

governance:
  applicable_to:
    archetypes: ["all"]

relationships:
  commonly_combined:
    - "performance-efficiency.performance-measurement.performance-baseline"
    - "performance-efficiency.performance-measurement.performance-regression-detection"
