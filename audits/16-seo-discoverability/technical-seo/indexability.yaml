# ============================================================
# AUDIT: Indexability Assessment
# ============================================================
# Evaluates whether crawled pages can be properly indexed by
# search engines and appear in search results.
# ============================================================

audit:
  id: "seo-discoverability.technical-seo.indexability"

  name: "Indexability Assessment"

  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "seo-discoverability"
  category_number: 16
  subcategory: "technical-seo"

  tier: "expert"
  estimated_duration: "2-3 hours"  # median: 2h

  completeness: "complete"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "codebase"

  default_profiles:
    - "full"
    - "production"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    This audit examines whether pages that search engines can crawl will
    actually be added to the search index and appear in search results.
    It evaluates indexing directives (meta robots, X-Robots-Tag), identifies
    duplicate content issues, checks for thin content, assesses canonical
    tag implementation, and analyzes Google Search Console index coverage.

  why_it_matters: |
    A page that is crawled but not indexed will never appear in search
    results. Common indexability issues include noindex directives,
    duplicate content, thin content, canonicalization errors, and
    manual penalties. Understanding and resolving indexability issues
    ensures your important pages can compete for search visibility.

  when_to_run:
    - "When pages are not appearing in search results"
    - "After implementing noindex directives"
    - "During duplicate content cleanup"
    - "After site migrations or redesigns"
    - "Monthly monitoring for index coverage changes"

prerequisites:
  required_artifacts:
    - type: "website"
      description: "Live website with searchable pages"
    - type: "gsc_access"
      description: "Google Search Console with Index Coverage data"

  access_requirements:
    - "Google Search Console with full owner/user permissions"
    - "Ability to run site: searches on target domain"
    - "Access to website source code"

discovery:
  code_patterns:
    - pattern: "noindex"
      type: "keyword"
      scope: "source"
      purpose: "Find pages explicitly blocked from indexing"
    - pattern: "canonical"
      type: "keyword"
      scope: "source"
      purpose: "Identify canonical URL declarations"
    - pattern: "meta\\s+name=[\"']robots[\"']\\s+content=[\"'][^\"']*noindex"
      type: "regex"
      scope: "source"
      purpose: "Find meta robots noindex directives"

  file_patterns:
    - glob: "**/*.html"
      purpose: "Check HTML pages for indexing directives"
    - glob: "**/sitemap*.xml"
      purpose: "Cross-reference sitemap with index status"

knowledge_sources:
  specifications:
    - id: "google-indexing-api"
      name: "Google Indexing API Documentation"
      url: "https://developers.google.com/search/apis/indexing-api/v3/quickstart"
      offline_cache: true
      priority: "optional"

  guides:
    - id: "google-index-coverage"
      name: "Google Index Coverage Report"
      url: "https://support.google.com/webmasters/answer/7440203"
      offline_cache: true
    - id: "google-noindex"
      name: "Block Search Indexing with noindex"
      url: "https://developers.google.com/search/docs/crawling-indexing/block-indexing"
      offline_cache: true
    - id: "moz-indexability"
      name: "Moz: Index Coverage and Indexability"
      url: "https://moz.com/learn/seo/index-coverage"
      offline_cache: true

tooling:
  static_analysis:
    - tool: "Screaming Frog SEO Spider"
      purpose: "Bulk analysis of indexing directives"
      offline_capable: true
    - tool: "Sitebulb"
      purpose: "Index coverage visualization"
      offline_capable: true

  infrastructure_tools:
    - tool: "Google Search Console"
      purpose: "Official index coverage data"
      command: "Index > Coverage report"
    - tool: "Google Search Operators"
      purpose: "Verify page indexation status"
      command: "site:example.com/specific-page"

  scripts:
    - id: "indexability-checker"
      language: "bash"
      purpose: "Check page indexing directives"
      source: "inline"
      code: |
        #!/bin/bash
        URL=$1
        echo "=== Checking indexability for: $URL ==="

        # Check meta robots
        echo "Meta Robots:"
        curl -s "$URL" | grep -i "meta.*robots"

        # Check X-Robots-Tag header
        echo "X-Robots-Tag Header:"
        curl -sI "$URL" | grep -i "X-Robots-Tag"

        # Check canonical
        echo "Canonical URL:"
        curl -s "$URL" | grep -i "canonical"

signals:
  critical:
    - id: "INDEX-CRIT-001"
      signal: "Important pages blocked by noindex directive"
      evidence_pattern: "noindex"
      explanation: |
        Pages with noindex directives are explicitly excluded from
        search engine indexes and will never appear in search results,
        regardless of their content quality or backlink profile.
      remediation: "Remove noindex directive from pages that should be indexed"

    - id: "INDEX-CRIT-002"
      signal: "Large portions of site excluded from index"
      evidence_indicators:
        - "Index Coverage report shows >30% pages excluded"
        - "site: search returns significantly fewer pages than sitemap"
      explanation: |
        When a large portion of the site is not indexed, the site loses
        significant search visibility and organic traffic potential.
      remediation: "Analyze exclusion reasons in GSC and address each category"

  high:
    - id: "INDEX-HIGH-001"
      signal: "Duplicate content causing index consolidation"
      explanation: |
        When search engines find duplicate content, they consolidate
        to a single version, potentially choosing the wrong URL or
        diluting ranking signals across duplicates.
      remediation: "Implement proper canonical tags and eliminate unnecessary duplicates"

    - id: "INDEX-HIGH-002"
      signal: "Soft 404 pages being indexed"
      explanation: |
        Soft 404s (pages that return 200 but display error content)
        waste crawl budget and can dilute site quality signals.
      remediation: "Return proper 404 status codes for non-existent pages"

  medium:
    - id: "INDEX-MED-001"
      signal: "Thin content pages in the index"
      remediation: "Consolidate thin pages, add substantial content, or noindex them"

    - id: "INDEX-MED-002"
      signal: "Crawled but not indexed pages increasing over time"
      remediation: "Improve content quality or consolidate low-value pages"

  low:
    - id: "INDEX-LOW-001"
      signal: "Index coverage fluctuations without clear cause"

  positive:
    - id: "INDEX-POS-001"
      signal: "All submitted sitemap URLs indexed"
    - id: "INDEX-POS-002"
      signal: "Index coverage stable or growing over time"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Index Coverage Report"
      description: |
        Analyze Google Search Console Index Coverage report to understand
        the current indexing status of all discovered pages.
      duration_estimate: "30 min"
      expected_findings:
        - "Valid indexed pages count"
        - "Excluded pages by reason"
        - "Errors preventing indexing"

    - id: "2"
      name: "Audit Indexing Directives"
      description: |
        Scan all pages for meta robots tags and X-Robots-Tag headers
        that may be blocking indexing.
      duration_estimate: "30 min"
      commands:
        - purpose: "Find all noindex directives"
          command: "grep -r 'noindex' --include='*.html' --include='*.php'"
      expected_findings:
        - "Pages with noindex directive"
        - "Pages with nofollow directive"
        - "HTTP header-based directives"

    - id: "3"
      name: "Identify Duplicate Content"
      description: |
        Find pages with duplicate or near-duplicate content that may
        cause indexing consolidation issues.
      duration_estimate: "45 min"
      expected_findings:
        - "Exact duplicate URLs"
        - "Near-duplicate content"
        - "WWW vs non-WWW duplicates"
        - "HTTP vs HTTPS duplicates"

    - id: "4"
      name: "Validate Canonical Implementation"
      description: |
        Review canonical tag implementation across the site to ensure
        proper signals for preferred URLs.
      duration_estimate: "30 min"
      commands:
        - purpose: "Extract canonical tags"
          command: "curl -s URL | grep -i canonical"
      expected_findings:
        - "Self-referencing canonicals"
        - "Cross-domain canonicals"
        - "Conflicting signals"

    - id: "5"
      name: "Perform Manual Index Checks"
      description: |
        Verify indexation status of key pages using site: search
        operators and URL Inspection tool.
      duration_estimate: "30 min"
      commands:
        - purpose: "Check if specific page is indexed"
          command: "site:example.com/specific-page (in Google search)"
      expected_findings:
        - "Indexed page confirmation"
        - "Index date and cached version"
        - "Mobile vs desktop indexing status"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Index Coverage Statistics"
        - "Exclusion Analysis"
        - "Duplicate Content Report"
        - "Recommendations"

  confidence_guidance:
    high: "Confirmed via GSC Index Coverage and URL Inspection"
    medium: "Identified via crawl tools, not verified in GSC"
    low: "Suspected based on code review"

offline:
  capability: "partial"

  cache_manifest:
    knowledge:
      - source_id: "google-index-coverage"
        priority: "required"
      - source_id: "google-noindex"
        priority: "required"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires GSC data analysis"
    full:
      included: true
      priority: 1
    production:
      included: true
      priority: 1

closeout_checklist:
  - id: "indexability-001"
    item: "No unintended noindex directives on important pages"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Review crawl report for noindex pages that should be indexed"
    expected: "Confirmed by reviewer"

  - id: "indexability-002"
    item: "Index Coverage report reviewed and exclusions understood"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Document all exclusion reasons and whether they are intentional"
    expected: "Confirmed by reviewer"

  - id: "indexability-003"
    item: "Duplicate content issues identified and documented"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "List all duplicate content scenarios with resolution plan"
    expected: "Confirmed by reviewer"

  - id: "indexability-004"
    item: "Key pages verified as indexed via site: search"
    level: "WARNING"
    verification: "manual"
    verification_notes: "Spot check top 10 important pages for index status"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["web-application", "marketing-site", "ecommerce", "content-site"]

  compliance_frameworks:
    - framework: "SEO Best Practices"
      controls: ["Index Management", "Content Quality"]

relationships:
  commonly_combined:
    - "seo-discoverability.technical-seo.crawlability"
    - "seo-discoverability.technical-seo.canonical-url"
    - "seo-discoverability.on-page-seo.content-quality"
