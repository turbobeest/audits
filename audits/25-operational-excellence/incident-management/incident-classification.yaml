audit:
  id: operational-excellence.incident-management.incident-classification
  name: Incident Classification Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: operational-excellence
  category_number: 25
  subcategory: incident-management
  tier: phd
  estimated_duration: 2-3 hours  # median: 2h
  completeness: complete
  requires_runtime: false
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: process
  default_profiles:
  - full
  - production
  blocks_phase: false
  parallelizable: true
description:
  what: |
    This audit examines the incident classification system including:
    - Severity level definitions (SEV1-SEV5 or P0-P4)
    - Impact criteria for each severity level
    - Urgency assessment frameworks
    - Classification decision trees
    - Response time SLAs per severity
    - Escalation triggers by severity
    - Classification accuracy and consistency
    - Historical classification review
  why_it_matters: |
    Accurate incident classification is fundamental to effective response:
    - Ensures critical incidents receive immediate attention
    - Prevents resource waste on incorrectly escalated minor issues
    - Enables meaningful metrics and trend analysis
    - Drives appropriate stakeholder communication
    - Aligns response effort with business impact

    Misclassification has significant consequences:
    - Over-classification causes alert fatigue and resource drain
    - Under-classification delays response to business-critical issues
    - Inconsistent classification undermines metrics and reporting
  when_to_run:
  - Quarterly operational reviews
  - When classification disputes arise frequently
  - After organizational changes affecting services
  - During SLA renegotiations
  - When incident metrics show anomalies
prerequisites:
  required_artifacts:
  - type: classification_documentation
    description: Severity level definitions and criteria
  - type: incident_history
    description: Historical incident records with classifications
  - type: sla_documentation
    description: Response time SLAs by severity
  access_requirements:
  - Read access to incident classification documentation
  - Access to incident tracking system
  - Historical incident data for analysis
  - SLA documentation access
discovery:
  code_patterns:
  - pattern: (sev|severity|priority)[_-]?[0-5]
    type: regex
    scope: config
    purpose: Detect severity level definitions
  - pattern: (critical|high|medium|low|informational).*(impact|urgency)
    type: regex
    scope: docs
    purpose: Detect impact classification criteria
  - pattern: (p[0-4]|sev[0-5]).*(response|acknowledge|resolve)
    type: regex
    scope: config
    purpose: Detect response SLAs by severity
  file_patterns:
  - glob: '**/severity*.{md,yaml,yml}'
    purpose: Severity definition files
  - glob: '**/classification*.{md,yaml,yml}'
    purpose: Classification criteria files
  - glob: '**/incident*policy*.{md,yaml,yml}'
    purpose: Incident policy documentation
knowledge_sources:
  specifications:
  - id: itil-priority-matrix
    name: ITIL Priority Matrix (Impact x Urgency)
    url: https://www.axelos.com/certifications/itil-service-management
    offline_cache: true
    priority: required
  - id: google-sre-severity
    name: Google SRE - Severity Definitions
    url: https://sre.google/sre-book/managing-incidents/
    offline_cache: true
    priority: required
  guides:
  - id: pagerduty-severity-guide
    name: PagerDuty Severity Levels Guide
    url: https://support.pagerduty.com/docs/incident-priority
    offline_cache: true
tooling:
  static_analysis:
  - tool: incident-classifier-analyzer
    purpose: Analyze classification consistency
    offline_capable: true
  scripts:
  - id: classification-consistency-check
    language: bash
    purpose: Identify classification documentation
    source: inline
    code: |
      #!/bin/bash
      echo "=== Classification Documentation Scan ==="
      grep -rniE "(sev|severity|priority|p[0-4]).*(definition|criteria|level)" \
        --include="*.md" --include="*.yaml" . 2>/dev/null | head -30
signals:
  critical:
  - id: IC-CRIT-001
    signal: No incident severity classification system defined
    evidence_indicators:
    - No severity levels documented
    - All incidents treated identically
    - No classification criteria exist
    explanation: |
      Without severity classification, organizations cannot prioritize response
      efforts effectively. Critical business-impacting incidents may be deprioritized
      while minor issues consume resources. ITIL and SRE frameworks both require
      structured classification systems.
    remediation: |
      - Define 4-5 severity levels with clear names
      - Document business impact criteria for each level
      - Create urgency assessment guidelines
      - Specify response time expectations per level
      - Train all responders on classification
  - id: IC-CRIT-002
    signal: Classification criteria are subjective and ambiguous
    evidence_indicators:
    - Vague terms like 'significant impact' without quantification
    - No customer impact metrics defined
    - Frequent classification disputes
    explanation: |
      Ambiguous classification criteria lead to inconsistent severity assignments
      depending on who classifies the incident. This undermines metrics accuracy
      and may cause inappropriate response escalation or delays.
    remediation: |
      - Quantify impact criteria (% users affected, revenue impact)
      - Define specific service degradation thresholds
      - Create classification decision trees
      - Document example incidents for each level
      - Establish classification review process
  high:
  - id: IC-HIGH-001
    signal: No response SLAs defined per severity level
    evidence_indicators:
    - Response expectations not documented
    - Same response time for all severities
    - No acknowledgment time targets
    explanation: |
      Without response SLAs, severity classification loses its operational purpose.
      SEV1 incidents should have aggressive response times while SEV4 may wait
      until business hours. ITIL defines this as the priority matrix outcome.
    remediation: |
      - Define acknowledgment SLA per severity (e.g., SEV1: 5 min)
      - Establish resolution targets per severity
      - Configure alerting to match SLAs
      - Track SLA adherence metrics
      - Review and adjust SLAs quarterly
  - id: IC-HIGH-002
    signal: Classification does not consider business impact
    evidence_indicators:
    - Technical metrics only (CPU, errors)
    - No revenue or customer impact criteria
    - Support ticket volume not considered
    explanation: |
      Technical symptoms do not always correlate with business impact. A high
      CPU alert may have zero customer impact, while a subtle data corruption
      issue may have severe business consequences. Classification must include
      business impact assessment.
    remediation: |
      - Add business impact criteria (revenue, customers, compliance)
      - Define customer-facing vs internal impact differences
      - Include reputation impact consideration
      - Create impact quantification guidelines
      - Train teams on business impact assessment
  medium:
  - id: IC-MED-001
    signal: Classification accuracy not measured
    evidence_indicators:
    - No post-incident classification review
    - Classification disputes not tracked
    - No accuracy metrics maintained
    explanation: |
      Without measuring classification accuracy, organizations cannot identify
      systematic bias or training gaps. Regular review of initial vs final
      classification helps improve the classification system.
    remediation: |
      - Compare initial and final severity in post-mortems
      - Track classification change frequency
      - Identify common misclassification patterns
      - Update criteria based on findings
      - Provide targeted training for problem areas
  - id: IC-MED-002
    signal: No guidance for classification edge cases
    evidence_indicators:
    - Partial outages unclear
    - Cascading failure classification undefined
    - Multi-service impact rules missing
    explanation: |
      Many incidents don't fit cleanly into severity categories. Without guidance
      for edge cases (partial outages, single-customer impact, cascading failures),
      classification becomes inconsistent.
    remediation: |
      - Document partial outage classification rules
      - Define multi-service impact handling
      - Create cascading failure guidelines
      - Establish single-customer impact criteria
      - Build classification FAQ from disputes
  low:
  - id: IC-LOW-001
    signal: Classification not automated where possible
    evidence_indicators:
    - Manual classification for all incidents
    - No auto-classification from monitoring
    - Alert severity disconnected from incident severity
    explanation: |
      While final classification may require human judgment, initial classification
      can often be automated based on alert source, affected service, and error rates.
      This accelerates response and ensures consistency.
    remediation: |
      - Configure auto-classification rules for common scenarios
      - Map alert priorities to incident severities
      - Implement classification suggestions
      - Allow human override with justification
      - Monitor auto-classification accuracy
  positive:
  - id: IC-POS-001
    signal: Comprehensive severity classification system
    evidence_indicators:
    - Clear severity levels with quantified criteria
    - Response SLAs per severity level
    - Regular classification accuracy review
  - id: IC-POS-002
    signal: Business-aligned classification criteria
    evidence_indicators:
    - Customer impact quantification
    - Revenue impact thresholds
    - Multi-dimensional classification matrix
procedure:
  context:
    cognitive_mode: analytical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Classification System Review
    description: |
      Examine severity classification definitions and criteria documentation.
    duration_estimate: 30 min
    commands:
    - purpose: Find severity definitions
      command: |
        find . -type f \( -name "*severity*" -o -name "*priority*" -o -name "*classification*" \) \
          \( -name "*.md" -o -name "*.yaml" \) 2>/dev/null | head -20
    - purpose: Search for severity criteria
      command: |
        grep -rniE "(sev|severity|priority)[_-]?[0-5].*(definition|criteria|when)" \
          --include="*.md" --include="*.yaml" . 2>/dev/null | head -30
    expected_findings:
    - Severity level definitions
    - Classification criteria
    - Decision trees or matrices
  - id: '2'
    name: Response SLA Assessment
    description: |
      Verify response time SLAs are defined for each severity level.
    duration_estimate: 30 min
    commands:
    - purpose: Find response time SLAs
      command: |
        grep -rniE "(response|acknowledge|resolve).*(time|sla|target|minutes|hours)" \
          --include="*.md" --include="*.yaml" . 2>/dev/null | head -20
    expected_findings:
    - Acknowledgment time targets
    - Resolution time expectations
    - Escalation timeframes
  - id: '3'
    name: Business Impact Criteria Review
    description: |
      Assess whether classification considers business impact metrics.
    duration_estimate: 30 min
    commands:
    - purpose: Search for business impact criteria
      command: |
        grep -rniE "(customer|revenue|user|business).*(impact|affected|loss)" \
          --include="*.md" --include="*.yaml" . 2>/dev/null | head -20
    expected_findings:
    - Customer impact thresholds
    - Revenue impact criteria
    - User percentage thresholds
  - id: '4'
    name: Classification Tooling Review
    description: |
      Examine how classification is implemented in incident tools.
    duration_estimate: 30 min
    commands:
    - purpose: Find incident tool priority configs
      command: |
        grep -rniE "(urgency|severity|priority).*(high|medium|low|critical)" \
          --include="*.yaml" --include="*.json" . 2>/dev/null | head -20
    expected_findings:
    - Tool priority configurations
    - Auto-classification rules
    - Priority mapping definitions
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Classification System Assessment
    - SLA Coverage Review
    - Business Alignment Analysis
    - Recommendations
  - type: classification_matrix
    format: table
    description: Severity levels with criteria and SLAs
  confidence_guidance:
    high: Classification system fully documented and configured
    medium: Partial documentation, some criteria inferred
    low: Classification system unclear, requires interviews
offline:
  capability: full
  cache_manifest:
    knowledge:
    - source_id: itil-priority-matrix
      priority: required
    - source_id: google-sre-severity
      priority: required
profiles:
  membership:
    quick:
      included: false
      reason: Classification audit requires comprehensive review
    full:
      included: true
      priority: 2
    production:
      included: true
      priority: 2
closeout_checklist:
- id: ic-001
  item: Severity levels are defined and documented
  level: CRITICAL
  verification: |
    grep -rniE "(sev|severity|priority)[_-]?[0-5]" \
      --include="*.md" --include="*.yaml" . 2>/dev/null | \
      wc -l | xargs -I{} test {} -gt 0 && echo "PASS" || echo "FAIL"
  expected: PASS
- id: ic-002
  item: Classification criteria are quantified
  level: BLOCKING
  verification: manual
  verification_notes: Verify criteria include specific metrics and thresholds
  expected: Confirmed by reviewer
- id: ic-003
  item: Response SLAs defined per severity
  level: BLOCKING
  verification: manual
  verification_notes: Verify acknowledgment and resolution SLAs exist per level
  expected: Confirmed by reviewer
- id: ic-004
  item: Business impact is considered in classification
  level: WARNING
  verification: |
    grep -rniE "(customer|revenue|business).*(impact|criteria)" \
      --include="*.md" --include="*.yaml" . 2>/dev/null | \
      wc -l | xargs -I{} test {} -gt 0 && echo "PASS" || echo "NEEDS_REVIEW"
  expected: PASS
governance:
  applicable_to:
    archetypes:
    - all
  compliance_frameworks:
  - framework: ITIL 4
    controls:
    - Incident Management
    - Priority Matrix
  - framework: SOC 2
    controls:
    - CC7.4
  - framework: ISO 27001
    controls:
    - A.16.1.1
    - A.16.1.4
relationships:
  commonly_combined:
  - operational-excellence.incident-management.incident-response-process
  - operational-excellence.incident-management.escalation-procedures
  - operational-excellence.service-level-management.slo-definition
