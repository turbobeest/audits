# ============================================================
# AUDIT: Human Oversight
# ============================================================
# Evaluates mechanisms for meaningful human oversight of AI
# systems and intervention capabilities.

audit:
  id: "ethical-societal.ai-algorithm-ethics.human-oversight"

  name: "Human Oversight"

  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "ethical-societal"
  category_number: 18
  subcategory: "ai-algorithm-ethics"

  tier: "expert"
  estimated_duration: "3-5 hours"  # median: 4h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "process"

  default_profiles:
    - "full"
    - "security"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Assesses human oversight mechanisms for AI systems including human-in-the-loop
    (human makes decisions), human-on-the-loop (human monitors and can intervene),
    and human-in-command (human has full override authority). Evaluates whether
    oversight is meaningful (not mere rubber-stamping) and whether humans have
    the tools, training, and authority to intervene effectively.

  why_it_matters: |
    Meaningful human oversight is essential for AI accountability and safety.
    EU AI Act requires human oversight measures for high-risk AI systems.
    Oversight must be more than nominal - humans must have real capability
    to understand, monitor, and override AI decisions. Automation bias can
    undermine oversight if not actively addressed through system design.

  when_to_run:
    - "When designing high-risk AI systems"
    - "Before deploying autonomous systems"
    - "During AI governance assessments"
    - "Following AI-related incidents"

prerequisites:
  required_artifacts:
    - type: "system_documentation"
      description: "AI system architecture and oversight design"
    - type: "operational_procedures"
      description: "Human oversight procedures and escalation paths"

  access_requirements:
    - "System design documentation"
    - "Operational procedures and training materials"
    - "Access to operators for interviews"

discovery:
  code_patterns:
    - pattern: "(human_review|manual_override|escalate|intervention)"
      type: "regex"
      scope: "source"
      purpose: "Find human intervention points"
    - pattern: "(approval_required|requires_human|wait_for_approval)"
      type: "regex"
      scope: "source"
      purpose: "Locate human-in-the-loop gates"
    - pattern: "(emergency_stop|kill_switch|disable|halt)"
      type: "regex"
      scope: "source"
      purpose: "Find emergency override mechanisms"

  file_patterns:
    - glob: "**/override*"
      purpose: "Find override mechanisms"
    - glob: "**/escalation*"
      purpose: "Locate escalation procedures"

  interviews:
    - role: "AI system operators"
      questions:
        - "How do you monitor the AI system's decisions?"
        - "When and how do you intervene?"
        - "Do you feel you have adequate tools and authority?"
      purpose: "Assess operator experience and capability"
    - role: "System designers"
      questions:
        - "What oversight mechanisms are built in?"
        - "How do you prevent automation bias?"
        - "What training do operators receive?"
      purpose: "Understand design intent for oversight"

knowledge_sources:
  specifications:
    - id: "cisa-ai-security"
      name: "CISA Guidelines for Secure AI System Development"
      url: "https://www.cisa.gov/resources-tools/resources/guidelines-secure-ai-system-development"
      offline_cache: true
      priority: "recommended"
    - id: "eu-ai-act-art14"
      name: "EU AI Act Article 14 - Human Oversight"
      url: "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206"
      offline_cache: true
      priority: "required"

  guides:
    - id: "hitl-guidance"
      name: "Human-in-the-Loop AI Design Guidelines"
      url: "https://pair.withgoogle.com/chapter/human-ai-interaction/"
      offline_cache: true
    - id: "automation-bias"
      name: "Mitigating Automation Bias in AI Systems"
      url: "https://www.nist.gov/itl/ai-risk-management-framework"
      offline_cache: true

  papers:
    - id: "meaningful-oversight"
      title: "Meaningful Human Control over Autonomous Systems"
      url: "https://link.springer.com/article/10.1007/s10676-019-09519-y"

tooling:
  assessment_tools:
    - tool: "Human oversight assessment framework"
      purpose: "Evaluate oversight mechanism adequacy"
    - tool: "Automation bias evaluation"
      purpose: "Assess risk of automation bias"

  scripts:
    - id: "oversight-finder"
      language: "bash"
      purpose: "Find human oversight mechanisms in code"
      source: "inline"
      code: |
        #!/bin/bash
        echo "=== Human-in-the-Loop Points ==="
        grep -r -E "(human_review|manual_override|requires_approval)" \
          --include="*.py" --include="*.ts" --include="*.java" . 2>/dev/null

        echo "=== Emergency Controls ==="
        grep -r -E "(emergency_stop|kill_switch|disable_system|halt)" \
          --include="*.py" --include="*.ts" --include="*.java" . 2>/dev/null

signals:
  critical:
    - id: "OVER-CRIT-001"
      signal: "No human oversight mechanism for high-risk AI system"
      evidence_indicators:
        - "System is classified as high-risk under EU AI Act"
        - "No human review or intervention capability"
        - "Fully autonomous operation without monitoring"
      explanation: |
        EU AI Act Article 14 requires human oversight measures for high-risk
        AI. Systems operating without oversight cannot be monitored for errors,
        bias, or harm. Human oversight is fundamental to AI accountability.
      remediation: "Implement human oversight mechanisms appropriate to risk level"

    - id: "OVER-CRIT-002"
      signal: "Override mechanisms exist but are inaccessible in practice"
      evidence_indicators:
        - "Override requires technical expertise operators lack"
        - "Override latency too high for effective intervention"
        - "Operators not trained on override procedures"
      explanation: |
        Nominal oversight without practical accessibility is not meaningful
        oversight. Operators must be able to actually exercise control.
      remediation: "Redesign overrides for practical accessibility and train operators"

  high:
    - id: "OVER-HIGH-001"
      signal: "Automation bias not addressed in system design"
      explanation: "Humans tend to defer to automated recommendations without scrutiny"
      remediation: "Implement automation bias mitigations (friction, uncertainty display, etc.)"

    - id: "OVER-HIGH-002"
      signal: "Operators lack information to make informed decisions"
      explanation: "Meaningful oversight requires understanding AI recommendations"
      remediation: "Provide operators with explainability tools and decision context"

  medium:
    - id: "OVER-MED-001"
      signal: "Oversight workload unsustainable for operators"
      remediation: "Optimize alert volumes and prioritize high-risk decisions"

    - id: "OVER-MED-002"
      signal: "No audit log of human oversight actions"
      remediation: "Log all oversight decisions and interventions"

  low:
    - id: "OVER-LOW-001"
      signal: "Operator training not regularly refreshed"
      remediation: "Establish regular oversight training and competency verification"

  positive:
    - id: "OVER-POS-001"
      signal: "Human-in-the-loop for all high-impact decisions"
    - id: "OVER-POS-002"
      signal: "Operators trained and empowered to override"
    - id: "OVER-POS-003"
      signal: "Emergency stop mechanism tested regularly"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Map oversight requirements"
      description: |
        Determine oversight requirements based on AI system risk level
        and regulatory requirements. Identify oversight type needed.
      duration_estimate: "30 min"
      questions:
        - "What is the risk classification of this AI system?"
        - "What oversight level is required (HITL, HOTL, HIC)?"
        - "What regulatory requirements apply?"
      expected_findings:
        - "Oversight requirements specification"
        - "Regulatory obligations"

    - id: "2"
      name: "Assess oversight mechanisms"
      description: |
        Evaluate implemented oversight mechanisms against requirements.
        Review intervention points, override capabilities, and monitoring.
      duration_estimate: "60 min"
      commands:
        - purpose: "Find oversight points in code"
          command: "grep -r -E '(human_review|manual_override|escalate)' --include='*.py' --include='*.ts' ."
      expected_findings:
        - "Oversight mechanism inventory"
        - "Gap analysis against requirements"

    - id: "3"
      name: "Evaluate practical accessibility"
      description: |
        Assess whether oversight mechanisms are practically accessible
        to operators. Test intervention latency and ease of use.
      duration_estimate: "60 min"
      questions:
        - "Can operators access overrides in time to intervene?"
        - "Do operators have necessary permissions and tools?"
        - "Is the interface understandable and usable?"
      expected_findings:
        - "Accessibility assessment"
        - "Practical barriers to oversight"

    - id: "4"
      name: "Assess automation bias mitigations"
      description: |
        Evaluate system design for automation bias mitigations.
        Check if humans are encouraged to exercise independent judgment.
      duration_estimate: "45 min"
      expected_findings:
        - "Automation bias risk assessment"
        - "Mitigation effectiveness"

    - id: "5"
      name: "Review operator training and competency"
      description: |
        Evaluate operator training programs and competency verification.
        Assess whether operators are prepared for effective oversight.
      duration_estimate: "45 min"
      questions:
        - "What training do operators receive?"
        - "How is competency verified?"
        - "When was training last updated?"
      expected_findings:
        - "Training adequacy assessment"
        - "Competency gaps"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "oversight_assessment"
      format: "structured"
      sections:
        - "Oversight Requirements"
        - "Mechanism Evaluation"
        - "Accessibility Assessment"
        - "Automation Bias Analysis"
        - "Training Assessment"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Key Findings"
        - "Recommendations"

  confidence_guidance:
    high: "Complete mechanism review and operator interviews"
    medium: "Mechanism review with limited operator engagement"
    low: "Based on documentation review only"

offline:
  capability: "full"

  cache_manifest:
    knowledge:
      - source_id: "eu-ai-act-art14"
        priority: "required"
      - source_id: "hitl-guidance"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires comprehensive oversight assessment"
    full:
      included: true
      priority: 1

closeout_checklist:
  - id: "over-001"
    item: "Human oversight requirements documented"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Oversight requirements defined based on risk level"
    expected: "Confirmed by reviewer"

  - id: "over-002"
    item: "Oversight mechanisms implemented"
    level: "CRITICAL"
    verification: "grep -r -E '(human_review|override|escalate)' --include='*.py' --include='*.ts' . | wc -l | xargs -I{} test {} -gt 0 && echo PASS || echo FAIL"
    expected: "PASS"

  - id: "over-003"
    item: "Emergency stop mechanism exists and tested"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Emergency override capability verified"
    expected: "Confirmed by reviewer"

  - id: "over-004"
    item: "Operators trained on oversight procedures"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Training records and competency verification exist"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["ai-ml-systems", "autonomous-systems", "high-risk-ai"]

  compliance_frameworks:
    - framework: "EU AI Act"
      controls: ["Article 14"]
    - framework: "NIST AI RMF"
      controls: ["GOVERN 1.4", "MANAGE 4.1"]

relationships:
  commonly_combined:
    - "ethical-societal.ai-algorithm-ethics.ai-safety-guardrails"
    - "ethical-societal.ai-algorithm-ethics.automated-decision-impact"
    - "ethical-societal.ai-algorithm-ethics.explainability"
