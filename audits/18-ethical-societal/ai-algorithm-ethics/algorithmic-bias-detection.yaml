# ============================================================
# AUDIT: Algorithmic Bias Detection
# ============================================================
# Evaluates AI/ML systems for discriminatory patterns and
# unfair treatment of protected demographic groups.

audit:
  id: "ethical-societal.ai-algorithm-ethics.algorithmic-bias-detection"

  name: "Algorithmic Bias Detection"

  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "ethical-societal"
  category_number: 18
  subcategory: "ai-algorithm-ethics"

  tier: "expert"
  estimated_duration: "4-8 hours"  # median: 6h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "codebase"

  default_profiles:
    - "full"
    - "security"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Examines AI/ML systems for algorithmic bias across protected demographic
    categories including race, gender, age, disability, and socioeconomic status.
    Analyzes training data composition, model outputs, and decision patterns
    to identify systematic discriminatory outcomes or disparate impact.

  why_it_matters: |
    Algorithmic bias can cause significant harm to individuals and communities,
    leading to discriminatory lending, hiring, healthcare, and criminal justice
    decisions. Beyond ethical concerns, biased systems expose organizations to
    regulatory penalties under EU AI Act, civil rights litigation, and severe
    reputational damage. Fair AI is both a moral imperative and legal requirement.

  when_to_run:
    - "Before deploying any AI/ML model making decisions affecting individuals"
    - "After significant model updates or retraining"
    - "When new protected categories are added to scope"
    - "Following reports of discriminatory outcomes"
    - "Annually for high-risk AI systems under EU AI Act"

prerequisites:
  required_artifacts:
    - type: "model_artifacts"
      description: "Access to trained ML models and model cards"
    - type: "training_data"
      description: "Training dataset or demographic distribution metadata"
    - type: "prediction_outputs"
      description: "Historical predictions with demographic labels"

  access_requirements:
    - "Read access to model training code and configuration"
    - "Access to demographic metadata for test populations"
    - "Ability to run model inference on test datasets"

discovery:
  code_patterns:
    - pattern: "(protected_attribute|sensitive_feature|demographic|race|gender|age|disability)"
      type: "regex"
      scope: "source"
      purpose: "Identify handling of protected attributes in model code"
    - pattern: "(fairness|bias|disparity|parity|equalized_odds)"
      type: "regex"
      scope: "source"
      purpose: "Find existing fairness metrics or bias mitigation code"
    - pattern: "(aif360|fairlearn|themis|ml-fairness)"
      type: "regex"
      scope: "config"
      purpose: "Detect use of fairness toolkits"

  file_patterns:
    - glob: "**/models/**"
      purpose: "Locate model definitions and training code"
    - glob: "**/fairness/**"
      purpose: "Find fairness evaluation modules"
    - glob: "**/*model_card*"
      purpose: "Locate model documentation"

knowledge_sources:
  specifications:
    - id: "cisa-ai-security"
      name: "CISA Guidelines for Secure AI System Development"
      url: "https://www.cisa.gov/resources-tools/resources/guidelines-secure-ai-system-development"
      offline_cache: true
      priority: "recommended"
    - id: "eu-ai-act"
      name: "EU Artificial Intelligence Act"
      url: "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206"
      offline_cache: true
      priority: "required"
    - id: "nist-ai-rmf"
      name: "NIST AI Risk Management Framework"
      url: "https://www.nist.gov/itl/ai-risk-management-framework"
      offline_cache: true
      priority: "required"

  guides:
    - id: "fairlearn-guide"
      name: "Fairlearn User Guide"
      url: "https://fairlearn.org/main/user_guide/"
      offline_cache: true
    - id: "google-ml-fairness"
      name: "Google ML Fairness Guide"
      url: "https://developers.google.com/machine-learning/fairness-overview"
      offline_cache: true

  papers:
    - id: "disparate-impact"
      title: "Certifying and Removing Disparate Impact"
      url: "https://arxiv.org/abs/1412.3756"
    - id: "fairness-definitions"
      title: "Fairness Definitions Explained"
      url: "https://fairware.cs.umass.edu/papers/Verma.pdf"

tooling:
  static_analysis:
    - tool: "fairlearn"
      purpose: "Compute fairness metrics and mitigation algorithms"
      offline_capable: true
    - tool: "aif360"
      purpose: "IBM AI Fairness 360 toolkit for bias detection"
      offline_capable: true
    - tool: "aequitas"
      purpose: "Bias and fairness audit toolkit"
      offline_capable: true

  scripts:
    - id: "bias-metrics"
      language: "python"
      purpose: "Calculate standard bias metrics across demographic groups"
      source: "inline"
      code: |
        # Requires: pip install fairlearn pandas numpy
        from fairlearn.metrics import (
            demographic_parity_difference,
            equalized_odds_difference,
            MetricFrame
        )
        import pandas as pd

        def compute_bias_metrics(y_true, y_pred, sensitive_features):
            """Compute standard fairness metrics."""
            metrics = {
                'demographic_parity_diff': demographic_parity_difference(
                    y_true, y_pred, sensitive_features=sensitive_features
                ),
                'equalized_odds_diff': equalized_odds_difference(
                    y_true, y_pred, sensitive_features=sensitive_features
                )
            }

            mf = MetricFrame(
                metrics={'selection_rate': lambda y_t, y_p: y_p.mean()},
                y_true=y_true,
                y_pred=y_pred,
                sensitive_features=sensitive_features
            )

            return metrics, mf.by_group

signals:
  critical:
    - id: "BIAS-CRIT-001"
      signal: "Demographic parity difference exceeds 0.1 (10%)"
      evidence_threshold: "demographic_parity_diff > 0.1"
      explanation: |
        A demographic parity difference greater than 10% indicates significant
        disparity in positive outcome rates between demographic groups, which
        may constitute illegal discrimination under civil rights law.
      remediation: "Apply bias mitigation algorithms (reweighting, resampling, or postprocessing)"

    - id: "BIAS-CRIT-002"
      signal: "Four-fifths rule violation detected"
      evidence_threshold: "selection_rate_ratio < 0.8"
      explanation: |
        The 80% rule (four-fifths rule) from US EEOC guidelines states that
        selection rates for protected groups should be at least 80% of the
        highest rate. Violation indicates potential disparate impact.
      remediation: "Review feature selection and apply fairness constraints during training"

  high:
    - id: "BIAS-HIGH-001"
      signal: "Equalized odds difference exceeds 0.05"
      explanation: "Model has different true positive or false positive rates across groups"
      remediation: "Apply equalized odds postprocessing or constrained optimization"

    - id: "BIAS-HIGH-002"
      signal: "Protected attributes used directly in model features"
      evidence_pattern: "(protected_class|race|gender|age).*feature"
      explanation: "Direct use of protected attributes can constitute disparate treatment"
      remediation: "Remove protected attributes from model features; audit for proxy variables"

  medium:
    - id: "BIAS-MED-001"
      signal: "No fairness metrics computed during model evaluation"
      remediation: "Add fairness metric calculation to model evaluation pipeline"

    - id: "BIAS-MED-002"
      signal: "Training data demographics not documented"
      remediation: "Create data documentation with demographic distribution analysis"

  low:
    - id: "BIAS-LOW-001"
      signal: "No model card documenting fairness considerations"
      remediation: "Create model card following Model Cards for Model Reporting framework"

  positive:
    - id: "BIAS-POS-001"
      signal: "Fairness constraints integrated into model training"
    - id: "BIAS-POS-002"
      signal: "Comprehensive fairness testing in CI/CD pipeline"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Identify AI/ML components"
      description: |
        Locate all AI/ML models in the codebase that make decisions affecting
        individuals. Document model purposes, input features, and outputs.
      duration_estimate: "30 min"
      commands:
        - purpose: "Find model definitions"
          command: "find . -type f \\( -name '*.py' -o -name '*.ipynb' \\) -exec grep -l -E '(sklearn|tensorflow|pytorch|keras|xgboost)' {} \\;"
        - purpose: "Locate model artifacts"
          command: "find . -type f \\( -name '*.pkl' -o -name '*.h5' -o -name '*.pt' -o -name '*.onnx' \\)"
      expected_findings:
        - "List of ML models and their purposes"
        - "Model input features and protected attribute handling"

    - id: "2"
      name: "Analyze training data demographics"
      description: |
        Review training data composition for demographic representation.
        Identify over/under-representation of protected groups.
      duration_estimate: "60 min"
      questions:
        - "What protected demographic groups are represented in training data?"
        - "Are any groups significantly under-represented (< 5%)?"
        - "How was data collected and potential selection bias?"
      expected_findings:
        - "Training data demographic distribution"
        - "Potential representation gaps"

    - id: "3"
      name: "Compute fairness metrics"
      description: |
        Run fairness metrics across protected demographic categories using
        fairlearn or equivalent toolkit. Calculate demographic parity,
        equalized odds, and calibration metrics.
      duration_estimate: "90 min"
      commands:
        - purpose: "Run fairness analysis script"
          command: "python -m fairlearn.widget --dataset ${TEST_DATASET} --model ${MODEL_PATH}"
      expected_findings:
        - "Quantified fairness metrics by demographic group"
        - "Identification of biased decision patterns"

    - id: "4"
      name: "Test for proxy discrimination"
      description: |
        Analyze whether non-protected features serve as proxies for protected
        attributes. Check for high correlation between features and demographics.
      duration_estimate: "60 min"
      expected_findings:
        - "Features correlated with protected attributes"
        - "Indirect discrimination pathways"

    - id: "5"
      name: "Document findings and recommendations"
      description: |
        Compile bias audit findings, metric results, and remediation
        recommendations. Reference applicable legal requirements.
      duration_estimate: "45 min"
      expected_findings:
        - "Comprehensive bias audit report"
        - "Prioritized remediation roadmap"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "fairness_report"
      format: "structured"
      sections:
        - "Demographic Representation Analysis"
        - "Fairness Metrics by Group"
        - "Proxy Discrimination Analysis"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Key Findings"
        - "Legal Compliance Assessment"
        - "Recommendations"

  confidence_guidance:
    high: "Quantitative fairness metrics computed on representative test data"
    medium: "Metrics computed but test data representativeness uncertain"
    low: "Unable to compute metrics; based on code review only"

offline:
  capability: "partial"

  cache_manifest:
    knowledge:
      - source_id: "eu-ai-act"
        priority: "required"
      - source_id: "nist-ai-rmf"
        priority: "required"
      - source_id: "fairlearn-guide"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires significant data analysis and metric computation"
    full:
      included: true
      priority: 1
    security:
      included: true
      priority: 2

closeout_checklist:
  - id: "bias-001"
    item: "All ML models affecting individuals identified and documented"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Reviewer confirms complete model inventory"
    expected: "Confirmed by reviewer"

  - id: "bias-002"
    item: "Fairness metrics computed for all high-risk models"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Verify demographic_parity_diff and equalized_odds metrics exist"
    expected: "Confirmed by reviewer"

  - id: "bias-003"
    item: "Four-fifths rule compliance verified"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Selection rate ratios >= 0.8 for all protected groups"
    expected: "Confirmed by reviewer"

  - id: "bias-004"
    item: "No direct use of protected attributes in model features"
    level: "BLOCKING"
    verification: "grep -r -E '(race|gender|ethnicity|age|disability).*=.*feature' --include='*.py' | wc -l | xargs -I{} test {} -eq 0 && echo PASS || echo FAIL"
    expected: "PASS"

governance:
  applicable_to:
    archetypes: ["ai-ml-systems", "decision-automation", "fintech", "healthcare"]

  compliance_frameworks:
    - framework: "EU AI Act"
      controls: ["Article 9 - Risk Management", "Article 10 - Data Governance"]
    - framework: "NIST AI RMF"
      controls: ["GOVERN 1.5", "MAP 2.3", "MEASURE 2.7"]

relationships:
  commonly_combined:
    - "ethical-societal.ai-algorithm-ethics.model-fairness"
    - "ethical-societal.ai-algorithm-ethics.ai-transparency"
    - "ethical-societal.ai-algorithm-ethics.explainability"
