# ============================================================
# AUDIT: Explainability
# ============================================================
# Evaluates AI/ML systems for interpretability and the ability
# to explain decisions to affected stakeholders.

audit:
  id: "ethical-societal.ai-algorithm-ethics.explainability"

  name: "Explainability"

  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "ethical-societal"
  category_number: 18
  subcategory: "ai-algorithm-ethics"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "codebase"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Assesses AI/ML system explainability including global model interpretability
    (understanding overall model behavior), local explanations (explaining
    individual predictions), and explanation quality (accuracy, completeness,
    understandability). Evaluates both technical explainability methods and
    stakeholder-appropriate explanation delivery.

  why_it_matters: |
    Explainability enables meaningful human oversight, informed consent, and
    effective recourse for affected individuals. GDPR Article 22 provides right
    to explanation for automated decisions. EU AI Act requires high-risk AI to
    be interpretable. Beyond compliance, explainability builds trust, enables
    debugging, and reveals potential biases or errors in model reasoning.

  when_to_run:
    - "When designing high-risk AI systems"
    - "Before deploying decision-making AI"
    - "When users request explanations for AI decisions"
    - "During model validation and testing"

prerequisites:
  required_artifacts:
    - type: "model_artifacts"
      description: "Trained ML models with prediction capabilities"
    - type: "test_data"
      description: "Representative examples for explanation generation"

  access_requirements:
    - "Model inference access"
    - "Feature engineering pipeline documentation"
    - "User interaction logs for explanation needs assessment"

discovery:
  code_patterns:
    - pattern: "(shap|lime|eli5|captum|interpret)"
      type: "regex"
      scope: "source"
      purpose: "Find explainability library usage"
    - pattern: "(feature_importance|attribution|saliency)"
      type: "regex"
      scope: "source"
      purpose: "Locate feature importance calculations"
    - pattern: "(explain|interpretation|reasoning)"
      type: "regex"
      scope: "source"
      purpose: "Find explanation generation code"

  file_patterns:
    - glob: "**/explain*"
      purpose: "Find explainability modules"
    - glob: "**/interpret*"
      purpose: "Locate interpretation code"

knowledge_sources:
  specifications:
    - id: "cisa-ai-security"
      name: "CISA Guidelines for Secure AI System Development"
      url: "https://www.cisa.gov/resources-tools/resources/guidelines-secure-ai-system-development"
      offline_cache: true
      priority: "recommended"
    - id: "gdpr-art22"
      name: "GDPR Article 22 - Automated Decision-Making"
      url: "https://gdpr-info.eu/art-22-gdpr/"
      offline_cache: true
      priority: "required"
    - id: "eu-ai-act"
      name: "EU AI Act - Interpretability Requirements"
      url: "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206"
      offline_cache: true
      priority: "required"

  guides:
    - id: "shap-docs"
      name: "SHAP Documentation"
      url: "https://shap.readthedocs.io/"
      offline_cache: true
    - id: "lime-docs"
      name: "LIME Documentation"
      url: "https://lime-ml.readthedocs.io/"
      offline_cache: true

  papers:
    - id: "shap-paper"
      title: "A Unified Approach to Interpreting Model Predictions"
      url: "https://arxiv.org/abs/1705.07874"
    - id: "lime-paper"
      title: "Why Should I Trust You? Explaining the Predictions of Any Classifier"
      url: "https://arxiv.org/abs/1602.04938"

tooling:
  static_analysis:
    - tool: "shap"
      purpose: "SHAP value computation for feature attribution"
      offline_capable: true
    - tool: "lime"
      purpose: "Local interpretable model-agnostic explanations"
      offline_capable: true
    - tool: "captum"
      purpose: "Model interpretability for PyTorch"
      offline_capable: true
    - tool: "interpret"
      purpose: "Microsoft InterpretML toolkit"
      offline_capable: true

  scripts:
    - id: "explainability-eval"
      language: "python"
      purpose: "Generate and evaluate model explanations"
      source: "inline"
      code: |
        import shap
        import numpy as np

        def generate_explanations(model, X, feature_names=None):
            """Generate SHAP explanations for model predictions."""
            explainer = shap.Explainer(model)
            shap_values = explainer(X)

            # Global feature importance
            global_importance = np.abs(shap_values.values).mean(axis=0)

            # Local explanations for each instance
            local_explanations = []
            for i, instance in enumerate(X):
                explanation = {
                    'features': feature_names or list(range(len(instance))),
                    'contributions': shap_values.values[i].tolist(),
                    'base_value': shap_values.base_values[i]
                }
                local_explanations.append(explanation)

            return {
                'global_importance': global_importance.tolist(),
                'local_explanations': local_explanations
            }

signals:
  critical:
    - id: "EXPL-CRIT-001"
      signal: "No explanation capability for high-risk AI decisions"
      evidence_indicators:
        - "AI makes significant decisions about individuals"
        - "No explainability methods implemented"
        - "Unable to explain individual decisions when requested"
      explanation: |
        High-risk AI systems must be interpretable under EU AI Act. GDPR
        provides right to explanation for automated decisions. Inability
        to explain decisions prevents meaningful recourse and oversight.
      remediation: "Implement explainability methods (SHAP, LIME, or interpretable models)"

  high:
    - id: "EXPL-HIGH-001"
      signal: "Explanations technically accurate but user-incomprehensible"
      explanation: "Explanations must be meaningful to affected stakeholders, not just ML engineers"
      remediation: "Develop stakeholder-appropriate explanation formats and language"

    - id: "EXPL-HIGH-002"
      signal: "Post-hoc explanations inconsistent with actual model behavior"
      explanation: "Explanation methods can produce plausible but inaccurate attributions"
      remediation: "Validate explanation fidelity against model behavior"

  medium:
    - id: "EXPL-MED-001"
      signal: "Only global explanations available; no individual prediction explanations"
      remediation: "Implement local explanation methods for individual decisions"

    - id: "EXPL-MED-002"
      signal: "Explanation generation too slow for real-time use"
      remediation: "Pre-compute explanations or use faster approximation methods"

  low:
    - id: "EXPL-LOW-001"
      signal: "Explanation not logged with predictions"
      remediation: "Store explanations alongside predictions for audit trail"

  positive:
    - id: "EXPL-POS-001"
      signal: "Interpretable model architecture chosen where possible"
    - id: "EXPL-POS-002"
      signal: "Explanation quality evaluated with user studies"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Assess explainability requirements"
      description: |
        Determine explainability requirements based on use case, risk level,
        and regulatory requirements. Identify stakeholders who need explanations.
      duration_estimate: "30 min"
      questions:
        - "Who needs explanations for this system's decisions?"
        - "What level of detail do different stakeholders need?"
        - "What regulatory requirements apply?"
      expected_findings:
        - "Stakeholder explanation needs"
        - "Required explanation types"

    - id: "2"
      name: "Evaluate global interpretability"
      description: |
        Assess overall model interpretability. Evaluate model architecture,
        feature importance, and global behavior understanding.
      duration_estimate: "60 min"
      commands:
        - purpose: "Check for interpretability library usage"
          command: "grep -r -E '(shap|lime|eli5|interpret)' --include='*.py' ."
      expected_findings:
        - "Model interpretability assessment"
        - "Global explanation capabilities"

    - id: "3"
      name: "Test local explanations"
      description: |
        Generate and evaluate local explanations for individual predictions.
        Test explanation consistency and accuracy.
      duration_estimate: "90 min"
      expected_findings:
        - "Local explanation quality"
        - "Explanation fidelity to model"

    - id: "4"
      name: "Assess explanation accessibility"
      description: |
        Evaluate whether explanations are understandable to intended
        audiences. Test with representative stakeholder groups if possible.
      duration_estimate: "60 min"
      questions:
        - "Are explanations understandable to non-technical stakeholders?"
        - "Do explanations provide actionable information for recourse?"
        - "Are explanations available in appropriate languages and formats?"
      expected_findings:
        - "Explanation accessibility assessment"
        - "Communication gaps"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "explainability_report"
      format: "structured"
      sections:
        - "Explainability Requirements"
        - "Global Interpretability Assessment"
        - "Local Explanation Evaluation"
        - "Accessibility Assessment"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Key Findings"
        - "Recommendations"

  confidence_guidance:
    high: "Full explanation generation and validation completed"
    medium: "Explanation methods evaluated but limited validation"
    low: "Assessment based on code review only"

offline:
  capability: "partial"

  cache_manifest:
    knowledge:
      - source_id: "gdpr-art22"
        priority: "required"
      - source_id: "shap-docs"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires runtime explanation generation"
    full:
      included: true
      priority: 2

closeout_checklist:
  - id: "expl-001"
    item: "Explainability requirements documented"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Stakeholder explanation needs documented"
    expected: "Confirmed by reviewer"

  - id: "expl-002"
    item: "Explainability methods implemented"
    level: "CRITICAL"
    verification: "grep -r -E '(shap|lime|eli5|interpret|captum)' --include='*.py' . | wc -l | xargs -I{} test {} -gt 0 && echo PASS || echo FAIL"
    expected: "PASS"

  - id: "expl-003"
    item: "Local explanations available for individual predictions"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Individual predictions can be explained"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["ai-ml-systems", "decision-automation", "regulated"]

  compliance_frameworks:
    - framework: "GDPR"
      controls: ["Article 22"]
    - framework: "EU AI Act"
      controls: ["Article 13 - Transparency"]

relationships:
  commonly_combined:
    - "ethical-societal.ai-algorithm-ethics.ai-transparency"
    - "ethical-societal.ai-algorithm-ethics.human-oversight"
