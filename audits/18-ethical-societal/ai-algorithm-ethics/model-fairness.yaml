# ============================================================
# AUDIT: Model Fairness
# ============================================================
# Evaluates ML models against multiple fairness criteria and
# ensures equitable treatment across different population segments.

audit:
  id: "ethical-societal.ai-algorithm-ethics.model-fairness"

  name: "Model Fairness"

  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "ethical-societal"
  category_number: 18
  subcategory: "ai-algorithm-ethics"

  tier: "expert"
  estimated_duration: "3-6 hours"  # median: 4h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "codebase"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Assesses ML model fairness using multiple mathematical definitions including
    individual fairness (similar individuals treated similarly), group fairness
    (equal treatment across demographic groups), and counterfactual fairness
    (decisions unchanged by protected attribute changes). Evaluates trade-offs
    between different fairness criteria and model performance.

  why_it_matters: |
    Different fairness definitions may conflict - achieving perfect demographic
    parity can compromise individual fairness, and vice versa. Organizations must
    make explicit choices about which fairness criteria to prioritize based on
    context. Understanding these trade-offs is essential for defensible AI
    governance and meeting regulatory requirements.

  when_to_run:
    - "When designing fairness requirements for new ML systems"
    - "During model selection and hyperparameter tuning"
    - "When stakeholders disagree about fairness standards"
    - "For regulatory compliance documentation"

prerequisites:
  required_artifacts:
    - type: "model_artifacts"
      description: "Trained ML models with prediction capabilities"
    - type: "test_data"
      description: "Labeled test dataset with demographic attributes"
    - type: "fairness_requirements"
      description: "Documented fairness requirements and constraints"

  access_requirements:
    - "Model inference API or direct model access"
    - "Ground truth labels for test population"
    - "Stakeholder input on fairness priorities"

discovery:
  code_patterns:
    - pattern: "(individual_fairness|lipschitz|metric_fairness)"
      type: "regex"
      scope: "source"
      purpose: "Find individual fairness implementations"
    - pattern: "(group_fairness|statistical_parity|conditional_parity)"
      type: "regex"
      scope: "source"
      purpose: "Locate group fairness metrics"
    - pattern: "(counterfactual|causal_fairness|path_specific)"
      type: "regex"
      scope: "source"
      purpose: "Identify counterfactual fairness analysis"

  file_patterns:
    - glob: "**/fairness/**"
      purpose: "Find fairness evaluation modules"
    - glob: "**/tests/*fair*"
      purpose: "Locate fairness tests"

knowledge_sources:
  specifications:
    - id: "cisa-ai-security"
      name: "CISA Guidelines for Secure AI System Development"
      url: "https://www.cisa.gov/resources-tools/resources/guidelines-secure-ai-system-development"
      offline_cache: true
      priority: "recommended"
    - id: "eu-ai-act"
      name: "EU Artificial Intelligence Act"
      url: "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206"
      offline_cache: true
      priority: "required"

  guides:
    - id: "fairness-tutorial"
      name: "Fairness in Machine Learning - Tutorial"
      url: "https://fairmlbook.org/"
      offline_cache: true
    - id: "ms-fairness"
      name: "Microsoft Responsible AI Fairness Guide"
      url: "https://www.microsoft.com/en-us/ai/responsible-ai"
      offline_cache: true

  papers:
    - id: "impossibility-fairness"
      title: "Inherent Trade-Offs in the Fair Determination of Risk Scores"
      url: "https://arxiv.org/abs/1609.05807"
    - id: "individual-fairness"
      title: "Fairness Through Awareness"
      url: "https://arxiv.org/abs/1104.3913"

tooling:
  static_analysis:
    - tool: "fairlearn"
      purpose: "Compute multiple fairness metrics and visualizations"
      offline_capable: true
    - tool: "aif360"
      purpose: "Comprehensive fairness metric suite"
      offline_capable: true

  scripts:
    - id: "fairness-criteria"
      language: "python"
      purpose: "Evaluate model against multiple fairness criteria"
      source: "inline"
      code: |
        from fairlearn.metrics import (
            demographic_parity_difference,
            demographic_parity_ratio,
            equalized_odds_difference,
            equalized_odds_ratio
        )
        from sklearn.metrics import accuracy_score
        import pandas as pd

        def evaluate_fairness_criteria(y_true, y_pred, y_prob, sensitive_features):
            """Evaluate model against multiple fairness definitions."""
            results = {
                'group_fairness': {
                    'demographic_parity_diff': demographic_parity_difference(
                        y_true, y_pred, sensitive_features=sensitive_features
                    ),
                    'demographic_parity_ratio': demographic_parity_ratio(
                        y_true, y_pred, sensitive_features=sensitive_features
                    ),
                    'equalized_odds_diff': equalized_odds_difference(
                        y_true, y_pred, sensitive_features=sensitive_features
                    ),
                },
                'performance': {
                    'overall_accuracy': accuracy_score(y_true, y_pred),
                }
            }
            return results

signals:
  critical:
    - id: "FAIR-CRIT-001"
      signal: "No fairness criteria defined for high-risk AI system"
      evidence_indicators:
        - "Model makes decisions affecting individuals"
        - "No documented fairness requirements"
        - "No fairness metrics in evaluation pipeline"
      explanation: |
        High-risk AI systems under EU AI Act must have documented fairness
        requirements. Absence of defined criteria makes it impossible to
        verify fair treatment and exposes organization to regulatory risk.
      remediation: "Define explicit fairness criteria with stakeholder input"

  high:
    - id: "FAIR-HIGH-001"
      signal: "Fairness-accuracy trade-off not documented"
      explanation: "All fairness constraints impact model performance; trade-offs must be explicit"
      remediation: "Document fairness-accuracy Pareto frontier and justify chosen operating point"

    - id: "FAIR-HIGH-002"
      signal: "Conflicting fairness criteria not reconciled"
      explanation: "Different fairness definitions may be incompatible; conflicts must be resolved"
      remediation: "Document which criteria take priority and justification for choices"

  medium:
    - id: "FAIR-MED-001"
      signal: "Individual fairness not evaluated"
      remediation: "Implement similarity metric and verify similar treatment for similar individuals"

    - id: "FAIR-MED-002"
      signal: "Subgroup fairness not analyzed"
      remediation: "Evaluate fairness for intersectional subgroups (e.g., age + gender)"

  low:
    - id: "FAIR-LOW-001"
      signal: "Fairness criteria not communicated to stakeholders"
      remediation: "Create accessible documentation of fairness choices and rationale"

  positive:
    - id: "FAIR-POS-001"
      signal: "Multiple fairness criteria explicitly evaluated and documented"
    - id: "FAIR-POS-002"
      signal: "Stakeholder-informed fairness requirements process"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review fairness requirements"
      description: |
        Examine documented fairness requirements and criteria. Identify
        which fairness definitions have been chosen and rationale.
      duration_estimate: "30 min"
      questions:
        - "What fairness criteria are required for this system?"
        - "How were fairness requirements determined?"
        - "Who was consulted in defining fairness criteria?"
      expected_findings:
        - "Documented fairness requirements or gap"
        - "Stakeholder consultation records"

    - id: "2"
      name: "Evaluate group fairness"
      description: |
        Compute group fairness metrics including demographic parity,
        equalized odds, and calibration across protected groups.
      duration_estimate: "60 min"
      commands:
        - purpose: "Run group fairness evaluation"
          command: "python -c 'from fairlearn.metrics import MetricFrame; # compute group metrics'"
      expected_findings:
        - "Quantified group fairness metrics"
        - "Identification of disparities"

    - id: "3"
      name: "Assess individual fairness"
      description: |
        Evaluate whether similar individuals receive similar predictions.
        Define and apply similarity metric for the domain.
      duration_estimate: "60 min"
      expected_findings:
        - "Individual fairness violation instances"
        - "Similarity metric appropriateness"

    - id: "4"
      name: "Analyze subgroup fairness"
      description: |
        Examine fairness for intersectional subgroups to identify
        hidden disparities in combined demographic categories.
      duration_estimate: "45 min"
      expected_findings:
        - "Subgroup disparity analysis"
        - "Intersectional fairness issues"

    - id: "5"
      name: "Document trade-off analysis"
      description: |
        Map fairness-accuracy trade-offs and document chosen operating
        point with justification for the balance achieved.
      duration_estimate: "45 min"
      expected_findings:
        - "Pareto frontier visualization"
        - "Trade-off justification documentation"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "fairness_report"
      format: "structured"
      sections:
        - "Fairness Criteria Assessment"
        - "Group Fairness Metrics"
        - "Individual Fairness Analysis"
        - "Subgroup Analysis"
        - "Trade-off Documentation"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Key Findings"
        - "Recommendations"

  confidence_guidance:
    high: "All fairness criteria evaluated with representative data"
    medium: "Key criteria evaluated but gaps in coverage"
    low: "Limited evaluation due to data or access constraints"

offline:
  capability: "partial"

  cache_manifest:
    knowledge:
      - source_id: "eu-ai-act"
        priority: "required"
      - source_id: "fairness-tutorial"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires comprehensive fairness analysis"
    full:
      included: true
      priority: 2

closeout_checklist:
  - id: "fair-001"
    item: "Fairness criteria explicitly documented"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Verify documented fairness requirements exist"
    expected: "Confirmed by reviewer"

  - id: "fair-002"
    item: "Group fairness metrics computed"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Demographic parity and equalized odds evaluated"
    expected: "Confirmed by reviewer"

  - id: "fair-003"
    item: "Fairness-accuracy trade-offs documented"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Trade-off analysis with justification exists"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["ai-ml-systems", "decision-automation"]

  compliance_frameworks:
    - framework: "EU AI Act"
      controls: ["Article 9", "Article 10"]
    - framework: "NIST AI RMF"
      controls: ["MEASURE 2.7", "MANAGE 2.2"]

relationships:
  commonly_combined:
    - "ethical-societal.ai-algorithm-ethics.algorithmic-bias-detection"
    - "ethical-societal.ai-algorithm-ethics.explainability"
