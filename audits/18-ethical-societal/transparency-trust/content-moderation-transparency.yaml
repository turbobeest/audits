audit:
  id: ethical-societal.transparency-trust.content-moderation-transparency
  name: Content Moderation Transparency
  version: 1.0.0
  last_updated: '2026-01-19'
  status: active
  category: ethical-societal
  category_number: 18
  subcategory: transparency-trust
  tier: phd
  estimated_duration: 4-6 hours  # median: 5h
  completeness: complete
  requires_runtime: false
  destructive: false
execution:
  automatable: manual
  severity: high
  scope: organizational
  default_profiles:
  - full
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Examines transparency in content moderation systems, assessing:
    - Community guidelines/content policy clarity
    - Enforcement action explanations
    - Appeal process availability and fairness
    - Transparency reporting (removal statistics)
    - Automated moderation disclosure
    - Consistency of enforcement
    - Notice to affected users
    - Government/legal request handling
  why_it_matters: |
    Content moderation affects free expression and user trust,
    making transparency essential:
    - DSA mandates transparency reporting for platforms
    - Users cannot comply with unclear rules
    - Unexplained removals feel arbitrary and unfair
    - Appeal processes enable correction of errors
    - Transparency reports enable accountability
    - Automated moderation can have high error rates
    - Trust requires understanding how rules are applied

    Transparent content moderation balances safety with fairness
    and demonstrates platform accountability.
  when_to_run:
  - When establishing or updating content policies
  - Before launching user-generated content features
  - During DSA compliance preparation
  - When content moderation complaints increase
prerequisites:
  required_artifacts:
  - type: content_policies
    description: Community guidelines and content policies
  - type: moderation_documentation
    description: Internal moderation procedures
  access_requirements:
  - Published content policies
  - Moderation team procedures
  - Enforcement statistics if available
  - Appeal process documentation
discovery:
  documents_to_review:
  - type: Community Guidelines
    purpose: Review content policy clarity
  - type: Terms of Service
    purpose: Check content-related terms
  - type: Help Center articles on moderation
    purpose: Assess user guidance on rules
  - type: Transparency reports
    purpose: Review public reporting practices
  - type: Internal moderation playbooks
    purpose: Understand enforcement practices
  interviews:
  - role: Trust & Safety team
    questions:
    - How are moderation policies developed?
    - How is enforcement consistency ensured?
    - What automated moderation is used?
    purpose: Understand moderation practices
  - role: Legal/Policy team
    questions:
    - How are DSA transparency requirements met?
    - How are government requests handled?
    - What appeal rights do users have?
    purpose: Assess regulatory compliance
  - role: Customer support
    questions:
    - What moderation complaints are common?
    - How do users respond to enforcement actions?
    - What appeals succeed and why?
    purpose: Identify transparency gaps
  file_patterns:
  - glob: '**/ETHICS.md'
    purpose: Ethics documentation
  - glob: '**/PRIVACY*.md'
    purpose: Privacy documentation
  - glob: '**/terms*.md'
    purpose: Terms of service
  - glob: '**/policy*.md'
    purpose: Policy documentation
  code_patterns:
  - pattern: consent|privacy|gdpr|ccpa|cookie
    type: regex
    scope: all
    purpose: Privacy/consent references
knowledge_sources:
  specifications:
  - id: dsa-moderation
    name: Digital Services Act - Content Moderation
    url: https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package
    offline_cache: true
    priority: required
  guides:
  - id: santa-clara-principles
    name: Santa Clara Principles on Transparency
    url: https://santaclaraprinciples.org/
    offline_cache: true
  - id: trust-safety-transparency
    name: Trust & Safety Transparency Best Practices
    url: https://dtspartnership.org/
    offline_cache: true
  papers:
  - id: content-moderation-scale
    title: Content Moderation at Scale
    url: https://www.law.georgetown.edu/
  learning_resources:
  - id: content-moderation-fundamentals
    title: Content Moderation Fundamentals
    type: course
    reference: Trust & Safety Professional Association
tooling:
  assessment_tools:
  - tool: Policy Clarity Assessment Template
    purpose: Evaluate policy comprehensibility
  - tool: Transparency Report Checklist
    purpose: Assess reporting completeness
  - tool: Appeal Process Audit Template
    purpose: Evaluate appeal fairness
  scripts:
  - id: moderation-disclosure-check
    language: bash
    purpose: Find moderation-related documentation
    source: inline
    code: |
      #!/bin/bash
      # Find content moderation documentation
      find . -name "*.md" -o -name "*.txt" -o -name "*.html" | \
        xargs grep -l -i "community\|guidelines\|moderation\|content.?policy" 2>/dev/null | head -20
signals:
  critical:
  - id: MOD-CRIT-001
    signal: Content removed without any explanation to user
    evidence_indicators:
    - Removals with no notification
    - Generic 'policy violation' without specifics
    - No indication of which rule was violated
    explanation: |
      Users cannot learn from or contest unexplained removals;
      DSA requires specific explanations for moderation actions.
    remediation: Provide specific explanations citing violated policies for all actions
  - id: MOD-CRIT-002
    signal: No appeal process for content moderation decisions
    evidence_indicators:
    - No appeal option available
    - Appeal process exists but not communicated
    - Appeals go unreviewed
    explanation: |
      Appeal processes are essential for correcting errors and
      are mandated by DSA for platforms of certain sizes.
    remediation: Implement and clearly communicate appeal process
  high:
  - id: MOD-HIGH-001
    signal: Community guidelines vague or ambiguous
    explanation: |
      Vague rules cannot be fairly enforced and leave users
      uncertain about what is permitted.
    remediation: Clarify guidelines with specific examples and clear boundaries
  - id: MOD-HIGH-002
    signal: No transparency reporting on moderation actions
    explanation: |
      Without transparency reports, platforms cannot be held
      accountable and users cannot assess fairness.
    remediation: Publish regular transparency reports with moderation statistics
  - id: MOD-HIGH-003
    signal: Automated moderation not disclosed
    explanation: |
      Users have a right to know when AI makes moderation decisions
      and to seek human review of automated actions.
    remediation: Disclose use of automated moderation and offer human review
  medium:
  - id: MOD-MED-001
    signal: Enforcement appears inconsistent across similar cases
    remediation: Document enforcement criteria and train for consistency
  - id: MOD-MED-002
    signal: Appeal process difficult to find or use
    remediation: Make appeal process prominent and accessible
  - id: MOD-MED-003
    signal: No timeline communicated for appeal decisions
    remediation: Set and communicate expected appeal response times
  low:
  - id: MOD-LOW-001
    signal: Policy examples use outdated or unclear scenarios
  positive:
  - id: MOD-POS-001
    signal: Clear, specific community guidelines with examples
  - id: MOD-POS-002
    signal: Transparent appeal process with human review
  - id: MOD-POS-003
    signal: Regular public transparency reports
  - id: MOD-POS-004
    signal: Santa Clara Principles adopted
procedure:
  context:
    cognitive_mode: critical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Policy Clarity Review
    description: |
      Assess community guidelines and content policies for
      clarity, completeness, and understandability.
    duration_estimate: 60 min
    questions:
    - Are prohibited content categories clearly defined?
    - Are examples provided for policy boundaries?
    - Can average users understand the rules?
    expected_findings:
    - Policy clarity assessment
    - Ambiguity identification
    - Example adequacy
  - id: '2'
    name: Enforcement Notification Review
    description: |
      Evaluate how users are notified of enforcement
      actions and what information is provided.
    duration_estimate: 45 min
    questions:
    - Are users notified when content is actioned?
    - What specific information is provided?
    - Is the violated rule cited?
    expected_findings:
    - Notification process assessment
    - Information completeness
    - Specificity of explanations
  - id: '3'
    name: Appeal Process Evaluation
    description: |
      Review the appeal process for accessibility,
      fairness, and effectiveness.
    duration_estimate: 45 min
    questions:
    - Is there an appeal process?
    - How accessible is the appeal option?
    - What is the appeal review process?
    - What percentage of appeals succeed?
    expected_findings:
    - Appeal process documentation
    - Accessibility assessment
    - Fairness indicators
  - id: '4'
    name: Transparency Reporting Review
    description: |
      Assess public transparency reporting on
      content moderation activities.
    duration_estimate: 45 min
    questions:
    - Are transparency reports published?
    - What statistics are disclosed?
    - How frequently are reports updated?
    expected_findings:
    - Transparency report status
    - Reporting completeness
    - Publication frequency
  - id: '5'
    name: Automated Moderation Assessment
    description: |
      Evaluate disclosure and human oversight of
      automated moderation systems.
    duration_estimate: 30 min
    questions:
    - Is automated moderation disclosed?
    - Can users request human review?
    - What are error rates for automated systems?
    expected_findings:
    - Automated moderation disclosure status
    - Human review availability
    - Error handling process
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Policy Clarity Assessment
    - Enforcement Transparency
    - Appeal Process Review
    - Transparency Reporting
    - Recommendations
  confidence_guidance:
    high: Complete policy review, internal process access, user feedback analyzed
    medium: Public-facing policies reviewed without internal access
    low: Limited external review only
offline:
  capability: full
  cache_manifest:
    knowledge:
    - source_id: dsa-moderation
      priority: required
    - source_id: santa-clara-principles
      priority: required
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive policy and process review
    full:
      included: true
      priority: 2
closeout_checklist:
- id: mod-001
  item: Community guidelines clarity assessed
  level: CRITICAL
  verification: manual
  verification_notes: Confirm policy review completed
  expected: Confirmed by reviewer
- id: mod-002
  item: Enforcement notification process reviewed
  level: CRITICAL
  verification: manual
  verification_notes: Verify notification assessment
  expected: Confirmed by reviewer
- id: mod-003
  item: Appeal process evaluated
  level: BLOCKING
  verification: manual
  verification_notes: Confirm appeal process review
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - platform
    - social-media
    - ugc-site
    - forum
  compliance_frameworks:
  - framework: Digital Services Act
    controls:
    - Art-14
    - Art-15
    - Art-16
    - Art-17
  - framework: Santa Clara Principles
    controls:
    - notice
    - appeal
    - transparency
relationships:
  commonly_combined:
  - ethical-societal.security-safety-ethics.harassment-prevention
  - ethical-societal.security-safety-ethics.misinformation-prevention
  - ethical-societal.transparency-trust.algorithm-disclosure
