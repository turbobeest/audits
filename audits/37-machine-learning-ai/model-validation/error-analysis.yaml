audit:
  id: machine-learning-ai.model-validation.error-analysis
  name: Error Analysis Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: model-validation
  tier: expert
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: medium
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates systematic error analysis practices including error categorization,
    failure mode identification, confusion matrix analysis, and error attribution.
    Examines whether errors are understood well enough to guide model improvement.
  why_it_matters: |
    Understanding why models fail is essential for targeted improvement. Without
    proper error analysis, teams waste effort on wrong problems or miss systematic
    failure modes. Good error analysis accelerates model improvement and identifies
    high-impact fixes.
  when_to_run:
  - After model training
  - When model performance stagnates
  - Before prioritizing improvements
  - During model debugging
prerequisites:
  required_artifacts:
  - type: model_predictions
    description: Model predictions on validation/test data
  - type: error_samples
    description: Access to misclassified examples
  - type: analysis_tools
    description: Error analysis infrastructure
  access_requirements:
  - Access to model predictions
  - Access to ground truth labels
  - Access to error analysis tooling
  - Access to data exploration tools
discovery:
  file_patterns:
  - glob: '**/analysis/**/*.py'
    purpose: Find analysis code
  - glob: '**/error_analysis/**'
    purpose: Find error analysis code
  - glob: '**/evaluation/**'
    purpose: Find evaluation code
knowledge_sources:
  guides:
  - id: ml-error-analysis
    name: ML Error Analysis Guide
    url: https://www.deeplearning.ai/courses/machine-learning-error-analysis/
  - id: zeno-docs
    name: Zeno ML Evaluation
    url: https://zenoml.com/
  standards:
  - id: error-analysis-best-practices
    name: Error Analysis Best Practices
    relevance: Systematic error investigation
tooling:
  analysis_tools:
  - tool: zeno
    purpose: ML evaluation and error analysis
  - tool: cleanlab
    purpose: Data-centric error analysis
  - tool: error-analysis
    purpose: Responsible AI error analysis
  - tool: whylogs
    purpose: Data and model profiling
signals:
  critical:
  - id: EA-CRIT-001
    signal: Systematic errors on protected groups
    evidence_indicators:
    - Higher error rates for demographic groups
    - Failure modes correlate with protected attributes
    - Disparate impact in errors
    explanation: |
      Systematic errors affecting protected groups indicate bias that can cause legal
      liability and harm to underrepresented populations.
    remediation: Investigate and mitigate group-specific error patterns
  - id: EA-CRIT-002
    signal: High-consequence errors not analyzed
    evidence_indicators:
    - Costly errors not prioritized in analysis
    - No differentiation by error severity
    - False negatives in safety-critical applications
    explanation: |
      Not all errors are equal; high-consequence errors require special attention and
      may need different handling strategies.
    remediation: Implement cost-sensitive error analysis
  high:
  - id: EA-HIGH-001
    signal: No systematic error analysis performed
    remediation: Implement structured error analysis process
  - id: EA-HIGH-002
    signal: Errors not categorized by type
    remediation: Create error taxonomy and categorize failures
  - id: EA-HIGH-003
    signal: Error patterns not actionable
    remediation: Connect error analysis to improvement actions
  medium:
  - id: EA-MED-001
    signal: Error analysis not updated with new data
    remediation: Regularly refresh error analysis
  - id: EA-MED-002
    signal: Confusion patterns not analyzed
    remediation: Examine confusion matrix for systematic patterns
  - id: EA-MED-003
    signal: Feature attribution for errors missing
    remediation: Analyze which features contribute to errors
  low:
  - id: EA-LOW-001
    signal: Error samples not stored
    remediation: Archive error samples for analysis
  - id: EA-LOW-002
    signal: Error analysis documentation incomplete
    remediation: Document error patterns and findings
  positive:
  - id: EA-POS-001
    signal: Systematic error taxonomy with categorization
  - id: EA-POS-002
    signal: Cost-weighted error analysis
  - id: EA-POS-003
    signal: Error analysis directly guides improvements
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Collect Error Samples
    description: Gather misclassified examples for analysis
    duration_estimate: 45 minutes
  - id: '2'
    name: Create Error Taxonomy
    description: Categorize errors into meaningful groups
    duration_estimate: 1 hour
  - id: '3'
    name: Analyze Group Differences
    description: Check for errors patterns across segments
    duration_estimate: 45 minutes
  - id: '4'
    name: Examine Confusion Patterns
    description: Analyze confusion matrix for systematic issues
    duration_estimate: 45 minutes
  - id: '5'
    name: Attribute Errors to Features
    description: Identify feature contributions to errors
    duration_estimate: 45 minutes
  - id: '6'
    name: Prioritize Improvements
    description: Connect analysis to actionable improvements
    duration_estimate: 30 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Error Taxonomy
    - Segment Analysis
    - Improvement Priorities
    - Recommendations
closeout_checklist:
- id: ea-001
  item: Error taxonomy created
  level: HIGH
  verification: manual
- id: ea-002
  item: Group differences analyzed
  level: CRITICAL
  verification: automated
- id: ea-003
  item: Improvements prioritized
  level: MEDIUM
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
  compliance_mappings:
  - framework: EU AI Act
    control: Performance Evaluation
    description: Requirements for understanding AI system performance
relationships:
  commonly_combined:
  - machine-learning-ai.model-validation.evaluation-metrics
  - machine-learning-ai.responsible-ai.bias-detection
  depends_on:
  - machine-learning-ai.model-validation.model-testing
  feeds_into:
  - machine-learning-ai.model-development.training-pipeline-quality
