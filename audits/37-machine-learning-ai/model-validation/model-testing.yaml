# ============================================================
# AUDIT: Model Testing
# Category: 37 - Machine Learning & AI
# Subcategory: model-validation
# ============================================================

audit:
  id: "machine-learning-ai.model-validation.model-testing"
  name: "Model Testing Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "model-validation"

  tier: "expert"
  estimated_duration: "5-7 hours"  # median: 6h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates comprehensive model testing practices including unit tests, integration
    tests, behavioral tests, performance tests, and stress tests. Examines test coverage,
    test data quality, and whether testing catches issues before production deployment.

  why_it_matters: |
    ML models require specialized testing beyond traditional software. Without proper
    testing, models may fail silently, produce biased outputs, or behave unexpectedly
    under edge cases. Comprehensive testing ensures models are reliable, fair, and
    safe for production deployment.

  when_to_run:
    - "Before model deployment"
    - "After model updates or retraining"
    - "When establishing testing practices"
    - "During ML quality reviews"

prerequisites:
  required_artifacts:
    - type: "model_artifacts"
      description: "Access to trained models"
    - type: "test_suite"
      description: "Existing test implementations"
    - type: "test_data"
      description: "Test datasets and expected outputs"

  access_requirements:
    - "Access to model serving endpoint"
    - "Access to test code and configurations"
    - "Access to test execution results"
    - "Access to performance benchmarks"

discovery:
  file_patterns:
    - glob: "**/tests/**/*.py"
      purpose: "Find test files"
    - glob: "**/test_*.py"
      purpose: "Find test modules"
    - glob: "**/conftest.py"
      purpose: "Find pytest configurations"
    - glob: "**/behavioral_tests/**"
      purpose: "Find behavioral tests"

knowledge_sources:
  guides:
    - id: "ml-test-score"
      name: "ML Test Score Guide"
      url: "https://arxiv.org/abs/1906.10742"
    - id: "deepchecks-docs"
      name: "Deepchecks Testing Documentation"
      url: "https://docs.deepchecks.com/"

  standards:
    - id: "ml-testing"
      name: "ML Testing Best Practices"
      relevance: "Comprehensive ML model testing"

tooling:
  analysis_tools:
    - tool: "deepchecks"
      purpose: "ML validation and testing"
    - tool: "pytest"
      purpose: "Test framework"
    - tool: "great_expectations"
      purpose: "Data validation in tests"
    - tool: "checklist"
      purpose: "Behavioral testing for NLP"
    - tool: "metamorphic"
      purpose: "Metamorphic testing"

signals:
  critical:
    - id: "MT-CRIT-001"
      signal: "No model testing before deployment"
      evidence_indicators:
        - "Models deployed without test suite"
        - "No automated testing in CI/CD"
        - "Manual testing only"
      explanation: |
        Deploying untested models risks production failures, biased outputs, and
        unexpected behavior that can cause significant harm to users.
      remediation: "Implement comprehensive test suite before any deployment"

    - id: "MT-CRIT-002"
      signal: "Tests pass but model fails in production"
      evidence_indicators:
        - "Test data not representative"
        - "Edge cases not covered"
        - "Production conditions not simulated"
      explanation: |
        Tests that don't represent production conditions give false confidence and
        allow broken models to be deployed.
      remediation: "Align test data and conditions with production reality"

  high:
    - id: "MT-HIGH-001"
      signal: "No behavioral testing"
      remediation: "Implement behavioral tests for expected model properties"

    - id: "MT-HIGH-002"
      signal: "No performance regression testing"
      remediation: "Add automated performance benchmarks to CI/CD"

    - id: "MT-HIGH-003"
      signal: "Test data leakage from training"
      remediation: "Ensure strict separation of test data from training"

  medium:
    - id: "MT-MED-001"
      signal: "Limited edge case coverage"
      remediation: "Add tests for boundary conditions and edge cases"

    - id: "MT-MED-002"
      signal: "No stress testing"
      remediation: "Test model behavior under load and unusual inputs"

    - id: "MT-MED-003"
      signal: "Tests not automated"
      remediation: "Automate test execution in CI/CD pipeline"

  low:
    - id: "MT-LOW-001"
      signal: "Test documentation incomplete"
      remediation: "Document test strategies and coverage"

    - id: "MT-LOW-002"
      signal: "Test results not tracked"
      remediation: "Implement test result tracking and trending"

  positive:
    - id: "MT-POS-001"
      signal: "Comprehensive behavioral test coverage"
    - id: "MT-POS-002"
      signal: "Automated testing in CI/CD"
    - id: "MT-POS-003"
      signal: "Performance regression detection"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Test Coverage"
      description: "Assess breadth and depth of test coverage"
      duration_estimate: "1 hour"

    - id: "2"
      name: "Evaluate Test Data"
      description: "Review quality and representativeness of test data"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Assess Behavioral Tests"
      description: "Review behavioral and property-based testing"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Check Performance Tests"
      description: "Evaluate performance and stress testing"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Review CI/CD Integration"
      description: "Verify test automation in deployment pipeline"
      duration_estimate: "45 minutes"

    - id: "6"
      name: "Analyze Test Results"
      description: "Review historical test results and failures"
      duration_estimate: "45 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Test Coverage Analysis"
        - "Test Quality Assessment"
        - "Automation Review"
        - "Recommendations"

closeout_checklist:
  - id: "mt-001"
    item: "Test coverage assessed"
    level: "CRITICAL"
    verification: "automated"

  - id: "mt-002"
    item: "Behavioral tests reviewed"
    level: "HIGH"
    verification: "manual"

  - id: "mt-003"
    item: "CI/CD integration verified"
    level: "HIGH"
    verification: "automated"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Testing and Validation"
      description: "Requirements for AI system testing"

relationships:
  commonly_combined:
    - "machine-learning-ai.model-validation.evaluation-metrics"
    - "machine-learning-ai.responsible-ai.bias-detection"
  depends_on:
    - "machine-learning-ai.model-development.training-pipeline-quality"
  feeds_into:
    - "machine-learning-ai.model-deployment.model-serving"
