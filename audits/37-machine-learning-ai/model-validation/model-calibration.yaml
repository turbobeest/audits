audit:
  id: machine-learning-ai.model-validation.model-calibration
  name: Model Calibration Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: model-validation
  tier: expert
  estimated_duration: 3-5 hours  # median: 4h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates model probability calibration to ensure predicted probabilities accurately
    reflect true likelihood of outcomes. Examines calibration curves, calibration metrics,
    and calibration techniques used to improve probability estimates.
  why_it_matters: |
    Many applications require well-calibrated probabilities for decision-making. Poorly
    calibrated models give misleading confidence estimates, leading to over- or under-
    confident decisions. Medical diagnosis, risk assessment, and uncertainty quantification
    all require proper calibration.
  when_to_run:
  - When probability estimates are used for decisions
  - After model training or retraining
  - When calibration issues suspected
  - For risk-sensitive applications
prerequisites:
  required_artifacts:
  - type: model_predictions
    description: Model probability predictions
  - type: ground_truth
    description: True outcomes for calibration evaluation
  - type: calibration_code
    description: Calibration evaluation implementation
  access_requirements:
  - Access to model predictions with probabilities
  - Access to validation data with labels
  - Access to calibration tools
discovery:
  file_patterns:
  - glob: '**/calibration/**/*.py'
    purpose: Find calibration code
  - glob: '**/uncertainty/**'
    purpose: Find uncertainty estimation code
  - glob: '**/reliability_diagram/**'
    purpose: Find calibration plotting
knowledge_sources:
  guides:
  - id: sklearn-calibration
    name: Scikit-learn Calibration Guide
    url: https://scikit-learn.org/stable/modules/calibration.html
  - id: uncertainty-quantification
    name: Uncertainty Quantification Guide
    url: https://arxiv.org/abs/1706.04599
  standards:
  - id: calibration-best-practices
    name: Model Calibration Best Practices
    relevance: Reliable probability estimation
tooling:
  analysis_tools:
  - tool: sklearn.calibration
    purpose: Calibration curves and methods
  - tool: uncertainty-toolbox
    purpose: Uncertainty and calibration metrics
  - tool: temperature-scaling
    purpose: Post-hoc calibration
  - tool: netcal
    purpose: Network calibration
signals:
  critical:
  - id: MC-CRIT-001
    signal: Severely miscalibrated predictions
    evidence_indicators:
    - Expected Calibration Error (ECE) > 0.15
    - Reliability diagram shows extreme deviation
    - Probability estimates meaningless
    explanation: |
      Severe miscalibration makes probability estimates unreliable for any downstream
      decision-making, potentially causing significant harm in risk-sensitive applications.
    remediation: Implement calibration techniques and verify with reliability diagrams
  - id: MC-CRIT-002
    signal: Calibration not evaluated for risk-sensitive application
    evidence_indicators:
    - Probabilities used for decisions without calibration check
    - Medical/financial decisions based on uncalibrated scores
    - No calibration metrics tracked
    explanation: |
      Using uncalibrated probabilities for important decisions can lead to systematically
      wrong risk assessments and harmful outcomes.
    remediation: Evaluate calibration before using probabilities for decisions
  high:
  - id: MC-HIGH-001
    signal: No calibration techniques applied
    remediation: Apply post-hoc calibration methods
  - id: MC-HIGH-002
    signal: Calibration evaluated only overall, not per-class
    remediation: Assess calibration for each class/segment
  - id: MC-HIGH-003
    signal: Calibration degrades after deployment
    remediation: Monitor calibration in production
  medium:
  - id: MC-MED-001
    signal: Calibration method not validated
    remediation: Validate calibration improvement with held-out data
  - id: MC-MED-002
    signal: No uncertainty quantification
    remediation: Implement uncertainty estimation for predictions
  - id: MC-MED-003
    signal: Calibration not monitored over time
    remediation: Track calibration metrics in production
  low:
  - id: MC-LOW-001
    signal: No reliability diagrams generated
    remediation: Create reliability diagrams for visualization
  - id: MC-LOW-002
    signal: Calibration documentation incomplete
    remediation: Document calibration approach and limitations
  positive:
  - id: MC-POS-001
    signal: Well-calibrated probabilities (ECE < 0.05)
  - id: MC-POS-002
    signal: Calibration validated per-class and per-segment
  - id: MC-POS-003
    signal: Production calibration monitoring in place
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Generate Calibration Metrics
    description: Calculate ECE, MCE, and related metrics
    duration_estimate: 45 minutes
  - id: '2'
    name: Create Reliability Diagrams
    description: Visualize calibration with reliability plots
    duration_estimate: 30 minutes
  - id: '3'
    name: Assess Per-Class Calibration
    description: Evaluate calibration breakdown by class
    duration_estimate: 45 minutes
  - id: '4'
    name: Review Calibration Methods
    description: Examine calibration techniques used
    duration_estimate: 30 minutes
  - id: '5'
    name: Check Production Monitoring
    description: Verify calibration tracking in production
    duration_estimate: 30 minutes
  - id: '6'
    name: Evaluate Decision Impact
    description: Assess impact of calibration on downstream decisions
    duration_estimate: 30 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Calibration Analysis
    - Reliability Diagrams
    - Decision Impact Assessment
    - Recommendations
closeout_checklist:
- id: mc-001
  item: Calibration metrics calculated
  level: CRITICAL
  verification: automated
- id: mc-002
  item: Per-class calibration assessed
  level: HIGH
  verification: automated
- id: mc-003
  item: Decision impact evaluated
  level: HIGH
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
    - risk-systems
  compliance_mappings:
  - framework: EU AI Act
    control: Accuracy
    description: Requirements for AI system accuracy and reliability
relationships:
  commonly_combined:
  - machine-learning-ai.model-validation.evaluation-metrics
  - machine-learning-ai.responsible-ai.explainability-implementation
  depends_on:
  - machine-learning-ai.model-validation.model-testing
  feeds_into:
  - machine-learning-ai.model-deployment.model-serving
