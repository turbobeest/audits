audit:
  id: machine-learning-ai.model-validation.evaluation-metrics
  name: Evaluation Metrics Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: model-validation
  tier: expert
  estimated_duration: 3-5 hours  # median: 4h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the appropriateness and implementation of model evaluation metrics
    including metric selection, calculation correctness, confidence intervals,
    and alignment with business objectives. Examines whether metrics accurately
    reflect model performance for the intended use case.
  why_it_matters: |
    Wrong metrics lead to wrong decisions. Models optimized for inappropriate metrics
    may appear successful but fail to deliver business value or may cause harm.
    Proper metric selection and implementation ensures models are evaluated on what
    actually matters.
  when_to_run:
  - During model development
  - When defining success criteria
  - After metric discrepancies discovered
  - During model review processes
prerequisites:
  required_artifacts:
  - type: evaluation_code
    description: Access to metric calculation code
  - type: business_requirements
    description: Business objectives documentation
  - type: evaluation_results
    description: Historical evaluation results
  access_requirements:
  - Access to metric implementations
  - Access to evaluation pipelines
  - Access to business requirements
  - Access to evaluation dashboards
discovery:
  file_patterns:
  - glob: '**/metrics/**/*.py'
    purpose: Find metric implementations
  - glob: '**/evaluation/**'
    purpose: Find evaluation code
  - glob: '**/scoring/**'
    purpose: Find scoring functions
knowledge_sources:
  guides:
  - id: sklearn-metrics
    name: Scikit-learn Metrics Guide
    url: https://scikit-learn.org/stable/modules/model_evaluation.html
  - id: torchmetrics-docs
    name: TorchMetrics Documentation
    url: https://torchmetrics.readthedocs.io/
  standards:
  - id: ml-metrics
    name: ML Metrics Best Practices
    relevance: Proper metric selection and implementation
tooling:
  analysis_tools:
  - tool: sklearn.metrics
    purpose: Standard ML metrics
  - tool: torchmetrics
    purpose: PyTorch metrics library
  - tool: evaluate
    purpose: Hugging Face evaluation
  - tool: fairlearn
    purpose: Fairness metrics
signals:
  critical:
  - id: EM-CRIT-001
    signal: Metrics misaligned with business objectives
    evidence_indicators:
    - Technical metrics high but business KPIs poor
    - Optimizing for proxy that doesn't reflect real goal
    - Success criteria not linked to business impact
    explanation: |
      Misaligned metrics cause teams to optimize for the wrong things, potentially
      deploying models that technically perform well but fail business objectives.
    remediation: Redefine metrics based on business value and user impact
  - id: EM-CRIT-002
    signal: Metric calculation errors
    evidence_indicators:
    - Custom metric implementations incorrect
    - Off-by-one errors in aggregation
    - Improper handling of edge cases
    explanation: |
      Incorrect metric calculations lead to wrong conclusions about model performance,
      potentially deploying bad models or rejecting good ones.
    remediation: Audit and validate metric implementations with known examples
  high:
  - id: EM-HIGH-001
    signal: No confidence intervals on metrics
    remediation: Implement bootstrapped confidence intervals
  - id: EM-HIGH-002
    signal: Using only single metric
    remediation: Use multiple complementary metrics
  - id: EM-HIGH-003
    signal: Metrics not stratified by segments
    remediation: Break down metrics by important subgroups
  medium:
  - id: EM-MED-001
    signal: Metric thresholds arbitrary
    remediation: Define thresholds based on business requirements
  - id: EM-MED-002
    signal: No baseline comparison
    remediation: Compare against baseline and existing solutions
  - id: EM-MED-003
    signal: Metrics not tracked over time
    remediation: Implement metric trending and visualization
  low:
  - id: EM-LOW-001
    signal: Metric documentation incomplete
    remediation: Document metric definitions and rationale
  - id: EM-LOW-002
    signal: Inconsistent metric naming
    remediation: Standardize metric naming conventions
  positive:
  - id: EM-POS-001
    signal: Metrics clearly linked to business value
  - id: EM-POS-002
    signal: Comprehensive multi-metric evaluation
  - id: EM-POS-003
    signal: Confidence intervals and statistical testing
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Review Metric Selection
    description: Assess appropriateness of chosen metrics
    duration_estimate: 45 minutes
  - id: '2'
    name: Validate Implementations
    description: Verify metric calculations are correct
    duration_estimate: 1 hour
  - id: '3'
    name: Check Business Alignment
    description: Evaluate metric-business objective mapping
    duration_estimate: 45 minutes
  - id: '4'
    name: Assess Statistical Rigor
    description: Review confidence intervals and significance testing
    duration_estimate: 30 minutes
  - id: '5'
    name: Review Segmentation
    description: Check metric breakdown by subgroups
    duration_estimate: 30 minutes
  - id: '6'
    name: Evaluate Reporting
    description: Assess metric dashboards and documentation
    duration_estimate: 30 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Metric Selection Analysis
    - Implementation Validation
    - Business Alignment
    - Recommendations
closeout_checklist:
- id: em-001
  item: Metric implementations validated
  level: CRITICAL
  verification: automated
- id: em-002
  item: Business alignment assessed
  level: HIGH
  verification: manual
- id: em-003
  item: Statistical rigor reviewed
  level: MEDIUM
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
  compliance_mappings:
  - framework: EU AI Act
    control: Performance Metrics
    description: Requirements for AI system performance measurement
relationships:
  commonly_combined:
  - machine-learning-ai.model-validation.model-testing
  - machine-learning-ai.responsible-ai.fairness-metrics
  depends_on:
  - machine-learning-ai.model-development.model-selection-process
  feeds_into:
  - machine-learning-ai.model-monitoring.performance-monitoring
