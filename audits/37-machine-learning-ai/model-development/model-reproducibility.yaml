audit:
  id: machine-learning-ai.model-development.model-reproducibility
  name: Model Reproducibility Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: model-development
  tier: expert
  estimated_duration: 3-5 hours  # median: 4h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the reproducibility of ML experiments and model training including
    random seed management, environment versioning, data versioning, and documentation
    of non-deterministic operations. Examines whether experiments can be reliably
    reproduced by different team members or in different environments.
  why_it_matters: |
    Reproducibility is fundamental to scientific validity and production reliability.
    Non-reproducible experiments make debugging impossible, prevent collaboration, and
    undermine confidence in model decisions. Reproducible ML enables reliable deployment
    and regulatory compliance.
  when_to_run:
  - Before claiming model performance results
  - When transitioning models to production
  - After changing training infrastructure
  - During compliance audits
prerequisites:
  required_artifacts:
  - type: training_code
    description: Access to all training code
  - type: environment_specs
    description: Environment and dependency specifications
  - type: data_versioning
    description: Data versioning system access
  access_requirements:
  - Access to training codebase
  - Access to environment configurations
  - Access to data versioning system
  - Access to experiment tracking
discovery:
  file_patterns:
  - glob: '**/requirements*.txt'
    purpose: Find dependency specifications
  - glob: '**/environment*.yaml'
    purpose: Find conda environments
  - glob: '**/Dockerfile*'
    purpose: Find container definitions
  - glob: '**/.dvc/**'
    purpose: Find DVC configurations
  - glob: '**/setup.py'
    purpose: Find package configurations
knowledge_sources:
  guides:
  - id: dvc-docs
    name: DVC Documentation
    url: https://dvc.org/doc
  - id: pytorch-reproducibility
    name: PyTorch Reproducibility Guide
    url: https://pytorch.org/docs/stable/notes/randomness.html
  standards:
  - id: ml-reproducibility
    name: ML Reproducibility Checklist
    relevance: Standards for reproducible ML research
tooling:
  analysis_tools:
  - tool: dvc
    purpose: Data version control
  - tool: mlflow
    purpose: Experiment tracking and reproducibility
  - tool: docker
    purpose: Environment containerization
  - tool: pip-tools
    purpose: Dependency pinning
signals:
  critical:
  - id: MR-CRIT-001
    signal: Experiments cannot be reproduced
    evidence_indicators:
    - Results vary significantly between runs
    - Unable to recreate reported metrics
    - Environment differences cause failures
    explanation: |
      Non-reproducible experiments undermine trust in results and make it impossible
      to debug issues or build on previous work reliably.
    remediation: Implement comprehensive reproducibility controls including seeds, versioning, and containerization
  - id: MR-CRIT-002
    signal: Training data not versioned
    evidence_indicators:
    - No data versioning system in place
    - Training data changes without tracking
    - Unable to retrieve historical training data
    explanation: |
      Without data versioning, it's impossible to reproduce training runs or understand
      how data changes affected model performance.
    remediation: Implement data versioning using DVC, Delta Lake, or similar
  high:
  - id: MR-HIGH-001
    signal: Random seeds not controlled
    remediation: Set and log random seeds for all sources of randomness
  - id: MR-HIGH-002
    signal: Dependencies not pinned
    remediation: Pin all dependencies with exact versions
  - id: MR-HIGH-003
    signal: Non-deterministic operations undocumented
    remediation: Document all non-deterministic operations and their impact
  medium:
  - id: MR-MED-001
    signal: Environment not containerized
    remediation: Create Docker containers for training environments
  - id: MR-MED-002
    signal: Hardware configurations not documented
    remediation: Document and log hardware configurations used
  - id: MR-MED-003
    signal: Experiment configurations scattered
    remediation: Centralize experiment configurations in version control
  low:
  - id: MR-LOW-001
    signal: Reproduction instructions incomplete
    remediation: Write comprehensive reproduction documentation
  - id: MR-LOW-002
    signal: Experiment tracking inconsistent
    remediation: Standardize experiment tracking practices
  positive:
  - id: MR-POS-001
    signal: Full reproducibility with containerized environments
  - id: MR-POS-002
    signal: Comprehensive data versioning in place
  - id: MR-POS-003
    signal: Automated reproducibility testing in CI/CD
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Assess Randomness Control
    description: Review random seed management across codebase
    duration_estimate: 30 minutes
  - id: '2'
    name: Review Environment Management
    description: Evaluate dependency and environment specifications
    duration_estimate: 45 minutes
  - id: '3'
    name: Check Data Versioning
    description: Assess data versioning practices and coverage
    duration_estimate: 45 minutes
  - id: '4'
    name: Test Reproducibility
    description: Attempt to reproduce a recent experiment
    duration_estimate: 1 hour
  - id: '5'
    name: Review Documentation
    description: Evaluate reproduction documentation quality
    duration_estimate: 30 minutes
  - id: '6'
    name: Assess Non-determinism
    description: Identify and document sources of non-determinism
    duration_estimate: 30 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Reproducibility Assessment
    - Environment Analysis
    - Data Versioning Review
    - Recommendations
closeout_checklist:
- id: mr-001
  item: Reproducibility tested
  level: CRITICAL
  verification: automated
- id: mr-002
  item: Data versioning verified
  level: CRITICAL
  verification: automated
- id: mr-003
  item: Environment documented
  level: HIGH
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
    - research-systems
  compliance_mappings:
  - framework: EU AI Act
    control: Traceability
    description: Requirements for AI system traceability and reproducibility
relationships:
  commonly_combined:
  - machine-learning-ai.mlops-infrastructure.experiment-tracking
  - machine-learning-ai.model-development.training-pipeline-quality
  depends_on:
  - version-control.git-practices
  feeds_into:
  - machine-learning-ai.model-deployment.model-versioning
