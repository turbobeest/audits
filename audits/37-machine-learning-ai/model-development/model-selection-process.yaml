audit:
  id: machine-learning-ai.model-development.model-selection-process
  name: Model Selection Process Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: model-development
  tier: expert
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the model selection process including candidate model evaluation,
    comparison methodology, baseline comparisons, and selection criteria. Examines
    whether model choices are justified, documented, and consider both technical
    performance and operational constraints.
  why_it_matters: |
    Selecting the right model architecture impacts performance, inference latency,
    maintainability, and operational costs. Poor model selection leads to suboptimal
    systems that may be overengineered for simple problems or underperform on complex
    ones. Proper selection balances accuracy with practical constraints.
  when_to_run:
  - At project inception when choosing model approach
  - Before committing to production model architecture
  - When considering model architecture changes
  - During cost optimization initiatives
prerequisites:
  required_artifacts:
  - type: experiment_results
    description: Results from model comparison experiments
  - type: requirements_doc
    description: Business and technical requirements
  - type: benchmark_data
    description: Benchmark datasets used for evaluation
  access_requirements:
  - Access to experiment tracking system
  - Access to model comparison reports
  - Access to performance benchmarks
  - Access to requirements documentation
discovery:
  file_patterns:
  - glob: '**/experiments/**/*.py'
    purpose: Find experiment code
  - glob: '**/benchmarks/**'
    purpose: Find benchmark configurations
  - glob: '**/model_comparison/**'
    purpose: Find model comparison results
  - glob: '**/baselines/**/*.py'
    purpose: Find baseline model implementations
knowledge_sources:
  guides:
  - id: papers-with-code
    name: Papers With Code Benchmarks
    url: https://paperswithcode.com/
  - id: huggingface-hub
    name: Hugging Face Model Hub
    url: https://huggingface.co/models
  standards:
  - id: ml-model-selection
    name: Model Selection Best Practices
    relevance: Systematic model evaluation and selection
tooling:
  analysis_tools:
  - tool: mlflow
    purpose: Experiment tracking and comparison
  - tool: wandb
    purpose: Model comparison dashboards
  - tool: optuna
    purpose: Model architecture search
  - tool: sklearn
    purpose: Model evaluation metrics
signals:
  critical:
  - id: MS-CRIT-001
    signal: No baseline model comparison performed
    evidence_indicators:
    - Complex model deployed without simple baseline
    - No comparison with existing solutions
    - No ablation studies conducted
    explanation: |
      Without baseline comparisons, it's impossible to know if complex models provide
      meaningful improvements over simpler alternatives, leading to unnecessary
      complexity and operational burden.
    remediation: Implement baseline models and conduct systematic comparisons
  - id: MS-CRIT-002
    signal: Model selected based solely on single metric
    evidence_indicators:
    - Only accuracy considered, ignoring latency
    - Business impact metrics not evaluated
    - Fairness metrics not considered
    explanation: |
      Single-metric optimization often leads to models that perform well on benchmarks
      but fail in production due to ignored constraints like latency, cost, or fairness.
    remediation: Define multi-objective selection criteria including operational metrics
  high:
  - id: MS-HIGH-001
    signal: Model selection not reproducible
    remediation: Document and version control all selection experiments
  - id: MS-HIGH-002
    signal: Operational constraints not considered
    remediation: Include latency, memory, and cost in selection criteria
  - id: MS-HIGH-003
    signal: Selection based on biased or non-representative data
    remediation: Ensure evaluation data represents production distribution
  medium:
  - id: MS-MED-001
    signal: Statistical significance not assessed
    remediation: Perform statistical significance testing on model comparisons
  - id: MS-MED-002
    signal: Model complexity not justified
    remediation: Document justification for model complexity decisions
  - id: MS-MED-003
    signal: Transfer learning options not explored
    remediation: Evaluate pre-trained models and transfer learning approaches
  low:
  - id: MS-LOW-001
    signal: Selection rationale not documented
    remediation: Document model selection decisions and rationale
  - id: MS-LOW-002
    signal: Alternative architectures not explored
    remediation: Conduct broader architecture search
  positive:
  - id: MS-POS-001
    signal: Comprehensive baseline comparisons documented
  - id: MS-POS-002
    signal: Multi-objective selection criteria with clear tradeoffs
  - id: MS-POS-003
    signal: Statistical significance testing in model comparison
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Review Selection Criteria
    description: Examine the criteria used for model selection decisions
    duration_estimate: 45 minutes
  - id: '2'
    name: Evaluate Baseline Comparisons
    description: Verify baseline models were implemented and compared
    duration_estimate: 1 hour
  - id: '3'
    name: Assess Evaluation Methodology
    description: Review statistical rigor of model comparisons
    duration_estimate: 45 minutes
  - id: '4'
    name: Check Operational Constraints
    description: Verify operational requirements were considered
    duration_estimate: 30 minutes
  - id: '5'
    name: Review Documentation
    description: Assess documentation of selection rationale
    duration_estimate: 30 minutes
  - id: '6'
    name: Validate Reproducibility
    description: Confirm ability to reproduce selection experiments
    duration_estimate: 45 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Selection Process Analysis
    - Baseline Comparison Review
    - Operational Considerations
    - Recommendations
closeout_checklist:
- id: ms-001
  item: Baseline comparisons verified
  level: CRITICAL
  verification: manual
- id: ms-002
  item: Selection criteria documented
  level: HIGH
  verification: manual
- id: ms-003
  item: Operational constraints assessed
  level: HIGH
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
  compliance_mappings:
  - framework: EU AI Act
    control: Technical Documentation
    description: Requirements for documenting AI system design choices
relationships:
  commonly_combined:
  - machine-learning-ai.model-development.hyperparameter-tuning
  - machine-learning-ai.model-validation.model-testing
  depends_on:
  - machine-learning-ai.data-quality.training-data-quality
  feeds_into:
  - machine-learning-ai.model-deployment.model-serving
