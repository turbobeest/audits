audit:
  id: machine-learning-ai.model-development.hyperparameter-tuning
  name: Hyperparameter Tuning Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: model-development
  tier: expert
  estimated_duration: 3-5 hours  # median: 4h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: medium
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates hyperparameter tuning processes including search strategies, validation
    methodology, resource allocation, and reproducibility. Examines whether tuning
    is performed systematically, documented properly, and avoids common pitfalls
    like overfitting to the validation set.
  why_it_matters: |
    Proper hyperparameter tuning can significantly improve model performance, but
    poor tuning practices lead to overfit models, wasted compute resources, and
    non-reproducible results. Systematic tuning with proper validation ensures
    models generalize well to production data.
  when_to_run:
  - Before finalizing model for deployment
  - After establishing new model architecture
  - When model performance is suboptimal
  - During compute resource optimization
prerequisites:
  required_artifacts:
  - type: training_code
    description: Access to model training scripts
  - type: experiment_logs
    description: Access to hyperparameter tuning logs
  - type: compute_metrics
    description: Resource utilization data
  access_requirements:
  - Read access to training codebase
  - Access to experiment tracking system
  - Access to hyperparameter search configurations
  - Access to compute resource metrics
discovery:
  file_patterns:
  - glob: '**/configs/**/*.yaml'
    purpose: Find hyperparameter configurations
  - glob: '**/hparams/**'
    purpose: Find hyperparameter definitions
  - glob: '**/optuna/**/*.py'
    purpose: Find Optuna tuning code
  - glob: '**/ray_tune/**/*.py'
    purpose: Find Ray Tune configurations
  - glob: '**/hyperopt/**/*.py'
    purpose: Find Hyperopt tuning code
knowledge_sources:
  guides:
  - id: optuna-docs
    name: Optuna Documentation
    url: https://optuna.readthedocs.io/
  - id: ray-tune-docs
    name: Ray Tune Documentation
    url: https://docs.ray.io/en/latest/tune/
  standards:
  - id: automl-best-practices
    name: AutoML Best Practices
    relevance: Systematic hyperparameter optimization
tooling:
  analysis_tools:
  - tool: optuna
    purpose: Hyperparameter optimization framework
  - tool: ray-tune
    purpose: Distributed hyperparameter tuning
  - tool: hyperopt
    purpose: Bayesian optimization for hyperparameters
  - tool: wandb
    purpose: Experiment tracking and sweeps
  - tool: mlflow
    purpose: Experiment tracking
signals:
  critical:
  - id: HT-CRIT-001
    signal: Hyperparameters tuned on test set
    evidence_indicators:
    - Test set metrics used in hyperparameter selection
    - No held-out validation set during tuning
    - Data leakage between tuning and evaluation
    explanation: |
      Tuning hyperparameters on the test set causes overfitting to that specific data,
      leading to overly optimistic performance estimates that don't generalize.
    remediation: Implement proper train/validation/test splits with held-out test set
  - id: HT-CRIT-002
    signal: Non-reproducible hyperparameter search
    evidence_indicators:
    - Random seeds not fixed
    - Search space not version controlled
    - Unable to recreate best configuration
    explanation: |
      Non-reproducible tuning makes it impossible to verify results or build on
      previous work, leading to wasted effort and unreliable models.
    remediation: Fix random seeds, version control search spaces, log all configurations
  high:
  - id: HT-HIGH-001
    signal: No cross-validation during tuning
    remediation: Implement k-fold cross-validation for hyperparameter evaluation
  - id: HT-HIGH-002
    signal: Inefficient search strategy for search space
    remediation: Use Bayesian optimization or informed search for large spaces
  - id: HT-HIGH-003
    signal: Excessive compute waste during tuning
    remediation: Implement early stopping and resource scheduling
  medium:
  - id: HT-MED-001
    signal: Search space too narrow or wide
    remediation: Analyze search results and adjust search space bounds
  - id: HT-MED-002
    signal: Tuning results not documented
    remediation: Document search space, best parameters, and performance metrics
  - id: HT-MED-003
    signal: No sensitivity analysis on hyperparameters
    remediation: Perform and document hyperparameter sensitivity analysis
  low:
  - id: HT-LOW-001
    signal: Manual hyperparameter tuning without systematic approach
    remediation: Adopt automated hyperparameter optimization framework
  - id: HT-LOW-002
    signal: Tuning metrics not aligned with business objectives
    remediation: Define tuning objectives based on business requirements
  positive:
  - id: HT-POS-001
    signal: Systematic Bayesian optimization with proper validation
  - id: HT-POS-002
    signal: Full experiment tracking with reproducible configurations
  - id: HT-POS-003
    signal: Efficient resource utilization with early stopping
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Review Tuning Methodology
    description: Examine hyperparameter search strategies and validation approaches
    duration_estimate: 45 minutes
  - id: '2'
    name: Validate Data Splits
    description: Verify proper separation of train/validation/test data
    duration_estimate: 30 minutes
  - id: '3'
    name: Analyze Search Space
    description: Evaluate search space definition and coverage
    duration_estimate: 30 minutes
  - id: '4'
    name: Check Reproducibility
    description: Verify ability to reproduce tuning results
    duration_estimate: 45 minutes
  - id: '5'
    name: Assess Resource Efficiency
    description: Evaluate compute resource utilization during tuning
    duration_estimate: 30 minutes
  - id: '6'
    name: Review Documentation
    description: Check documentation of tuning results and decisions
    duration_estimate: 30 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Tuning Methodology Analysis
    - Reproducibility Assessment
    - Resource Efficiency
    - Recommendations
closeout_checklist:
- id: ht-001
  item: Validation methodology verified
  level: CRITICAL
  verification: manual
- id: ht-002
  item: Reproducibility confirmed
  level: HIGH
  verification: automated
- id: ht-003
  item: Resource efficiency assessed
  level: MEDIUM
  verification: automated
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
  compliance_mappings:
  - framework: ISO/IEC 23894
    control: AI System Documentation
    description: Requirements for documenting AI system development
relationships:
  commonly_combined:
  - machine-learning-ai.model-development.model-selection-process
  - machine-learning-ai.mlops-infrastructure.experiment-tracking
  depends_on:
  - machine-learning-ai.data-quality.training-data-quality
  feeds_into:
  - machine-learning-ai.model-validation.model-testing
