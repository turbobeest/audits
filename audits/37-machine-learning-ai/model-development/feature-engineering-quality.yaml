# ============================================================
# AUDIT: Feature Engineering Quality
# Category: 37 - Machine Learning & AI
# Subcategory: model-development
# ============================================================

audit:
  id: "machine-learning-ai.model-development.feature-engineering-quality"
  name: "Feature Engineering Quality Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "model-development"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Examines the quality and robustness of feature engineering pipelines, including
    feature creation, transformation, selection, and documentation practices. Evaluates
    whether features are properly versioned, tested, and maintain consistency between
    training and serving environments.

  why_it_matters: |
    Feature engineering is often the most impactful component of ML model performance.
    Poor feature engineering leads to training-serving skew, data leakage, and model
    degradation. Well-engineered features improve model accuracy, interpretability,
    and maintainability while reducing technical debt.

  when_to_run:
    - "Before model deployment to production"
    - "After significant feature pipeline changes"
    - "When model performance degrades unexpectedly"
    - "During ML system architecture reviews"

prerequisites:
  required_artifacts:
    - type: "feature_pipeline"
      description: "Access to feature engineering code and pipelines"
    - type: "feature_store"
      description: "Access to feature store if implemented"
    - type: "training_data"
      description: "Sample training datasets for validation"

  access_requirements:
    - "Read access to feature engineering codebase"
    - "Access to feature store APIs"
    - "Access to data transformation logs"
    - "Access to feature documentation"

discovery:
  file_patterns:
    - glob: "**/features/**/*.py"
      purpose: "Find feature engineering code"
    - glob: "**/transformers/**/*.py"
      purpose: "Find data transformation code"
    - glob: "**/preprocessing/**/*.py"
      purpose: "Find preprocessing pipelines"
    - glob: "**/feature_store/**"
      purpose: "Find feature store configurations"
    - glob: "**/dbt/**/*.sql"
      purpose: "Find SQL-based feature transformations"

knowledge_sources:
  guides:
    - id: "feast-docs"
      name: "Feast Feature Store Documentation"
      url: "https://docs.feast.dev/"
    - id: "sklearn-preprocessing"
      name: "Scikit-learn Preprocessing Guide"
      url: "https://scikit-learn.org/stable/modules/preprocessing.html"

  standards:
    - id: "ml-engineering-best-practices"
      name: "Google ML Engineering Best Practices"
      relevance: "Feature engineering standards"

tooling:
  analysis_tools:
    - tool: "feast"
      purpose: "Feature store management"
    - tool: "great_expectations"
      purpose: "Feature data validation"
    - tool: "featuretools"
      purpose: "Automated feature engineering"
    - tool: "tfdv"
      purpose: "TensorFlow Data Validation"

signals:
  critical:
    - id: "FE-CRIT-001"
      signal: "Training-serving skew detected in feature values"
      evidence_indicators:
        - "Feature distributions differ significantly between training and serving"
        - "Point-in-time correctness violations"
        - "Different transformation logic in training vs serving"
      explanation: |
        Training-serving skew occurs when features computed during training differ from
        those computed during inference, leading to degraded model performance in production.
      remediation: "Unify feature computation using a feature store with consistent transformations"

    - id: "FE-CRIT-002"
      signal: "Data leakage in feature engineering"
      evidence_indicators:
        - "Future data used in feature computation"
        - "Target variable information leaking into features"
        - "Test set statistics used in training transformations"
      explanation: |
        Data leakage artificially inflates model performance during development but causes
        catastrophic failures in production when leaked information is unavailable.
      remediation: "Implement strict temporal splits and audit feature computation logic"

  high:
    - id: "FE-HIGH-001"
      signal: "No feature versioning implemented"
      remediation: "Implement feature versioning using feature store or custom solution"

    - id: "FE-HIGH-002"
      signal: "Feature transformations not tested"
      remediation: "Add unit tests for all feature transformation functions"

    - id: "FE-HIGH-003"
      signal: "Missing feature documentation"
      remediation: "Document feature definitions, sources, and business meaning"

  medium:
    - id: "FE-MED-001"
      signal: "Redundant or highly correlated features"
      remediation: "Perform feature selection to remove redundant features"

    - id: "FE-MED-002"
      signal: "Inconsistent null handling across features"
      remediation: "Standardize null handling strategy across feature pipeline"

    - id: "FE-MED-003"
      signal: "Feature computation latency too high for serving"
      remediation: "Optimize feature computation or pre-compute features"

  low:
    - id: "FE-LOW-001"
      signal: "Feature naming conventions inconsistent"
      remediation: "Establish and enforce feature naming standards"

    - id: "FE-LOW-002"
      signal: "Feature importance analysis not documented"
      remediation: "Document feature importance and selection rationale"

  positive:
    - id: "FE-POS-001"
      signal: "Comprehensive feature store with versioning"
    - id: "FE-POS-002"
      signal: "Full feature lineage tracking implemented"
    - id: "FE-POS-003"
      signal: "Automated feature validation in CI/CD"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Inventory Feature Engineering Assets"
      description: "Catalog all feature engineering code, pipelines, and configurations"
      duration_estimate: "30 minutes"

    - id: "2"
      name: "Validate Feature Consistency"
      description: "Compare feature computation between training and serving environments"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Check for Data Leakage"
      description: "Analyze feature computation for temporal leakage and target leakage"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Review Feature Documentation"
      description: "Assess completeness of feature definitions and business context"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Evaluate Feature Testing"
      description: "Review test coverage for feature transformation functions"
      duration_estimate: "45 minutes"

    - id: "6"
      name: "Assess Feature Store Implementation"
      description: "Evaluate feature store architecture and capabilities"
      duration_estimate: "1 hour"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Feature Pipeline Architecture"
        - "Training-Serving Consistency Analysis"
        - "Data Leakage Assessment"
        - "Recommendations"

closeout_checklist:
  - id: "fe-001"
    item: "Training-serving skew analysis completed"
    level: "CRITICAL"
    verification: "automated"

  - id: "fe-002"
    item: "Data leakage check performed"
    level: "CRITICAL"
    verification: "manual"

  - id: "fe-003"
    item: "Feature documentation reviewed"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications", "data-platforms"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Data Governance"
      description: "Requirements for training data quality and documentation"

relationships:
  commonly_combined:
    - "machine-learning-ai.data-quality.training-data-quality"
    - "machine-learning-ai.mlops-infrastructure.feature-store-management"
  depends_on:
    - "data-state-management.data-quality"
  feeds_into:
    - "machine-learning-ai.model-validation.model-testing"
