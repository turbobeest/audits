audit:
  id: machine-learning-ai.data-quality.data-drift-detection
  name: Data Drift Detection Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: data-quality
  tier: expert
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates systems and processes for detecting data drift including input drift,
    concept drift, and label drift. Examines statistical methods used, thresholds
    configured, alerting mechanisms, and response procedures when drift is detected.
  why_it_matters: |
    Data drift causes model performance to degrade silently over time as production
    data diverges from training data. Without drift detection, models can become
    dangerously unreliable without anyone noticing until significant harm occurs.
    Early drift detection enables proactive model maintenance.
  when_to_run:
  - When setting up production ML monitoring
  - After model deployment
  - When model performance degrades
  - During periodic ML system reviews
prerequisites:
  required_artifacts:
  - type: monitoring_system
    description: Access to drift monitoring infrastructure
  - type: baseline_data
    description: Reference data distributions
  - type: production_data
    description: Access to production data streams
  access_requirements:
  - Access to monitoring dashboards
  - Access to drift detection configurations
  - Access to production data samples
  - Access to alerting systems
discovery:
  file_patterns:
  - glob: '**/monitoring/**/*.py'
    purpose: Find monitoring code
  - glob: '**/drift/**'
    purpose: Find drift detection configs
  - glob: '**/evidently/**'
    purpose: Find Evidently configurations
  - glob: '**/alerts/**'
    purpose: Find alerting configurations
knowledge_sources:
  guides:
  - id: evidently-docs
    name: Evidently AI Documentation
    url: https://docs.evidentlyai.com/
  - id: nannyml-docs
    name: NannyML Documentation
    url: https://nannyml.readthedocs.io/
  standards:
  - id: drift-detection
    name: Data Drift Detection Best Practices
    relevance: Statistical methods for drift detection
tooling:
  analysis_tools:
  - tool: evidently
    purpose: ML monitoring and drift detection
  - tool: nannyml
    purpose: Performance estimation and drift
  - tool: whylogs
    purpose: Data profiling and drift
  - tool: alibi-detect
    purpose: Outlier and drift detection
  - tool: deepchecks
    purpose: ML validation and monitoring
signals:
  critical:
  - id: DD-CRIT-001
    signal: No drift detection implemented
    evidence_indicators:
    - No monitoring of input distributions
    - No comparison to training data
    - No alerting on data changes
    explanation: |
      Without drift detection, models can silently degrade, producing increasingly
      unreliable predictions that may cause significant harm before detection.
    remediation: Implement comprehensive drift detection using Evidently or similar
  - id: DD-CRIT-002
    signal: Severe drift detected but not acted upon
    evidence_indicators:
    - Drift alerts ignored or disabled
    - No response procedures defined
    - Models continue serving despite drift
    explanation: |
      Detected drift without response defeats the purpose of monitoring and allows
      degraded models to continue causing harm.
    remediation: Define and implement drift response procedures
  high:
  - id: DD-HIGH-001
    signal: Drift thresholds not calibrated
    remediation: Calibrate thresholds based on model sensitivity analysis
  - id: DD-HIGH-002
    signal: Only univariate drift monitored
    remediation: Implement multivariate drift detection
  - id: DD-HIGH-003
    signal: Concept drift not monitored
    remediation: Add monitoring for concept drift when labels available
  medium:
  - id: DD-MED-001
    signal: Drift monitoring latency too high
    remediation: Reduce monitoring frequency or implement streaming detection
  - id: DD-MED-002
    signal: No root cause analysis for drift
    remediation: Implement feature attribution for drift sources
  - id: DD-MED-003
    signal: Baseline data not updated
    remediation: Define process for updating reference distributions
  low:
  - id: DD-LOW-001
    signal: Drift reports not reviewed regularly
    remediation: Establish regular drift review cadence
  - id: DD-LOW-002
    signal: Historical drift trends not tracked
    remediation: Implement drift trend analysis and visualization
  positive:
  - id: DD-POS-001
    signal: Comprehensive multivariate drift monitoring
  - id: DD-POS-002
    signal: Automated response to significant drift
  - id: DD-POS-003
    signal: Well-calibrated drift thresholds with low false positives
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Review Drift Detection Implementation
    description: Examine drift detection methods and configuration
    duration_estimate: 1 hour
  - id: '2'
    name: Evaluate Statistical Methods
    description: Assess appropriateness of drift detection statistics
    duration_estimate: 45 minutes
  - id: '3'
    name: Check Threshold Calibration
    description: Review how thresholds were set and calibrated
    duration_estimate: 45 minutes
  - id: '4'
    name: Test Alerting System
    description: Verify drift alerts are working properly
    duration_estimate: 30 minutes
  - id: '5'
    name: Review Response Procedures
    description: Assess procedures when drift is detected
    duration_estimate: 45 minutes
  - id: '6'
    name: Analyze Historical Drift
    description: Review past drift events and responses
    duration_estimate: 45 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Drift Detection Architecture
    - Threshold Analysis
    - Response Procedures
    - Recommendations
closeout_checklist:
- id: dd-001
  item: Drift detection coverage verified
  level: CRITICAL
  verification: automated
- id: dd-002
  item: Alerting tested
  level: HIGH
  verification: automated
- id: dd-003
  item: Response procedures documented
  level: HIGH
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
  compliance_mappings:
  - framework: EU AI Act
    control: Post-Market Monitoring
    description: Requirements for monitoring AI systems in production
relationships:
  commonly_combined:
  - machine-learning-ai.model-monitoring.performance-monitoring
  - machine-learning-ai.model-monitoring.alerting-configuration
  depends_on:
  - machine-learning-ai.data-quality.training-data-quality
  feeds_into:
  - machine-learning-ai.model-deployment.model-retraining
