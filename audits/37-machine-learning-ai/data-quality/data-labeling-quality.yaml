# ============================================================
# AUDIT: Data Labeling Quality
# Category: 37 - Machine Learning & AI
# Subcategory: data-quality
# ============================================================

audit:
  id: "machine-learning-ai.data-quality.data-labeling-quality"
  name: "Data Labeling Quality Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "data-quality"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates the quality of data labeling processes including label accuracy, consistency,
    annotator agreement, labeling guidelines, and quality assurance procedures. Examines
    whether labels are reliable enough to train production ML models.

  why_it_matters: |
    Label quality directly determines the ceiling of supervised model performance. Poor
    labels create models that learn from noise rather than signal, leading to unreliable
    predictions. High-quality labeling requires clear guidelines, trained annotators,
    and rigorous quality control.

  when_to_run:
    - "Before using newly labeled data"
    - "When model performance is unexpectedly poor"
    - "After changing labeling vendors or processes"
    - "During labeling program reviews"

prerequisites:
  required_artifacts:
    - type: "labeled_data"
      description: "Access to labeled datasets"
    - type: "labeling_guidelines"
      description: "Annotation guidelines and instructions"
    - type: "annotator_metadata"
      description: "Information about annotators and their work"

  access_requirements:
    - "Access to labeled data samples"
    - "Access to labeling platform"
    - "Access to inter-annotator agreement metrics"
    - "Access to labeling guidelines"

discovery:
  file_patterns:
    - glob: "**/labels/**"
      purpose: "Find label files"
    - glob: "**/annotations/**"
      purpose: "Find annotation data"
    - glob: "**/labeling/**/*.md"
      purpose: "Find labeling guidelines"
    - glob: "**/ground_truth/**"
      purpose: "Find ground truth data"

knowledge_sources:
  guides:
    - id: "label-studio-docs"
      name: "Label Studio Documentation"
      url: "https://labelstud.io/guide/"
    - id: "prodigy-docs"
      name: "Prodigy Annotation Tool"
      url: "https://prodi.gy/docs"

  standards:
    - id: "labeling-best-practices"
      name: "Data Labeling Best Practices"
      relevance: "Standards for annotation quality"

tooling:
  analysis_tools:
    - tool: "label-studio"
      purpose: "Labeling platform"
    - tool: "prodigy"
      purpose: "Active learning labeling"
    - tool: "cleanlab"
      purpose: "Label error detection"
    - tool: "snorkel"
      purpose: "Programmatic labeling"
    - tool: "cohen_kappa"
      purpose: "Inter-annotator agreement"

signals:
  critical:
    - id: "DLQ-CRIT-001"
      signal: "Systematic labeling errors discovered"
      evidence_indicators:
        - "Consistent mislabeling of specific categories"
        - "Annotator misunderstanding of guidelines"
        - "Label distribution significantly different from expected"
      explanation: |
        Systematic labeling errors cause models to learn incorrect patterns consistently,
        leading to predictable failures that may cause significant harm.
      remediation: "Audit labels, retrain annotators, and relabel affected data"

    - id: "DLQ-CRIT-002"
      signal: "Very low inter-annotator agreement"
      evidence_indicators:
        - "Cohen's Kappa below 0.4"
        - "High variance in individual annotator labels"
        - "Ambiguous cases not handled consistently"
      explanation: |
        Low agreement indicates the labeling task is poorly defined or annotators are
        inadequately trained, making labels unreliable for training.
      remediation: "Clarify guidelines, improve training, and implement consensus mechanisms"

  high:
    - id: "DLQ-HIGH-001"
      signal: "No inter-annotator agreement measurement"
      remediation: "Implement double-labeling and agreement metrics"

    - id: "DLQ-HIGH-002"
      signal: "Labeling guidelines ambiguous or incomplete"
      remediation: "Revise guidelines with clear examples and edge cases"

    - id: "DLQ-HIGH-003"
      signal: "No quality assurance sampling"
      remediation: "Implement random sampling and expert review"

  medium:
    - id: "DLQ-MED-001"
      signal: "Annotator performance not tracked"
      remediation: "Implement per-annotator quality metrics"

    - id: "DLQ-MED-002"
      signal: "No handling for ambiguous cases"
      remediation: "Define process for handling and escalating ambiguous samples"

    - id: "DLQ-MED-003"
      signal: "Label versioning not implemented"
      remediation: "Implement label versioning to track changes"

  low:
    - id: "DLQ-LOW-001"
      signal: "Labeling metadata incomplete"
      remediation: "Track annotator ID, timestamp, and confidence"

    - id: "DLQ-LOW-002"
      signal: "No feedback loop to annotators"
      remediation: "Implement feedback on annotation quality"

  positive:
    - id: "DLQ-POS-001"
      signal: "High inter-annotator agreement (Kappa > 0.8)"
    - id: "DLQ-POS-002"
      signal: "Comprehensive quality assurance process"
    - id: "DLQ-POS-003"
      signal: "Clear guidelines with extensive examples"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Labeling Guidelines"
      description: "Assess clarity and completeness of annotation instructions"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Analyze Inter-Annotator Agreement"
      description: "Calculate and evaluate agreement metrics"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Sample Label Quality"
      description: "Manually review sample of labels for accuracy"
      duration_estimate: "1.5 hours"

    - id: "4"
      name: "Assess QA Processes"
      description: "Review quality assurance procedures"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Check Annotator Training"
      description: "Evaluate annotator onboarding and training"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Review Label Error Detection"
      description: "Use automated tools to detect potential label errors"
      duration_estimate: "45 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Agreement Analysis"
        - "Quality Sample Review"
        - "Process Assessment"
        - "Recommendations"

closeout_checklist:
  - id: "dlq-001"
    item: "Inter-annotator agreement calculated"
    level: "CRITICAL"
    verification: "automated"

  - id: "dlq-002"
    item: "Label quality sample reviewed"
    level: "HIGH"
    verification: "manual"

  - id: "dlq-003"
    item: "Guidelines assessed"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Data Governance"
      description: "Requirements for training data annotation quality"

relationships:
  commonly_combined:
    - "machine-learning-ai.data-quality.training-data-quality"
    - "machine-learning-ai.responsible-ai.bias-detection"
  depends_on:
    - "data-state-management.data-governance"
  feeds_into:
    - "machine-learning-ai.model-development.training-pipeline-quality"
