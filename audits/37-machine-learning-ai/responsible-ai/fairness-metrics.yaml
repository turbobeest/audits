# ============================================================
# AUDIT: Fairness Metrics
# Category: 37 - Machine Learning & AI
# Subcategory: responsible-ai
# ============================================================

audit:
  id: "machine-learning-ai.responsible-ai.fairness-metrics"
  name: "Fairness Metrics Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "responsible-ai"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates the selection, implementation, and monitoring of fairness metrics including
    demographic parity, equalized odds, calibration, and individual fairness measures.
    Examines whether chosen metrics align with fairness goals and are properly computed.

  why_it_matters: |
    Different fairness metrics capture different notions of fairness, and some are
    mutually incompatible. Selecting appropriate metrics for the context is crucial
    for meaningful fairness evaluation. Wrong metric choices can give false confidence
    while actual unfairness persists.

  when_to_run:
    - "When establishing fairness evaluation"
    - "Before model deployment"
    - "When fairness requirements change"
    - "During compliance reviews"

prerequisites:
  required_artifacts:
    - type: "fairness_requirements"
      description: "Fairness goals documentation"
    - type: "model_predictions"
      description: "Model outputs for evaluation"
    - type: "ground_truth"
      description: "Labels for metric computation"

  access_requirements:
    - "Access to model predictions"
    - "Access to protected attribute data"
    - "Access to fairness metric implementations"
    - "Access to requirements documentation"

discovery:
  file_patterns:
    - glob: "**/fairness/**/*.py"
      purpose: "Find fairness implementations"
    - glob: "**/metrics/fairness/**"
      purpose: "Find fairness metrics"
    - glob: "**/evaluation/**"
      purpose: "Find evaluation code"

knowledge_sources:
  guides:
    - id: "fairlearn-metrics"
      name: "Fairlearn Metrics Guide"
      url: "https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html"
    - id: "fairness-definitions"
      name: "Fairness Definitions Explained"
      url: "https://arxiv.org/abs/1610.02413"

  standards:
    - id: "fairness-metrics-standards"
      name: "Fairness Metrics Standards"
      relevance: "Standard fairness metric definitions"

tooling:
  analysis_tools:
    - tool: "fairlearn"
      purpose: "Fairness metrics library"
    - tool: "aif360"
      purpose: "Comprehensive fairness metrics"
    - tool: "responsible-ai-widgets"
      purpose: "Fairness visualization"
    - tool: "sklearn-fairness"
      purpose: "Scikit-learn fairness"

signals:
  critical:
    - id: "FM-CRIT-001"
      signal: "Fairness metrics incompatible with goals"
      evidence_indicators:
        - "Metric doesn't measure intended fairness concept"
        - "Metric allows unfair outcomes to pass"
        - "Context requires different fairness definition"
      explanation: |
        Using wrong metrics provides false assurance of fairness while actual unfairness
        persists, potentially causing harm to protected groups.
      remediation: "Reassess fairness requirements and select appropriate metrics"

    - id: "FM-CRIT-002"
      signal: "Fairness metrics incorrectly implemented"
      evidence_indicators:
        - "Calculation errors in metric implementation"
        - "Metrics computed on wrong population"
        - "Thresholds applied incorrectly"
      explanation: |
        Incorrect metric implementation leads to wrong conclusions about model fairness,
        allowing biased models to be deployed.
      remediation: "Audit and correct metric implementations"

  high:
    - id: "FM-HIGH-001"
      signal: "Using single fairness metric only"
      remediation: "Use multiple complementary fairness metrics"

    - id: "FM-HIGH-002"
      signal: "Metric tradeoffs not understood"
      remediation: "Document tradeoffs between chosen metrics"

    - id: "FM-HIGH-003"
      signal: "Fairness thresholds arbitrary"
      remediation: "Define thresholds based on context and requirements"

  medium:
    - id: "FM-MED-001"
      signal: "Metrics not monitored in production"
      remediation: "Add production fairness monitoring"

    - id: "FM-MED-002"
      signal: "Confidence intervals not computed"
      remediation: "Add uncertainty quantification to metrics"

    - id: "FM-MED-003"
      signal: "Metric selection not documented"
      remediation: "Document rationale for metric choices"

  low:
    - id: "FM-LOW-001"
      signal: "Historical fairness not tracked"
      remediation: "Track fairness metrics over time"

    - id: "FM-LOW-002"
      signal: "Fairness dashboards incomplete"
      remediation: "Enhance fairness visualization"

  positive:
    - id: "FM-POS-001"
      signal: "Multiple appropriate metrics with documented rationale"
    - id: "FM-POS-002"
      signal: "Production fairness monitoring"
    - id: "FM-POS-003"
      signal: "Context-appropriate thresholds"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Fairness Requirements"
      description: "Understand fairness goals and constraints"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Assess Metric Selection"
      description: "Evaluate appropriateness of chosen metrics"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Validate Implementations"
      description: "Verify metric calculations are correct"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Analyze Metric Tradeoffs"
      description: "Understand tradeoffs between metrics"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Check Threshold Setting"
      description: "Review how fairness thresholds are set"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Review Documentation"
      description: "Assess documentation of metric choices"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Metric Selection Analysis"
        - "Implementation Validation"
        - "Tradeoff Assessment"
        - "Recommendations"

closeout_checklist:
  - id: "fm-001"
    item: "Metric appropriateness assessed"
    level: "CRITICAL"
    verification: "manual"

  - id: "fm-002"
    item: "Implementations validated"
    level: "CRITICAL"
    verification: "automated"

  - id: "fm-003"
    item: "Tradeoffs documented"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications", "decision-systems"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Fairness Measures"
      description: "Requirements for measuring AI fairness"

relationships:
  commonly_combined:
    - "machine-learning-ai.responsible-ai.bias-detection"
    - "machine-learning-ai.model-validation.evaluation-metrics"
  depends_on:
    - "machine-learning-ai.model-validation.model-testing"
  feeds_into:
    - "machine-learning-ai.model-monitoring.performance-monitoring"
