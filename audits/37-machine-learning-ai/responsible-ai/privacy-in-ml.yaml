# ============================================================
# AUDIT: Privacy in ML
# Category: 37 - Machine Learning & AI
# Subcategory: responsible-ai
# ============================================================

audit:
  id: "machine-learning-ai.responsible-ai.privacy-in-ml"
  name: "Privacy in ML Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "responsible-ai"

  tier: "expert"
  estimated_duration: "5-7 hours"  # median: 6h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates privacy protection in ML systems including training data privacy,
    model privacy (preventing membership inference and model inversion), differential
    privacy implementation, and privacy-preserving ML techniques.

  why_it_matters: |
    ML models can memorize and leak training data, revealing sensitive information
    about individuals. Privacy attacks can extract training data or determine membership,
    violating privacy regulations and causing harm. Privacy-preserving ML protects
    individuals while enabling beneficial AI.

  when_to_run:
    - "When training on sensitive data"
    - "Before deploying models trained on personal data"
    - "During privacy compliance reviews"
    - "After privacy incidents"

prerequisites:
  required_artifacts:
    - type: "training_data_info"
      description: "Information about training data sensitivity"
    - type: "privacy_requirements"
      description: "Privacy requirements documentation"
    - type: "model_access"
      description: "Access to deployed models"

  access_requirements:
    - "Access to training data documentation"
    - "Access to privacy implementations"
    - "Access to model inference"
    - "Access to privacy compliance records"

discovery:
  file_patterns:
    - glob: "**/privacy/**/*.py"
      purpose: "Find privacy implementations"
    - glob: "**/differential_privacy/**"
      purpose: "Find DP implementations"
    - glob: "**/federated/**"
      purpose: "Find federated learning"

knowledge_sources:
  guides:
    - id: "opacus-docs"
      name: "Opacus Documentation"
      url: "https://opacus.ai/"
    - id: "tensorflow-privacy"
      name: "TensorFlow Privacy"
      url: "https://github.com/tensorflow/privacy"

  standards:
    - id: "ml-privacy"
      name: "ML Privacy Standards"
      relevance: "Privacy-preserving ML requirements"

tooling:
  analysis_tools:
    - tool: "opacus"
      purpose: "Differential privacy for PyTorch"
    - tool: "tensorflow-privacy"
      purpose: "Differential privacy for TensorFlow"
    - tool: "pysyft"
      purpose: "Privacy-preserving ML"
    - tool: "ml-privacy-meter"
      purpose: "Privacy attack evaluation"

signals:
  critical:
    - id: "PML-CRIT-001"
      signal: "Model memorizes and leaks sensitive data"
      evidence_indicators:
        - "Membership inference attacks successful"
        - "Training data extractable from model"
        - "Model inversion reveals private information"
      explanation: |
        Models that leak training data violate privacy regulations and can expose
        sensitive personal information, causing significant harm.
      remediation: "Implement differential privacy and validate privacy protections"

    - id: "PML-CRIT-002"
      signal: "Training on personal data without privacy protections"
      evidence_indicators:
        - "PII in training data without consent"
        - "No differential privacy or anonymization"
        - "Privacy impact assessment not conducted"
      explanation: |
        Training on personal data without protections creates privacy risks and
        potential regulatory violations.
      remediation: "Implement privacy-preserving training techniques"

  high:
    - id: "PML-HIGH-001"
      signal: "No privacy attack evaluation"
      remediation: "Conduct membership inference and model inversion tests"

    - id: "PML-HIGH-002"
      signal: "Differential privacy epsilon too large"
      remediation: "Reduce privacy budget for stronger guarantees"

    - id: "PML-HIGH-003"
      signal: "Privacy-utility tradeoff not analyzed"
      remediation: "Document privacy-utility tradeoffs"

  medium:
    - id: "PML-MED-001"
      signal: "No privacy budget tracking"
      remediation: "Implement privacy budget accounting"

    - id: "PML-MED-002"
      signal: "Gradient clipping not implemented"
      remediation: "Add gradient clipping for DP training"

    - id: "PML-MED-003"
      signal: "No federated learning for sensitive data"
      remediation: "Consider federated learning approach"

  low:
    - id: "PML-LOW-001"
      signal: "Privacy documentation incomplete"
      remediation: "Document privacy measures and guarantees"

    - id: "PML-LOW-002"
      signal: "Privacy training not provided"
      remediation: "Train team on ML privacy"

  positive:
    - id: "PML-POS-001"
      signal: "Validated differential privacy with appropriate epsilon"
    - id: "PML-POS-002"
      signal: "Privacy attacks evaluated and mitigated"
    - id: "PML-POS-003"
      signal: "Privacy budget properly tracked"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Assess Data Sensitivity"
      description: "Understand privacy requirements for training data"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Review Privacy Protections"
      description: "Evaluate implemented privacy measures"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Conduct Privacy Attacks"
      description: "Test for membership inference and model inversion"
      duration_estimate: "1.5 hours"

    - id: "4"
      name: "Evaluate DP Implementation"
      description: "Review differential privacy if implemented"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Check Compliance"
      description: "Verify privacy regulation compliance"
      duration_estimate: "45 minutes"

    - id: "6"
      name: "Document Findings"
      description: "Create privacy assessment report"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Privacy Protection Assessment"
        - "Attack Evaluation Results"
        - "Compliance Analysis"
        - "Recommendations"

closeout_checklist:
  - id: "pml-001"
    item: "Privacy attacks evaluated"
    level: "CRITICAL"
    verification: "automated"

  - id: "pml-002"
    item: "DP implementation reviewed"
    level: "HIGH"
    verification: "manual"

  - id: "pml-003"
    item: "Compliance verified"
    level: "CRITICAL"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications"]

  compliance_mappings:
    - framework: "GDPR"
      control: "Data Protection"
      description: "Personal data processing requirements"
    - framework: "CCPA"
      control: "Privacy Rights"
      description: "Consumer privacy rights"

relationships:
  commonly_combined:
    - "machine-learning-ai.data-quality.training-data-quality"
    - "privacy-security.data-privacy"
  depends_on:
    - "data-state-management.data-governance"
  feeds_into:
    - "machine-learning-ai.model-deployment.model-serving"
