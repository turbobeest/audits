# ============================================================
# AUDIT: Bias Detection
# Category: 37 - Machine Learning & AI
# Subcategory: responsible-ai
# ============================================================

audit:
  id: "machine-learning-ai.responsible-ai.bias-detection"
  name: "Bias Detection Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "responsible-ai"

  tier: "expert"
  estimated_duration: "5-8 hours"  # median: 6h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  requires_human_evaluation: true
  severity: "critical"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates bias detection practices including identification of protected attributes,
    bias metrics computation, sliced analysis, and bias mitigation strategies. Examines
    whether models are evaluated for unfair treatment of different groups.

  why_it_matters: |
    Biased models can cause significant harm to individuals and groups, leading to unfair
    treatment, discrimination, and legal liability. Systematic bias detection is essential
    for responsible AI deployment and compliance with emerging AI regulations.

  when_to_run:
    - "Before model deployment"
    - "When using new data sources"
    - "During compliance reviews"
    - "After bias-related incidents"

prerequisites:
  required_artifacts:
    - type: "model_artifacts"
      description: "Access to trained models"
    - type: "protected_attributes"
      description: "List of protected attributes"
    - type: "evaluation_data"
      description: "Data with demographic information"

  access_requirements:
    - "Access to model predictions"
    - "Access to protected attribute data"
    - "Access to bias analysis tools"
    - "Access to compliance documentation"

discovery:
  file_patterns:
    - glob: "**/fairness/**/*.py"
      purpose: "Find fairness code"
    - glob: "**/bias/**"
      purpose: "Find bias analysis code"
    - glob: "**/responsible_ai/**"
      purpose: "Find responsible AI implementations"

knowledge_sources:
  guides:
    - id: "fairlearn-docs"
      name: "Fairlearn Documentation"
      url: "https://fairlearn.org/main/"
    - id: "aif360-docs"
      name: "AI Fairness 360"
      url: "https://aif360.mybluemix.net/"

  standards:
    - id: "nist-ai-bias"
      name: "NIST AI Bias Standards"
      relevance: "Bias assessment guidelines"

tooling:
  analysis_tools:
    - tool: "fairlearn"
      purpose: "Fairness metrics and mitigation"
    - tool: "aif360"
      purpose: "AI Fairness 360 toolkit"
    - tool: "what-if-tool"
      purpose: "ML fairness exploration"
    - tool: "responsible-ai-toolbox"
      purpose: "Microsoft responsible AI"

signals:
  critical:
    - id: "BD-CRIT-001"
      signal: "Significant bias detected against protected groups"
      evidence_indicators:
        - "Disparate impact ratio below 0.8"
        - "Large performance gaps between groups"
        - "Systematic disadvantage to specific demographics"
      explanation: |
        Significant bias can cause real harm to affected individuals and groups, creating
        legal liability and reputational damage while perpetuating discrimination.
      remediation: "Implement bias mitigation techniques and validate effectiveness"

    - id: "BD-CRIT-002"
      signal: "No bias evaluation performed"
      evidence_indicators:
        - "Model deployed without fairness analysis"
        - "Protected attributes not considered"
        - "No fairness metrics tracked"
      explanation: |
        Deploying models without bias evaluation risks unknowingly causing harm to
        protected groups.
      remediation: "Conduct comprehensive bias evaluation before deployment"

  high:
    - id: "BD-HIGH-001"
      signal: "Protected attributes not identified"
      remediation: "Identify and document relevant protected attributes"

    - id: "BD-HIGH-002"
      signal: "Proxy discrimination not analyzed"
      remediation: "Analyze features for proxy discrimination"

    - id: "BD-HIGH-003"
      signal: "Bias evaluation uses limited metrics"
      remediation: "Use multiple fairness metrics comprehensively"

  medium:
    - id: "BD-MED-001"
      signal: "Bias analysis not automated"
      remediation: "Integrate bias analysis into CI/CD"

    - id: "BD-MED-002"
      signal: "Intersectional bias not analyzed"
      remediation: "Evaluate bias across attribute combinations"

    - id: "BD-MED-003"
      signal: "Bias thresholds not defined"
      remediation: "Define acceptable thresholds for fairness metrics"

  low:
    - id: "BD-LOW-001"
      signal: "Bias documentation incomplete"
      remediation: "Document bias analysis methodology and results"

    - id: "BD-LOW-002"
      signal: "Historical bias analysis not retained"
      remediation: "Archive bias analysis for trending"

  positive:
    - id: "BD-POS-001"
      signal: "Comprehensive bias analysis with multiple metrics"
    - id: "BD-POS-002"
      signal: "Automated bias monitoring in production"
    - id: "BD-POS-003"
      signal: "Documented mitigation strategies"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Identify Protected Attributes"
      description: "Determine relevant protected attributes"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Compute Fairness Metrics"
      description: "Calculate multiple bias metrics"
      duration_estimate: "1.5 hours"

    - id: "3"
      name: "Analyze Proxy Discrimination"
      description: "Check for indirect discrimination"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Evaluate Intersectional Bias"
      description: "Assess bias across attribute combinations"
      duration_estimate: "1 hour"

    - id: "5"
      name: "Review Mitigation Options"
      description: "Identify potential bias mitigation strategies"
      duration_estimate: "45 minutes"

    - id: "6"
      name: "Document Findings"
      description: "Create comprehensive bias report"
      duration_estimate: "45 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Fairness Metrics Analysis"
        - "Protected Group Analysis"
        - "Mitigation Recommendations"
        - "Compliance Assessment"

closeout_checklist:
  - id: "bd-001"
    item: "Fairness metrics computed"
    level: "CRITICAL"
    verification: "automated"

  - id: "bd-002"
    item: "Intersectional analysis completed"
    level: "HIGH"
    verification: "automated"

  - id: "bd-003"
    item: "Mitigation options documented"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications", "decision-systems"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Non-Discrimination"
      description: "Requirements for avoiding discrimination"
    - framework: "NIST AI RMF"
      control: "Fairness"
      description: "AI fairness requirements"

relationships:
  commonly_combined:
    - "machine-learning-ai.responsible-ai.fairness-metrics"
    - "machine-learning-ai.data-quality.training-data-quality"
  depends_on:
    - "machine-learning-ai.model-validation.model-testing"
  feeds_into:
    - "machine-learning-ai.model-deployment.model-serving"
