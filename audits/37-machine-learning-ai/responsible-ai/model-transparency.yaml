audit:
  id: machine-learning-ai.responsible-ai.model-transparency
  name: Model Transparency Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: responsible-ai
  tier: expert
  estimated_duration: 3-5 hours  # median: 4h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates model transparency practices including model cards, documentation,
    capability and limitation disclosure, and appropriate user communication. Examines
    whether stakeholders have sufficient information about AI system behavior.
  why_it_matters: |
    Transparency enables appropriate reliance on AI systems, supports accountability,
    and is required by emerging AI regulations. Without transparency, users cannot make
    informed decisions about AI use, and organizations cannot demonstrate responsible
    AI practices.
  when_to_run:
  - Before public deployment
  - When documentation needs updating
  - During compliance reviews
  - When stakeholder requirements change
prerequisites:
  required_artifacts:
  - type: model_documentation
    description: Existing model documentation
  - type: model_cards
    description: Model cards if available
  - type: user_communication
    description: User-facing AI descriptions
  access_requirements:
  - Access to model documentation
  - Access to model cards
  - Access to user interfaces
  - Access to stakeholder requirements
discovery:
  file_patterns:
  - glob: '**/docs/**/*.md'
    purpose: Find documentation
  - glob: '**/model_cards/**'
    purpose: Find model cards
  - glob: '**/transparency/**'
    purpose: Find transparency documentation
knowledge_sources:
  guides:
  - id: model-cards
    name: Model Cards Paper
    url: https://arxiv.org/abs/1810.03993
  - id: transparency-guide
    name: AI Transparency Guide
    url: https://pair.withgoogle.com/chapter/transparency/
  standards:
  - id: ai-transparency
    name: AI Transparency Standards
    relevance: Transparency requirements
tooling:
  analysis_tools:
  - tool: model-card-toolkit
    purpose: Model card generation
  - tool: datasheets
    purpose: Dataset documentation
signals:
  critical:
  - id: MT-CRIT-001
    signal: No transparency documentation for high-risk AI
    evidence_indicators:
    - High-risk application without model cards
    - No capability/limitation disclosure
    - Users unaware they're interacting with AI
    explanation: |
      Lack of transparency for high-risk AI violates regulations and prevents
      appropriate oversight and informed consent.
    remediation: Create comprehensive transparency documentation
  - id: MT-CRIT-002
    signal: Documentation contains false claims
    evidence_indicators:
    - Overstated capabilities
    - Hidden limitations
    - Misleading performance claims
    explanation: |
      False documentation is worse than no documentation, creating harmful
      over-reliance and potential liability.
    remediation: Correct documentation to accurately represent system
  high:
  - id: MT-HIGH-001
    signal: Model cards missing or incomplete
    remediation: Create comprehensive model cards
  - id: MT-HIGH-002
    signal: Limitations not clearly documented
    remediation: Document known limitations prominently
  - id: MT-HIGH-003
    signal: Intended use not specified
    remediation: Define and document intended use cases
  medium:
  - id: MT-MED-001
    signal: Documentation not accessible
    remediation: Make documentation available to stakeholders
  - id: MT-MED-002
    signal: Training data not described
    remediation: Add training data documentation
  - id: MT-MED-003
    signal: Documentation not maintained
    remediation: Establish documentation update process
  low:
  - id: MT-LOW-001
    signal: Documentation format inconsistent
    remediation: Standardize documentation format
  - id: MT-LOW-002
    signal: Technical documentation not translated for lay audiences
    remediation: Create user-appropriate documentation
  positive:
  - id: MT-POS-001
    signal: Comprehensive model cards with clear limitations
  - id: MT-POS-002
    signal: User-appropriate transparency disclosures
  - id: MT-POS-003
    signal: Regularly maintained documentation
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Review Transparency Requirements
    description: Understand stakeholder and regulatory needs
    duration_estimate: 30 minutes
  - id: '2'
    name: Assess Model Cards
    description: Evaluate model card completeness
    duration_estimate: 45 minutes
  - id: '3'
    name: Verify Accuracy
    description: Check documentation accuracy
    duration_estimate: 45 minutes
  - id: '4'
    name: Check Limitation Disclosure
    description: Verify limitations are clearly documented
    duration_estimate: 30 minutes
  - id: '5'
    name: Evaluate Accessibility
    description: Assess documentation accessibility
    duration_estimate: 30 minutes
  - id: '6'
    name: Review User Communication
    description: Check user-facing AI disclosures
    duration_estimate: 30 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Documentation Assessment
    - Accuracy Verification
    - Accessibility Review
    - Recommendations
closeout_checklist:
- id: mt-001
  item: Model cards reviewed
  level: CRITICAL
  verification: manual
- id: mt-002
  item: Accuracy verified
  level: CRITICAL
  verification: manual
- id: mt-003
  item: Accessibility assessed
  level: MEDIUM
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
  compliance_mappings:
  - framework: EU AI Act
    control: Transparency
    description: AI system transparency requirements
relationships:
  commonly_combined:
  - machine-learning-ai.responsible-ai.explainability-implementation
  - machine-learning-ai.model-deployment.model-versioning
  depends_on:
  - machine-learning-ai.model-validation.model-testing
  feeds_into:
  - machine-learning-ai.responsible-ai.human-oversight
