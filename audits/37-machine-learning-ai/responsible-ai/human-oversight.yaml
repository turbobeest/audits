# ============================================================
# AUDIT: Human Oversight
# Category: 37 - Machine Learning & AI
# Subcategory: responsible-ai
# ============================================================

audit:
  id: "machine-learning-ai.responsible-ai.human-oversight"
  name: "Human Oversight Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "responsible-ai"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates human oversight mechanisms for AI systems including human-in-the-loop
    processes, override capabilities, monitoring interfaces, and escalation procedures.
    Examines whether humans can effectively supervise and control AI system behavior.

  why_it_matters: |
    Human oversight is essential for catching AI errors, handling edge cases, and
    maintaining accountability. Without effective oversight, AI systems can cause
    harm without detection or correction. Proper oversight enables safe AI deployment
    while maintaining human control.

  when_to_run:
    - "Before deploying autonomous systems"
    - "When oversight failures occur"
    - "During compliance reviews"
    - "When changing human-AI workflows"

prerequisites:
  required_artifacts:
    - type: "oversight_procedures"
      description: "Human oversight documentation"
    - type: "override_mechanisms"
      description: "Override system access"
    - type: "monitoring_interfaces"
      description: "Human monitoring tools"

  access_requirements:
    - "Access to oversight interfaces"
    - "Access to override mechanisms"
    - "Access to escalation procedures"
    - "Access to audit logs"

discovery:
  file_patterns:
    - glob: "**/oversight/**"
      purpose: "Find oversight implementations"
    - glob: "**/review/**"
      purpose: "Find review queue code"
    - glob: "**/escalation/**"
      purpose: "Find escalation procedures"
    - glob: "**/override/**"
      purpose: "Find override mechanisms"

knowledge_sources:
  guides:
    - id: "hitl-design"
      name: "Human-in-the-Loop Design"
      url: "https://pair.withgoogle.com/guidebook/"
    - id: "eu-ai-oversight"
      name: "EU AI Act Human Oversight"
      url: "https://artificialintelligenceact.eu/"

  standards:
    - id: "human-oversight-standards"
      name: "AI Human Oversight Standards"
      relevance: "Human control requirements"

tooling:
  analysis_tools:
    - tool: "label-studio"
      purpose: "Human review interface"
    - tool: "prodigy"
      purpose: "Active learning with human review"
    - tool: "custom-review-tools"
      purpose: "Domain-specific review interfaces"

signals:
  critical:
    - id: "HO-CRIT-001"
      signal: "No human oversight for high-stakes decisions"
      evidence_indicators:
        - "Fully autonomous high-risk decisions"
        - "No human review capability"
        - "Override mechanisms missing"
      explanation: |
        High-stakes AI decisions without human oversight risk causing significant harm
        without opportunity for human intervention or correction.
      remediation: "Implement human-in-the-loop for high-stakes decisions"

    - id: "HO-CRIT-002"
      signal: "Override mechanisms non-functional"
      evidence_indicators:
        - "Cannot override AI decisions"
        - "Override too slow to be useful"
        - "Override not properly logged"
      explanation: |
        Non-functional overrides mean humans cannot intervene when AI systems make errors,
        allowing harm to continue uncorrected.
      remediation: "Fix and test override mechanisms"

  high:
    - id: "HO-HIGH-001"
      signal: "Oversight interface unusable"
      remediation: "Improve oversight interface usability"

    - id: "HO-HIGH-002"
      signal: "No escalation path for uncertain cases"
      remediation: "Implement escalation procedures"

    - id: "HO-HIGH-003"
      signal: "Oversight actions not logged"
      remediation: "Implement comprehensive audit logging"

  medium:
    - id: "HO-MED-001"
      signal: "Oversight workload unsustainable"
      remediation: "Optimize case selection for human review"

    - id: "HO-MED-002"
      signal: "Reviewers lack necessary context"
      remediation: "Enhance information provided to reviewers"

    - id: "HO-MED-003"
      signal: "No feedback loop from oversight"
      remediation: "Use oversight findings to improve model"

  low:
    - id: "HO-LOW-001"
      signal: "Oversight metrics not tracked"
      remediation: "Track oversight volume and outcomes"

    - id: "HO-LOW-002"
      signal: "Oversight documentation incomplete"
      remediation: "Document oversight procedures"

  positive:
    - id: "HO-POS-001"
      signal: "Effective human-in-the-loop for high-stakes decisions"
    - id: "HO-POS-002"
      signal: "Fast, reliable override capability"
    - id: "HO-POS-003"
      signal: "Comprehensive audit logging"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Oversight Requirements"
      description: "Understand oversight needs and regulations"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Assess HITL Processes"
      description: "Evaluate human-in-the-loop implementation"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Test Override Mechanisms"
      description: "Verify override capabilities work"
      duration_estimate: "45 minutes"

    - id: "4"
      name: "Evaluate Interfaces"
      description: "Assess oversight interface usability"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Check Audit Logging"
      description: "Verify oversight actions are logged"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Review Escalation"
      description: "Assess escalation procedures"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Oversight Implementation"
        - "Override Capability"
        - "Interface Assessment"
        - "Recommendations"

closeout_checklist:
  - id: "ho-001"
    item: "Override mechanisms tested"
    level: "CRITICAL"
    verification: "manual"

  - id: "ho-002"
    item: "Interface usability assessed"
    level: "HIGH"
    verification: "manual"

  - id: "ho-003"
    item: "Audit logging verified"
    level: "HIGH"
    verification: "automated"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications", "autonomous-systems"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Human Oversight"
      description: "Requirements for human oversight of AI"

relationships:
  commonly_combined:
    - "machine-learning-ai.responsible-ai.explainability-implementation"
    - "machine-learning-ai.model-monitoring.alerting-configuration"
  depends_on:
    - "machine-learning-ai.model-deployment.model-serving"
  feeds_into:
    - "machine-learning-ai.model-monitoring.incident-response"
