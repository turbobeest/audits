# ============================================================
# AUDIT: Explainability Implementation
# Category: 37 - Machine Learning & AI
# Subcategory: responsible-ai
# ============================================================

audit:
  id: "machine-learning-ai.responsible-ai.explainability-implementation"
  name: "Explainability Implementation Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "responsible-ai"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates model explainability implementations including global and local
    explanation methods, feature importance, counterfactual explanations, and
    explanation interfaces. Examines whether explanations are accurate, useful,
    and appropriate for intended audiences.

  why_it_matters: |
    Explainability is crucial for trust, debugging, compliance, and user understanding.
    Poor explanations mislead users, hide problems, and fail compliance requirements.
    Good explanations enable appropriate reliance on AI systems and support human oversight.

  when_to_run:
    - "Before deployment requiring explainability"
    - "When explanations seem incorrect"
    - "During compliance reviews"
    - "When adding new explanation methods"

prerequisites:
  required_artifacts:
    - type: "explanation_implementations"
      description: "Access to explainability code"
    - type: "model_artifacts"
      description: "Models to explain"
    - type: "audience_requirements"
      description: "Explanation audience documentation"

  access_requirements:
    - "Access to explainability tools"
    - "Access to model inference"
    - "Access to explanation interfaces"
    - "Access to user requirements"

discovery:
  file_patterns:
    - glob: "**/explainability/**/*.py"
      purpose: "Find explainability code"
    - glob: "**/shap/**"
      purpose: "Find SHAP implementations"
    - glob: "**/lime/**"
      purpose: "Find LIME implementations"
    - glob: "**/interpret/**"
      purpose: "Find interpretation code"

knowledge_sources:
  guides:
    - id: "shap-docs"
      name: "SHAP Documentation"
      url: "https://shap.readthedocs.io/"
    - id: "lime-docs"
      name: "LIME Documentation"
      url: "https://lime-ml.readthedocs.io/"
    - id: "captum-docs"
      name: "Captum Documentation"
      url: "https://captum.ai/"

  standards:
    - id: "explainability-standards"
      name: "AI Explainability Standards"
      relevance: "Explainable AI requirements"

tooling:
  analysis_tools:
    - tool: "shap"
      purpose: "SHAP explanations"
    - tool: "lime"
      purpose: "Local explanations"
    - tool: "captum"
      purpose: "PyTorch interpretability"
    - tool: "alibi"
      purpose: "Model explanations"
    - tool: "interpret"
      purpose: "InterpretML library"

signals:
  critical:
    - id: "EI-CRIT-001"
      signal: "Explanations misleading or incorrect"
      evidence_indicators:
        - "Explanations don't match model behavior"
        - "Important features missing from explanations"
        - "Explanations lead to wrong conclusions"
      explanation: |
        Incorrect explanations are worse than no explanations because they create false
        confidence and can lead to harmful decisions.
      remediation: "Validate explanations against model behavior and correct implementations"

    - id: "EI-CRIT-002"
      signal: "Required explainability not implemented"
      evidence_indicators:
        - "High-risk application without explanations"
        - "Regulatory requirements not met"
        - "No explanation capability for decisions"
      explanation: |
        Missing explainability in regulated or high-stakes applications creates compliance
        risk and prevents appropriate human oversight.
      remediation: "Implement appropriate explainability methods"

  high:
    - id: "EI-HIGH-001"
      signal: "Explanations not appropriate for audience"
      remediation: "Adapt explanations for intended users"

    - id: "EI-HIGH-002"
      signal: "Only using single explanation method"
      remediation: "Use multiple complementary explanation approaches"

    - id: "EI-HIGH-003"
      signal: "Explanations too slow for use case"
      remediation: "Optimize or cache explanations"

  medium:
    - id: "EI-MED-001"
      signal: "No global model explanations"
      remediation: "Add global feature importance and model summaries"

    - id: "EI-MED-002"
      signal: "Explanation validation missing"
      remediation: "Validate explanations for faithfulness"

    - id: "EI-MED-003"
      signal: "No counterfactual explanations"
      remediation: "Add counterfactual explanation capability"

  low:
    - id: "EI-LOW-001"
      signal: "Explanation documentation incomplete"
      remediation: "Document explanation methods and limitations"

    - id: "EI-LOW-002"
      signal: "Explanation interfaces not user-friendly"
      remediation: "Improve explanation presentation"

  positive:
    - id: "EI-POS-001"
      signal: "Validated explanations appropriate for audience"
    - id: "EI-POS-002"
      signal: "Multiple explanation methods implemented"
    - id: "EI-POS-003"
      signal: "Clear documentation of limitations"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Explainability Requirements"
      description: "Understand explanation needs and audiences"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Assess Implementation"
      description: "Evaluate explanation methods used"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Validate Explanation Accuracy"
      description: "Test that explanations match model behavior"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Evaluate User Appropriateness"
      description: "Assess if explanations suit intended audience"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Check Performance"
      description: "Verify explanation latency is acceptable"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Review Documentation"
      description: "Assess explanation documentation"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Implementation Analysis"
        - "Validation Results"
        - "User Appropriateness"
        - "Recommendations"

closeout_checklist:
  - id: "ei-001"
    item: "Explanation accuracy validated"
    level: "CRITICAL"
    verification: "manual"

  - id: "ei-002"
    item: "Audience appropriateness assessed"
    level: "HIGH"
    verification: "manual"

  - id: "ei-003"
    item: "Performance verified"
    level: "MEDIUM"
    verification: "automated"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications", "decision-systems"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Transparency"
      description: "Requirements for AI system explainability"
    - framework: "GDPR"
      control: "Right to Explanation"
      description: "Automated decision explanation requirements"

relationships:
  commonly_combined:
    - "machine-learning-ai.responsible-ai.bias-detection"
    - "machine-learning-ai.model-validation.model-testing"
  depends_on:
    - "machine-learning-ai.model-deployment.model-serving"
  feeds_into:
    - "machine-learning-ai.responsible-ai.human-oversight"
