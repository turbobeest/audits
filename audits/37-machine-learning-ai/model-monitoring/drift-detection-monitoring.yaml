# ============================================================
# AUDIT: Drift Detection Monitoring
# Category: 37 - Machine Learning & AI
# Subcategory: model-monitoring
# ============================================================

audit:
  id: "machine-learning-ai.model-monitoring.drift-detection-monitoring"
  name: "Drift Detection Monitoring Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "model-monitoring"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates production drift detection systems including input drift, prediction
    drift, and concept drift monitoring. Examines statistical methods, thresholds,
    and response procedures when drift is detected in deployed models.

  why_it_matters: |
    Data and concept drift cause model degradation over time as production data diverges
    from training data. Without drift monitoring, performance degrades silently until
    significant harm occurs. Early drift detection enables proactive model maintenance.

  when_to_run:
    - "After model deployment"
    - "When performance unexpectedly degrades"
    - "During monitoring system reviews"
    - "Periodically for drift validation"

prerequisites:
  required_artifacts:
    - type: "drift_monitoring"
      description: "Access to drift detection systems"
    - type: "reference_data"
      description: "Baseline data distributions"
    - type: "production_data"
      description: "Access to production data streams"

  access_requirements:
    - "Access to drift detection tools"
    - "Access to production data"
    - "Access to reference distributions"
    - "Access to alerting systems"

discovery:
  file_patterns:
    - glob: "**/drift/**/*.py"
      purpose: "Find drift detection code"
    - glob: "**/monitoring/**"
      purpose: "Find monitoring implementations"
    - glob: "**/evidently/**"
      purpose: "Find Evidently configurations"
    - glob: "**/alibi_detect/**"
      purpose: "Find Alibi Detect configs"

knowledge_sources:
  guides:
    - id: "evidently-drift"
      name: "Evidently Data Drift Guide"
      url: "https://docs.evidentlyai.com/user-guide/data-drift"
    - id: "alibi-detect"
      name: "Alibi Detect Documentation"
      url: "https://docs.seldon.io/projects/alibi-detect/"

  standards:
    - id: "drift-detection-methods"
      name: "Drift Detection Methods"
      relevance: "Statistical drift detection techniques"

tooling:
  analysis_tools:
    - tool: "evidently"
      purpose: "Drift detection and reporting"
    - tool: "alibi-detect"
      purpose: "Drift and outlier detection"
    - tool: "nannyml"
      purpose: "Performance estimation with drift"
    - tool: "whylogs"
      purpose: "Data profiling for drift"

signals:
  critical:
    - id: "DDM-CRIT-001"
      signal: "No drift detection in production"
      evidence_indicators:
        - "Input distributions not monitored"
        - "No comparison to training data"
        - "Drift discovered only through failures"
      explanation: |
        Without drift detection, models can silently degrade as production data changes,
        leading to increasingly poor predictions until major failures occur.
      remediation: "Implement comprehensive drift monitoring"

    - id: "DDM-CRIT-002"
      signal: "Severe drift detected without response"
      evidence_indicators:
        - "Significant distribution changes ignored"
        - "No remediation triggered"
        - "Model continues serving with known drift"
      explanation: |
        Ignoring detected drift defeats the purpose of monitoring and allows degraded
        models to continue harming users.
      remediation: "Implement drift response procedures and enforce them"

  high:
    - id: "DDM-HIGH-001"
      signal: "Only univariate drift monitored"
      remediation: "Add multivariate drift detection"

    - id: "DDM-HIGH-002"
      signal: "Drift thresholds not calibrated"
      remediation: "Calibrate thresholds based on model sensitivity"

    - id: "DDM-HIGH-003"
      signal: "Concept drift not monitored"
      remediation: "Implement concept drift detection when labels available"

  medium:
    - id: "DDM-MED-001"
      signal: "Drift detection latency too high"
      remediation: "Reduce monitoring frequency for faster detection"

    - id: "DDM-MED-002"
      signal: "No drift attribution"
      remediation: "Implement feature-level drift analysis"

    - id: "DDM-MED-003"
      signal: "Reference data not updated"
      remediation: "Define reference data update strategy"

  low:
    - id: "DDM-LOW-001"
      signal: "Drift metrics not visualized"
      remediation: "Add drift dashboards"

    - id: "DDM-LOW-002"
      signal: "Historical drift not tracked"
      remediation: "Retain drift history for trend analysis"

  positive:
    - id: "DDM-POS-001"
      signal: "Comprehensive multivariate drift detection"
    - id: "DDM-POS-002"
      signal: "Automated response to significant drift"
    - id: "DDM-POS-003"
      signal: "Feature-level drift attribution"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Drift Detection Coverage"
      description: "Assess which drift types are monitored"
      duration_estimate: "1 hour"

    - id: "2"
      name: "Evaluate Statistical Methods"
      description: "Review drift detection algorithms"
      duration_estimate: "45 minutes"

    - id: "3"
      name: "Check Threshold Calibration"
      description: "Assess threshold settings and rationale"
      duration_estimate: "45 minutes"

    - id: "4"
      name: "Test Detection Capability"
      description: "Validate drift detection with synthetic drift"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Review Response Procedures"
      description: "Evaluate actions taken when drift detected"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Analyze Historical Drift"
      description: "Review past drift events and responses"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Detection Coverage"
        - "Threshold Analysis"
        - "Response Procedures"
        - "Recommendations"

closeout_checklist:
  - id: "ddm-001"
    item: "Drift detection coverage verified"
    level: "CRITICAL"
    verification: "automated"

  - id: "ddm-002"
    item: "Thresholds calibrated"
    level: "HIGH"
    verification: "manual"

  - id: "ddm-003"
    item: "Response procedures documented"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Post-Market Monitoring"
      description: "Requirements for monitoring AI performance"

relationships:
  commonly_combined:
    - "machine-learning-ai.model-monitoring.performance-monitoring"
    - "machine-learning-ai.data-quality.data-drift-detection"
  depends_on:
    - "machine-learning-ai.data-quality.training-data-quality"
  feeds_into:
    - "machine-learning-ai.model-deployment.progressive-rollout"
