# ============================================================
# AUDIT: Performance Monitoring
# Category: 37 - Machine Learning & AI
# Subcategory: model-monitoring
# ============================================================

audit:
  id: "machine-learning-ai.model-monitoring.performance-monitoring"
  name: "Performance Monitoring Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "model-monitoring"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates ML model performance monitoring including metric collection, baseline
    comparison, degradation detection, and performance trending. Examines whether
    model performance in production is tracked effectively and issues are detected early.

  why_it_matters: |
    Production model performance can degrade silently due to data drift, concept drift,
    or infrastructure issues. Without proper monitoring, degradation goes undetected
    until significant harm occurs. Proactive monitoring enables early intervention
    and maintains model reliability.

  when_to_run:
    - "After model deployment"
    - "When establishing monitoring practices"
    - "After monitoring gaps discovered"
    - "During model health reviews"

prerequisites:
  required_artifacts:
    - type: "monitoring_infrastructure"
      description: "Access to monitoring systems"
    - type: "baseline_metrics"
      description: "Expected performance baselines"
    - type: "ground_truth"
      description: "Labels for performance calculation"

  access_requirements:
    - "Access to monitoring dashboards"
    - "Access to metric collection systems"
    - "Access to logging infrastructure"
    - "Access to alerting systems"

discovery:
  file_patterns:
    - glob: "**/monitoring/**/*.py"
      purpose: "Find monitoring code"
    - glob: "**/metrics/**"
      purpose: "Find metric definitions"
    - glob: "**/dashboards/**"
      purpose: "Find dashboard configs"
    - glob: "**/prometheus/**"
      purpose: "Find Prometheus configs"

knowledge_sources:
  guides:
    - id: "evidently-docs"
      name: "Evidently AI Documentation"
      url: "https://docs.evidentlyai.com/"
    - id: "nannyml-docs"
      name: "NannyML Documentation"
      url: "https://nannyml.readthedocs.io/"

  standards:
    - id: "ml-monitoring"
      name: "ML Monitoring Best Practices"
      relevance: "Production ML observability"

tooling:
  analysis_tools:
    - tool: "evidently"
      purpose: "ML monitoring platform"
    - tool: "nannyml"
      purpose: "Performance estimation"
    - tool: "prometheus"
      purpose: "Metric collection"
    - tool: "grafana"
      purpose: "Visualization and dashboards"
    - tool: "datadog"
      purpose: "Observability platform"

signals:
  critical:
    - id: "PM-CRIT-001"
      signal: "No production performance monitoring"
      evidence_indicators:
        - "Model metrics not collected"
        - "No dashboards for model health"
        - "Performance unknown until complaints"
      explanation: |
        Without monitoring, models can fail silently for extended periods, causing
        cumulative harm to users and business outcomes.
      remediation: "Implement comprehensive performance monitoring immediately"

    - id: "PM-CRIT-002"
      signal: "Significant performance degradation undetected"
      evidence_indicators:
        - "Performance dropped below acceptable thresholds"
        - "Degradation discovered through user complaints"
        - "No alerts triggered"
      explanation: |
        Undetected degradation allows poor model predictions to continue affecting
        users until the problem becomes severe enough to be noticed.
      remediation: "Improve monitoring coverage and alert sensitivity"

  high:
    - id: "PM-HIGH-001"
      signal: "No baseline comparison for metrics"
      remediation: "Establish and track performance baselines"

    - id: "PM-HIGH-002"
      signal: "Ground truth labels not available for monitoring"
      remediation: "Implement label collection or use proxy metrics"

    - id: "PM-HIGH-003"
      signal: "Monitoring latency too high"
      remediation: "Reduce monitoring delay for faster detection"

  medium:
    - id: "PM-MED-001"
      signal: "Metrics not segmented"
      remediation: "Break down metrics by important segments"

    - id: "PM-MED-002"
      signal: "No performance trending"
      remediation: "Implement trend analysis and visualization"

    - id: "PM-MED-003"
      signal: "Monitoring dashboards incomplete"
      remediation: "Enhance dashboards with key metrics"

  low:
    - id: "PM-LOW-001"
      signal: "Metric definitions not documented"
      remediation: "Document all monitored metrics"

    - id: "PM-LOW-002"
      signal: "Historical metrics not retained"
      remediation: "Configure appropriate metric retention"

  positive:
    - id: "PM-POS-001"
      signal: "Comprehensive monitoring with quick detection"
    - id: "PM-POS-002"
      signal: "Segmented metrics with trend analysis"
    - id: "PM-POS-003"
      signal: "Well-designed dashboards for visibility"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Monitoring Coverage"
      description: "Assess which metrics are collected"
      duration_estimate: "1 hour"

    - id: "2"
      name: "Evaluate Detection Capability"
      description: "Test ability to detect degradation"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Check Baseline Tracking"
      description: "Review baseline establishment and comparison"
      duration_estimate: "45 minutes"

    - id: "4"
      name: "Assess Segmentation"
      description: "Verify metrics are broken down appropriately"
      duration_estimate: "30 minutes"

    - id: "5"
      name: "Review Dashboards"
      description: "Evaluate monitoring visualizations"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Check Ground Truth"
      description: "Assess label availability for metrics"
      duration_estimate: "45 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Monitoring Coverage"
        - "Detection Analysis"
        - "Dashboard Review"
        - "Recommendations"

closeout_checklist:
  - id: "pm-001"
    item: "Monitoring coverage assessed"
    level: "CRITICAL"
    verification: "automated"

  - id: "pm-002"
    item: "Detection capability tested"
    level: "CRITICAL"
    verification: "manual"

  - id: "pm-003"
    item: "Dashboards reviewed"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Post-Market Monitoring"
      description: "Requirements for AI system monitoring"

relationships:
  commonly_combined:
    - "machine-learning-ai.model-monitoring.drift-detection-monitoring"
    - "machine-learning-ai.model-monitoring.alerting-configuration"
  depends_on:
    - "machine-learning-ai.model-deployment.model-serving"
  feeds_into:
    - "machine-learning-ai.model-deployment.progressive-rollout"
