# ============================================================
# AUDIT: LLM Safety
# Category: 37 - Machine Learning & AI
# Subcategory: llm-operations
# ============================================================

audit:
  id: "machine-learning-ai.llm-operations.llm-safety"
  name: "LLM Safety Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "llm-operations"

  tier: "expert"
  estimated_duration: "5-7 hours"  # median: 6h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates LLM safety practices including content filtering, jailbreak prevention,
    prompt injection defense, toxicity detection, and harmful output prevention.
    Examines whether LLM systems are protected against misuse and harmful outputs.

  why_it_matters: |
    LLMs can be manipulated to produce harmful, illegal, or dangerous content through
    prompt injection and jailbreaking. Without proper safety measures, LLM applications
    can cause significant harm and create legal liability. Safety is essential for
    responsible LLM deployment.

  when_to_run:
    - "Before deploying LLM applications"
    - "After safety incidents"
    - "During security reviews"
    - "When regulations require"

prerequisites:
  required_artifacts:
    - type: "llm_system"
      description: "Access to LLM application"
    - type: "safety_filters"
      description: "Safety filtering implementations"
    - type: "red_team_data"
      description: "Red team test cases"

  access_requirements:
    - "Access to LLM application"
    - "Access to safety systems"
    - "Access to content filtering"
    - "Access to monitoring tools"

discovery:
  file_patterns:
    - glob: "**/safety/**"
      purpose: "Find safety implementations"
    - glob: "**/guardrails/**"
      purpose: "Find guardrails code"
    - glob: "**/filters/**"
      purpose: "Find content filters"
    - glob: "**/moderation/**"
      purpose: "Find moderation code"

knowledge_sources:
  guides:
    - id: "nemo-guardrails"
      name: "NeMo Guardrails"
      url: "https://github.com/NVIDIA/NeMo-Guardrails"
    - id: "llm-security"
      name: "LLM Security Guide"
      url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/"

  standards:
    - id: "llm-safety-best-practices"
      name: "LLM Safety Best Practices"
      relevance: "Safe LLM deployment"

tooling:
  analysis_tools:
    - tool: "nemo-guardrails"
      purpose: "LLM guardrails"
    - tool: "rebuff"
      purpose: "Prompt injection detection"
    - tool: "perspective"
      purpose: "Toxicity detection"
    - tool: "openai-moderation"
      purpose: "Content moderation"

signals:
  critical:
    - id: "LS-CRIT-001"
      signal: "LLM producing harmful content"
      evidence_indicators:
        - "Toxic or offensive outputs"
        - "Dangerous information provided"
        - "Illegal content generated"
      explanation: |
        Harmful LLM outputs can cause direct harm to users and society, create legal
        liability, and damage organizational reputation.
      remediation: "Implement comprehensive content filtering and guardrails"

    - id: "LS-CRIT-002"
      signal: "LLM vulnerable to prompt injection"
      evidence_indicators:
        - "Instructions overridden by user input"
        - "System prompts extracted"
        - "Jailbreaks successful"
      explanation: |
        Prompt injection allows attackers to bypass safety controls, access sensitive
        information, or make the LLM perform unintended actions.
      remediation: "Implement prompt injection defenses and input sanitization"

  high:
    - id: "LS-HIGH-001"
      signal: "No content filtering"
      remediation: "Implement input and output content filtering"

    - id: "LS-HIGH-002"
      signal: "No red teaming performed"
      remediation: "Conduct red team testing for adversarial inputs"

    - id: "LS-HIGH-003"
      signal: "No monitoring for harmful outputs"
      remediation: "Implement safety monitoring and alerting"

  medium:
    - id: "LS-MED-001"
      signal: "Safety filters not tuned"
      remediation: "Tune filters to balance safety and utility"

    - id: "LS-MED-002"
      signal: "No user reporting mechanism"
      remediation: "Allow users to report problematic outputs"

    - id: "LS-MED-003"
      signal: "Safety not tested systematically"
      remediation: "Add safety tests to evaluation"

  low:
    - id: "LS-LOW-001"
      signal: "Safety documentation incomplete"
      remediation: "Document safety measures and limitations"

    - id: "LS-LOW-002"
      signal: "Safety incidents not tracked"
      remediation: "Track and learn from safety incidents"

  positive:
    - id: "LS-POS-001"
      signal: "Multi-layer safety with guardrails"
    - id: "LS-POS-002"
      signal: "Regular red team testing"
    - id: "LS-POS-003"
      signal: "Comprehensive safety monitoring"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Safety Architecture"
      description: "Assess safety systems and guardrails"
      duration_estimate: "1 hour"

    - id: "2"
      name: "Test Prompt Injection"
      description: "Attempt prompt injection attacks"
      duration_estimate: "1.5 hours"

    - id: "3"
      name: "Test Content Filtering"
      description: "Evaluate content filter effectiveness"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Conduct Red Team Testing"
      description: "Test with adversarial inputs"
      duration_estimate: "1 hour"

    - id: "5"
      name: "Review Monitoring"
      description: "Assess safety monitoring"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Check Incident Response"
      description: "Review safety incident procedures"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Safety Architecture"
        - "Attack Testing Results"
        - "Content Filter Assessment"
        - "Recommendations"

closeout_checklist:
  - id: "ls-001"
    item: "Prompt injection tested"
    level: "CRITICAL"
    verification: "manual"

  - id: "ls-002"
    item: "Content filtering verified"
    level: "CRITICAL"
    verification: "automated"

  - id: "ls-003"
    item: "Red team testing completed"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["llm-systems", "ai-applications"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "AI Safety"
      description: "Requirements for safe AI systems"
    - framework: "OWASP LLM Top 10"
      control: "LLM Security"
      description: "LLM security requirements"

relationships:
  commonly_combined:
    - "machine-learning-ai.llm-operations.hallucination-detection"
    - "machine-learning-ai.responsible-ai.human-oversight"
  depends_on:
    - "machine-learning-ai.llm-operations.llm-integration"
  feeds_into:
    - "machine-learning-ai.model-deployment.model-serving"
