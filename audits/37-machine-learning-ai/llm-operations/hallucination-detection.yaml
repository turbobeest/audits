# ============================================================
# AUDIT: Hallucination Detection
# Category: 37 - Machine Learning & AI
# Subcategory: llm-operations
# ============================================================

audit:
  id: "machine-learning-ai.llm-operations.hallucination-detection"
  name: "Hallucination Detection Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "llm-operations"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates hallucination detection and mitigation practices including detection
    methods, confidence calibration, factual grounding, and post-processing verification.
    Examines whether LLM outputs are trustworthy and factually accurate.

  why_it_matters: |
    Hallucinations are a fundamental limitation of LLMs that can cause significant harm
    when users trust fabricated information. Detecting and mitigating hallucinations is
    essential for deploying LLMs safely in any application where factual accuracy matters.

  when_to_run:
    - "Before deploying LLM applications"
    - "When factual accuracy is critical"
    - "After hallucination incidents"
    - "During LLM system reviews"

prerequisites:
  required_artifacts:
    - type: "llm_system"
      description: "Access to LLM application"
    - type: "detection_tools"
      description: "Hallucination detection implementations"
    - type: "evaluation_data"
      description: "Data with ground truth for evaluation"

  access_requirements:
    - "Access to LLM outputs"
    - "Access to detection tools"
    - "Access to evaluation frameworks"
    - "Access to source documents"

discovery:
  file_patterns:
    - glob: "**/hallucination/**"
      purpose: "Find hallucination detection code"
    - glob: "**/fact_check/**"
      purpose: "Find fact checking code"
    - glob: "**/verification/**"
      purpose: "Find verification code"
    - glob: "**/grounding/**"
      purpose: "Find grounding implementations"

knowledge_sources:
  guides:
    - id: "hallucination-survey"
      name: "LLM Hallucination Survey"
      url: "https://arxiv.org/abs/2311.05232"
    - id: "selfcheckgpt"
      name: "SelfCheckGPT"
      url: "https://arxiv.org/abs/2303.08896"

  standards:
    - id: "hallucination-mitigation"
      name: "Hallucination Mitigation Best Practices"
      relevance: "Reducing LLM hallucinations"

tooling:
  analysis_tools:
    - tool: "selfcheckgpt"
      purpose: "Self-consistency hallucination detection"
    - tool: "factool"
      purpose: "Factuality verification"
    - tool: "langchain-fact-check"
      purpose: "Fact checking chains"
    - tool: "truera"
      purpose: "LLM quality monitoring"

signals:
  critical:
    - id: "HD-CRIT-001"
      signal: "No hallucination detection implemented"
      evidence_indicators:
        - "LLM outputs not verified"
        - "No factuality checks"
        - "Users trust unverified outputs"
      explanation: |
        Without hallucination detection, fabricated information reaches users,
        potentially causing significant harm especially in critical applications.
      remediation: "Implement hallucination detection and mitigation"

    - id: "HD-CRIT-002"
      signal: "High hallucination rate in production"
      evidence_indicators:
        - "Frequent factually incorrect outputs"
        - "User complaints about accuracy"
        - "Failed fact checks"
      explanation: |
        High hallucination rates undermine trust and can cause real harm when
        users act on incorrect information.
      remediation: "Improve grounding, add verification, or adjust use case"

  high:
    - id: "HD-HIGH-001"
      signal: "No confidence calibration"
      remediation: "Implement confidence estimation for outputs"

    - id: "HD-HIGH-002"
      signal: "No citation or attribution"
      remediation: "Add source citations to enable verification"

    - id: "HD-HIGH-003"
      signal: "Post-processing verification missing"
      remediation: "Add fact-checking post-processing"

  medium:
    - id: "HD-MED-001"
      signal: "Detection only at development time"
      remediation: "Add runtime hallucination monitoring"

    - id: "HD-MED-002"
      signal: "No user warning for uncertainty"
      remediation: "Communicate uncertainty to users"

    - id: "HD-MED-003"
      signal: "Single detection method only"
      remediation: "Use multiple detection approaches"

  low:
    - id: "HD-LOW-001"
      signal: "Hallucination metrics not tracked"
      remediation: "Track hallucination rates over time"

    - id: "HD-LOW-002"
      signal: "Detection documentation incomplete"
      remediation: "Document detection approach and limitations"

  positive:
    - id: "HD-POS-001"
      signal: "Multi-method hallucination detection"
    - id: "HD-POS-002"
      signal: "Calibrated confidence with appropriate uncertainty"
    - id: "HD-POS-003"
      signal: "Runtime monitoring with alerts"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Evaluate Detection Methods"
      description: "Assess hallucination detection implementation"
      duration_estimate: "1 hour"

    - id: "2"
      name: "Test Detection Accuracy"
      description: "Evaluate detection on known examples"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Check Confidence Calibration"
      description: "Assess confidence estimation"
      duration_estimate: "45 minutes"

    - id: "4"
      name: "Review Mitigation Strategies"
      description: "Evaluate approaches to reduce hallucination"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Assess Runtime Monitoring"
      description: "Check production hallucination monitoring"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Review User Communication"
      description: "Evaluate how uncertainty is communicated"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Detection Analysis"
        - "Mitigation Assessment"
        - "Monitoring Review"
        - "Recommendations"

closeout_checklist:
  - id: "hd-001"
    item: "Detection methods evaluated"
    level: "CRITICAL"
    verification: "automated"

  - id: "hd-002"
    item: "Calibration assessed"
    level: "HIGH"
    verification: "automated"

  - id: "hd-003"
    item: "Monitoring reviewed"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["llm-systems", "ai-applications", "knowledge-systems"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Accuracy"
      description: "Requirements for AI system accuracy"

relationships:
  commonly_combined:
    - "machine-learning-ai.llm-operations.rag-quality"
    - "machine-learning-ai.llm-operations.llm-evaluation"
  depends_on:
    - "machine-learning-ai.llm-operations.llm-integration"
  feeds_into:
    - "machine-learning-ai.llm-operations.llm-safety"
