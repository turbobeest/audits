# ============================================================
# AUDIT: LLM Evaluation
# Category: 37 - Machine Learning & AI
# Subcategory: llm-operations
# ============================================================

audit:
  id: "machine-learning-ai.llm-operations.llm-evaluation"
  name: "LLM Evaluation Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "llm-operations"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates LLM evaluation practices including benchmark selection, evaluation metrics,
    human evaluation processes, and automated evaluation methods. Examines whether LLM
    quality is measured effectively for the intended use case.

  why_it_matters: |
    LLM evaluation is challenging because quality is multidimensional and subjective.
    Poor evaluation leads to deploying suboptimal models or rejecting good ones.
    Effective evaluation enables informed decisions about model selection, prompting,
    and system design.

  when_to_run:
    - "When selecting LLMs"
    - "Before deploying LLM applications"
    - "When evaluating prompt changes"
    - "During LLM system reviews"

prerequisites:
  required_artifacts:
    - type: "evaluation_framework"
      description: "Access to evaluation infrastructure"
    - type: "benchmarks"
      description: "Evaluation benchmarks"
    - type: "llm_outputs"
      description: "LLM outputs for evaluation"

  access_requirements:
    - "Access to evaluation tools"
    - "Access to LLM systems"
    - "Access to benchmark data"
    - "Access to human evaluation if used"

discovery:
  file_patterns:
    - glob: "**/evaluation/**"
      purpose: "Find evaluation code"
    - glob: "**/benchmarks/**"
      purpose: "Find benchmark configs"
    - glob: "**/evals/**"
      purpose: "Find eval implementations"

knowledge_sources:
  guides:
    - id: "openai-evals"
      name: "OpenAI Evals"
      url: "https://github.com/openai/evals"
    - id: "lm-evaluation-harness"
      name: "LM Evaluation Harness"
      url: "https://github.com/EleutherAI/lm-evaluation-harness"
    - id: "helm"
      name: "HELM Benchmark"
      url: "https://crfm.stanford.edu/helm/"

  standards:
    - id: "llm-evaluation-best-practices"
      name: "LLM Evaluation Best Practices"
      relevance: "Effective LLM quality measurement"

tooling:
  analysis_tools:
    - tool: "openai-evals"
      purpose: "LLM evaluation framework"
    - tool: "lm-evaluation-harness"
      purpose: "Comprehensive LLM benchmarks"
    - tool: "helm"
      purpose: "Holistic evaluation"
    - tool: "langsmith"
      purpose: "LLM observability and evaluation"
    - tool: "promptfoo"
      purpose: "Prompt evaluation"

signals:
  critical:
    - id: "LE-CRIT-001"
      signal: "No systematic LLM evaluation"
      evidence_indicators:
        - "LLM selected without benchmarking"
        - "Quality assessed informally"
        - "No metrics for LLM performance"
      explanation: |
        Without systematic evaluation, LLM selection and optimization are based on
        intuition, leading to suboptimal systems and missed issues.
      remediation: "Implement systematic evaluation with appropriate benchmarks"

    - id: "LE-CRIT-002"
      signal: "Evaluation not relevant to use case"
      evidence_indicators:
        - "Generic benchmarks for specific use case"
        - "Metrics don't reflect actual requirements"
        - "Production behavior differs from benchmarks"
      explanation: |
        Irrelevant evaluation gives false confidence and fails to predict actual
        system performance for the intended application.
      remediation: "Create evaluation aligned with actual use case"

  high:
    - id: "LE-HIGH-001"
      signal: "No human evaluation"
      remediation: "Add human evaluation for subjective quality"

    - id: "LE-HIGH-002"
      signal: "Single evaluation metric only"
      remediation: "Use multiple metrics capturing different aspects"

    - id: "LE-HIGH-003"
      signal: "No regression testing"
      remediation: "Implement evaluation regression testing"

  medium:
    - id: "LE-MED-001"
      signal: "Evaluation not automated"
      remediation: "Automate evaluation in CI/CD"

    - id: "LE-MED-002"
      signal: "Evaluation set not diverse"
      remediation: "Expand evaluation to cover edge cases"

    - id: "LE-MED-003"
      signal: "No LLM-as-judge evaluation"
      remediation: "Consider LLM-based automated evaluation"

  low:
    - id: "LE-LOW-001"
      signal: "Evaluation results not tracked"
      remediation: "Track evaluation results over time"

    - id: "LE-LOW-002"
      signal: "Evaluation documentation incomplete"
      remediation: "Document evaluation methodology"

  positive:
    - id: "LE-POS-001"
      signal: "Comprehensive evaluation aligned with use case"
    - id: "LE-POS-002"
      signal: "Combination of automated and human evaluation"
    - id: "LE-POS-003"
      signal: "Automated regression testing"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Evaluation Framework"
      description: "Assess evaluation infrastructure"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Assess Metric Selection"
      description: "Evaluate appropriateness of metrics"
      duration_estimate: "45 minutes"

    - id: "3"
      name: "Check Benchmark Quality"
      description: "Review evaluation datasets and benchmarks"
      duration_estimate: "45 minutes"

    - id: "4"
      name: "Review Human Evaluation"
      description: "Assess human evaluation processes"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Check Automation"
      description: "Review evaluation automation"
      duration_estimate: "30 minutes"

    - id: "6"
      name: "Assess Use Case Alignment"
      description: "Verify evaluation matches actual use"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Framework Assessment"
        - "Metric Analysis"
        - "Use Case Alignment"
        - "Recommendations"

closeout_checklist:
  - id: "le-001"
    item: "Evaluation framework assessed"
    level: "CRITICAL"
    verification: "manual"

  - id: "le-002"
    item: "Use case alignment verified"
    level: "HIGH"
    verification: "manual"

  - id: "le-003"
    item: "Automation reviewed"
    level: "MEDIUM"
    verification: "automated"

governance:
  applicable_to:
    archetypes: ["llm-systems", "ai-applications"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Testing"
      description: "AI system testing requirements"

relationships:
  commonly_combined:
    - "machine-learning-ai.llm-operations.prompt-management"
    - "machine-learning-ai.llm-operations.hallucination-detection"
  depends_on:
    - "machine-learning-ai.llm-operations.llm-integration"
  feeds_into:
    - "machine-learning-ai.llm-operations.llm-safety"
