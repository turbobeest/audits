audit:
  id: machine-learning-ai.llm-operations.llm-cost-optimization
  name: LLM Cost Optimization Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: llm-operations
  tier: expert
  estimated_duration: 3-5 hours  # median: 4h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: medium
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates LLM cost management practices including token optimization, model
    selection, caching strategies, prompt efficiency, and cost monitoring. Examines
    whether LLM costs are controlled while maintaining quality.
  why_it_matters: |
    LLM API costs can grow rapidly with scale. Without cost optimization, LLM
    applications become economically unsustainable. Effective cost management enables
    sustainable LLM operations while maintaining quality.
  when_to_run:
  - When scaling LLM applications
  - After cost overruns
  - During budget planning
  - Periodically for optimization
prerequisites:
  required_artifacts:
  - type: cost_data
    description: LLM cost information
  - type: usage_metrics
    description: Token usage data
  - type: llm_configs
    description: Model configurations
  access_requirements:
  - Access to billing data
  - Access to usage metrics
  - Access to model configurations
  - Access to prompt implementations
discovery:
  file_patterns:
  - glob: '**/llm/**'
    purpose: Find LLM code
  - glob: '**/prompts/**'
    purpose: Find prompt files
  - glob: '**/cache/**'
    purpose: Find caching code
knowledge_sources:
  guides:
  - id: llm-cost-guide
    name: LLM Cost Management Guide
    url: https://platform.openai.com/docs/guides/production-best-practices
  - id: prompt-optimization
    name: Prompt Optimization
    url: https://platform.openai.com/docs/guides/prompt-engineering
  standards:
  - id: llm-cost-best-practices
    name: LLM Cost Best Practices
    relevance: Efficient LLM usage
tooling:
  analysis_tools:
  - tool: helicone
    purpose: LLM cost tracking
  - tool: langsmith
    purpose: LLM observability
  - tool: semantic-cache
    purpose: Response caching
  - tool: litellm
    purpose: Model routing
signals:
  critical:
  - id: LCO-CRIT-001
    signal: LLM costs significantly exceeding budget
    evidence_indicators:
    - Unexpected cost spikes
    - Budget overruns
    - No cost controls
    explanation: |
      Uncontrolled LLM costs can quickly become unsustainable, threatening
      project viability and organizational budgets.
    remediation: Implement cost controls and optimization strategies
  - id: LCO-CRIT-002
    signal: No cost visibility
    evidence_indicators:
    - Cannot track costs by feature
    - No usage attribution
    - Costs discovered after the fact
    explanation: |
      Without cost visibility, optimization is impossible and cost issues
      go undetected until they become severe.
    remediation: Implement comprehensive cost tracking
  high:
  - id: LCO-HIGH-001
    signal: Using expensive models for simple tasks
    remediation: Route simple tasks to smaller, cheaper models
  - id: LCO-HIGH-002
    signal: No response caching
    remediation: Implement semantic caching
  - id: LCO-HIGH-003
    signal: Prompts not optimized for tokens
    remediation: Optimize prompts for efficiency
  medium:
  - id: LCO-MED-001
    signal: No cost alerts
    remediation: Set up cost alerting thresholds
  - id: LCO-MED-002
    signal: Not using batching
    remediation: Batch requests where possible
  - id: LCO-MED-003
    signal: Output tokens not controlled
    remediation: Limit output tokens appropriately
  low:
  - id: LCO-LOW-001
    signal: Cost benchmarks not established
    remediation: Establish per-request cost benchmarks
  - id: LCO-LOW-002
    signal: Cost trends not analyzed
    remediation: Analyze cost trends over time
  positive:
  - id: LCO-POS-001
    signal: Comprehensive cost tracking by feature
  - id: LCO-POS-002
    signal: Effective caching reducing costs
  - id: LCO-POS-003
    signal: Model routing for cost efficiency
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Analyze Cost Data
    description: Review LLM cost breakdown
    duration_estimate: 45 minutes
  - id: '2'
    name: Assess Model Selection
    description: Review model choices for tasks
    duration_estimate: 45 minutes
  - id: '3'
    name: Evaluate Caching
    description: Review caching implementation
    duration_estimate: 30 minutes
  - id: '4'
    name: Check Prompt Efficiency
    description: Assess prompt token usage
    duration_estimate: 30 minutes
  - id: '5'
    name: Review Monitoring
    description: Evaluate cost monitoring
    duration_estimate: 30 minutes
  - id: '6'
    name: Identify Optimizations
    description: Find cost reduction opportunities
    duration_estimate: 30 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Cost Analysis
    - Optimization Opportunities
    - Monitoring Assessment
    - Recommendations
closeout_checklist:
- id: lco-001
  item: Cost tracking verified
  level: CRITICAL
  verification: automated
- id: lco-002
  item: Model selection reviewed
  level: HIGH
  verification: manual
- id: lco-003
  item: Caching assessed
  level: MEDIUM
  verification: automated
governance:
  applicable_to:
    archetypes:
    - llm-systems
    - ai-applications
  compliance_mappings:
  - framework: FinOps
    control: Cost Management
    description: Cloud and API cost optimization
relationships:
  commonly_combined:
  - machine-learning-ai.llm-operations.llm-integration
  - machine-learning-ai.llm-operations.prompt-management
  depends_on:
  - machine-learning-ai.llm-operations.llm-integration
  feeds_into:
  - machine-learning-ai.mlops-infrastructure.compute-resource-management
