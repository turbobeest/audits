# ============================================================
# AUDIT: RAG Quality
# Category: 37 - Machine Learning & AI
# Subcategory: llm-operations
# ============================================================

audit:
  id: "machine-learning-ai.llm-operations.rag-quality"
  name: "RAG Quality Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "llm-operations"

  tier: "expert"
  estimated_duration: "5-7 hours"  # median: 6h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates Retrieval-Augmented Generation (RAG) system quality including retrieval
    accuracy, chunking strategies, embedding quality, context relevance, and generation
    faithfulness. Examines whether RAG systems provide accurate, grounded responses.

  why_it_matters: |
    RAG systems are only as good as their retrieval and grounding. Poor RAG quality
    leads to irrelevant responses, hallucinations, and failure to leverage knowledge
    bases effectively. Quality RAG enables trustworthy AI assistants and knowledge systems.

  when_to_run:
    - "When building RAG systems"
    - "After RAG quality complaints"
    - "When updating knowledge bases"
    - "During RAG system reviews"

prerequisites:
  required_artifacts:
    - type: "rag_system"
      description: "Access to RAG implementation"
    - type: "vector_store"
      description: "Access to vector database"
    - type: "evaluation_data"
      description: "RAG evaluation dataset"

  access_requirements:
    - "Access to RAG pipeline code"
    - "Access to vector store"
    - "Access to evaluation tools"
    - "Access to knowledge base"

discovery:
  file_patterns:
    - glob: "**/rag/**"
      purpose: "Find RAG implementation"
    - glob: "**/retrieval/**"
      purpose: "Find retrieval code"
    - glob: "**/embeddings/**"
      purpose: "Find embedding code"
    - glob: "**/vector_store/**"
      purpose: "Find vector store configs"

knowledge_sources:
  guides:
    - id: "langchain-rag"
      name: "LangChain RAG Guide"
      url: "https://python.langchain.com/docs/use_cases/question_answering/"
    - id: "rag-evaluation"
      name: "RAG Evaluation Guide"
      url: "https://docs.ragas.io/"

  standards:
    - id: "rag-best-practices"
      name: "RAG Best Practices"
      relevance: "Effective RAG implementation"

tooling:
  analysis_tools:
    - tool: "ragas"
      purpose: "RAG evaluation framework"
    - tool: "langchain"
      purpose: "RAG implementation"
    - tool: "llamaindex"
      purpose: "RAG framework"
    - tool: "chroma"
      purpose: "Vector database"
    - tool: "pinecone"
      purpose: "Vector database"

signals:
  critical:
    - id: "RAG-CRIT-001"
      signal: "RAG retrieving irrelevant context"
      evidence_indicators:
        - "Low retrieval precision"
        - "Wrong documents returned"
        - "Context not matching queries"
      explanation: |
        Irrelevant retrieval causes the LLM to generate responses based on wrong
        information, leading to incorrect and misleading outputs.
      remediation: "Improve retrieval with better embeddings, chunking, or reranking"

    - id: "RAG-CRIT-002"
      signal: "RAG hallucinating despite context"
      evidence_indicators:
        - "Generated content not in retrieved context"
        - "Fabricated information in responses"
        - "Low faithfulness scores"
      explanation: |
        Hallucination defeats the purpose of RAG, producing ungrounded responses
        that users may incorrectly trust.
      remediation: "Improve grounding through prompting, fine-tuning, or post-processing"

  high:
    - id: "RAG-HIGH-001"
      signal: "Poor chunking strategy"
      remediation: "Optimize chunking for semantic coherence"

    - id: "RAG-HIGH-002"
      signal: "No retrieval quality monitoring"
      remediation: "Implement retrieval metrics monitoring"

    - id: "RAG-HIGH-003"
      signal: "Stale knowledge base"
      remediation: "Implement knowledge base update pipeline"

  medium:
    - id: "RAG-MED-001"
      signal: "No reranking implemented"
      remediation: "Add reranking for retrieval quality"

    - id: "RAG-MED-002"
      signal: "Context window not optimized"
      remediation: "Tune context selection strategy"

    - id: "RAG-MED-003"
      signal: "No RAG evaluation framework"
      remediation: "Implement systematic RAG evaluation"

  low:
    - id: "RAG-LOW-001"
      signal: "No source attribution"
      remediation: "Add source citations to responses"

    - id: "RAG-LOW-002"
      signal: "Embedding model not optimized"
      remediation: "Evaluate and select optimal embedding model"

  positive:
    - id: "RAG-POS-001"
      signal: "High retrieval precision and recall"
    - id: "RAG-POS-002"
      signal: "Faithful responses grounded in context"
    - id: "RAG-POS-003"
      signal: "Comprehensive RAG monitoring"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Evaluate Retrieval Quality"
      description: "Measure retrieval precision and recall"
      duration_estimate: "1.5 hours"

    - id: "2"
      name: "Assess Chunking Strategy"
      description: "Review document chunking approach"
      duration_estimate: "45 minutes"

    - id: "3"
      name: "Check Generation Faithfulness"
      description: "Evaluate grounding of generated responses"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Review Embedding Quality"
      description: "Assess embedding model performance"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Evaluate End-to-End Quality"
      description: "Test full RAG pipeline"
      duration_estimate: "45 minutes"

    - id: "6"
      name: "Check Knowledge Freshness"
      description: "Review knowledge base update process"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Retrieval Analysis"
        - "Generation Quality"
        - "Pipeline Assessment"
        - "Recommendations"

closeout_checklist:
  - id: "rag-001"
    item: "Retrieval quality measured"
    level: "CRITICAL"
    verification: "automated"

  - id: "rag-002"
    item: "Faithfulness assessed"
    level: "CRITICAL"
    verification: "automated"

  - id: "rag-003"
    item: "Chunking reviewed"
    level: "HIGH"
    verification: "manual"

governance:
  applicable_to:
    archetypes: ["llm-systems", "ai-applications", "knowledge-systems"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Accuracy"
      description: "Requirements for AI system accuracy"

relationships:
  commonly_combined:
    - "machine-learning-ai.llm-operations.llm-evaluation"
    - "machine-learning-ai.llm-operations.hallucination-detection"
  depends_on:
    - "machine-learning-ai.llm-operations.llm-integration"
  feeds_into:
    - "machine-learning-ai.llm-operations.llm-evaluation"
