# ============================================================
# AUDIT: Model Optimization
# Category: 37 - Machine Learning & AI
# Subcategory: model-deployment
# ============================================================

audit:
  id: "machine-learning-ai.model-deployment.model-optimization"
  name: "Model Optimization Audit"
  version: "1.0.0"
  last_updated: "2025-01-22"
  status: "active"

  category: "machine-learning-ai"
  category_number: 37
  subcategory: "model-deployment"

  tier: "expert"
  estimated_duration: "4-6 hours"  # median: 5h

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "medium"
  scope: "ml-systems"

  default_profiles:
    - "full"
    - "quick"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Evaluates model optimization techniques including quantization, pruning, knowledge
    distillation, and compilation for efficient inference. Examines tradeoffs between
    model size, latency, and accuracy to ensure optimizations meet deployment requirements.

  why_it_matters: |
    Unoptimized models consume excessive compute resources, have high latency, and
    may be too large for edge deployment. Proper optimization reduces costs, improves
    user experience, and enables deployment to resource-constrained environments
    while maintaining acceptable accuracy.

  when_to_run:
    - "Before production deployment"
    - "When latency requirements not met"
    - "During cost optimization efforts"
    - "For edge deployment preparation"

prerequisites:
  required_artifacts:
    - type: "model_artifacts"
      description: "Access to trained models"
    - type: "optimization_code"
      description: "Optimization implementations"
    - type: "performance_benchmarks"
      description: "Latency and accuracy measurements"

  access_requirements:
    - "Access to model artifacts"
    - "Access to optimization tools"
    - "Access to benchmarking infrastructure"
    - "Access to deployment targets"

discovery:
  file_patterns:
    - glob: "**/optimization/**/*.py"
      purpose: "Find optimization code"
    - glob: "**/quantization/**"
      purpose: "Find quantization code"
    - glob: "**/distillation/**"
      purpose: "Find distillation code"
    - glob: "**/onnx/**"
      purpose: "Find ONNX exports"

knowledge_sources:
  guides:
    - id: "pytorch-quant"
      name: "PyTorch Quantization"
      url: "https://pytorch.org/docs/stable/quantization.html"
    - id: "tensorrt-docs"
      name: "TensorRT Documentation"
      url: "https://docs.nvidia.com/deeplearning/tensorrt/"
    - id: "onnx-runtime"
      name: "ONNX Runtime Documentation"
      url: "https://onnxruntime.ai/docs/"

  standards:
    - id: "model-optimization-best-practices"
      name: "Model Optimization Best Practices"
      relevance: "Efficient inference deployment"

tooling:
  analysis_tools:
    - tool: "tensorrt"
      purpose: "GPU inference optimization"
    - tool: "onnx-runtime"
      purpose: "Cross-platform inference"
    - tool: "pytorch-quantization"
      purpose: "Model quantization"
    - tool: "neural-compressor"
      purpose: "Intel optimization toolkit"
    - tool: "tensorflow-lite"
      purpose: "Mobile and edge optimization"

signals:
  critical:
    - id: "MO-CRIT-001"
      signal: "Optimization causes significant accuracy degradation"
      evidence_indicators:
        - "Accuracy drops more than acceptable threshold"
        - "Edge cases fail after optimization"
        - "No validation of optimized model"
      explanation: |
        Aggressive optimization can degrade model accuracy below acceptable levels,
        leading to poor predictions that harm users.
      remediation: "Validate optimized models thoroughly and tune optimization parameters"

    - id: "MO-CRIT-002"
      signal: "Optimized model produces different outputs"
      evidence_indicators:
        - "Numerical differences beyond tolerance"
        - "Edge case behavior changed"
        - "No equivalence testing"
      explanation: |
        Optimized models should be functionally equivalent to originals. Unexpected
        differences indicate bugs that can cause production issues.
      remediation: "Implement thorough equivalence testing for optimizations"

  high:
    - id: "MO-HIGH-001"
      signal: "No optimization applied for latency-sensitive use"
      remediation: "Apply appropriate optimization techniques"

    - id: "MO-HIGH-002"
      signal: "Optimization not validated on representative data"
      remediation: "Test optimization on production-like data"

    - id: "MO-HIGH-003"
      signal: "Memory footprint exceeds deployment constraints"
      remediation: "Apply model compression techniques"

  medium:
    - id: "MO-MED-001"
      signal: "Optimization techniques not explored"
      remediation: "Evaluate multiple optimization approaches"

    - id: "MO-MED-002"
      signal: "Accuracy-latency tradeoff not documented"
      remediation: "Document optimization tradeoffs"

    - id: "MO-MED-003"
      signal: "Optimization not reproducible"
      remediation: "Version optimization configurations"

  low:
    - id: "MO-LOW-001"
      signal: "Optimization benchmarks incomplete"
      remediation: "Complete benchmarking across targets"

    - id: "MO-LOW-002"
      signal: "Optimization documentation missing"
      remediation: "Document optimization procedures"

  positive:
    - id: "MO-POS-001"
      signal: "Validated optimization meeting requirements"
    - id: "MO-POS-002"
      signal: "Well-documented accuracy-latency tradeoffs"
    - id: "MO-POS-003"
      signal: "Comprehensive equivalence testing"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Review Current State"
      description: "Assess baseline model performance"
      duration_estimate: "45 minutes"

    - id: "2"
      name: "Evaluate Optimization Techniques"
      description: "Review applied optimization methods"
      duration_estimate: "1 hour"

    - id: "3"
      name: "Validate Accuracy"
      description: "Test optimized model accuracy"
      duration_estimate: "1 hour"

    - id: "4"
      name: "Benchmark Performance"
      description: "Measure latency and throughput improvements"
      duration_estimate: "45 minutes"

    - id: "5"
      name: "Test Equivalence"
      description: "Verify optimized model equivalence"
      duration_estimate: "45 minutes"

    - id: "6"
      name: "Assess Tradeoffs"
      description: "Evaluate accuracy-latency tradeoffs"
      duration_estimate: "30 minutes"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Optimization Analysis"
        - "Performance Benchmarks"
        - "Accuracy Validation"
        - "Recommendations"

closeout_checklist:
  - id: "mo-001"
    item: "Accuracy validated"
    level: "CRITICAL"
    verification: "automated"

  - id: "mo-002"
    item: "Performance benchmarked"
    level: "HIGH"
    verification: "automated"

  - id: "mo-003"
    item: "Equivalence tested"
    level: "HIGH"
    verification: "automated"

governance:
  applicable_to:
    archetypes: ["ml-systems", "ai-applications", "edge-systems"]

  compliance_mappings:
    - framework: "EU AI Act"
      control: "Accuracy"
      description: "Requirements for AI system accuracy maintenance"

relationships:
  commonly_combined:
    - "machine-learning-ai.model-deployment.model-serving"
    - "machine-learning-ai.model-validation.model-testing"
  depends_on:
    - "machine-learning-ai.model-development.model-architecture-design"
  feeds_into:
    - "machine-learning-ai.model-deployment.model-serving"
