audit:
  id: machine-learning-ai.model-deployment.ab-testing
  name: A/B Testing for ML Models Audit
  version: 1.0.0
  last_updated: '2025-01-22'
  status: active
  category: machine-learning-ai
  category_number: 37
  subcategory: model-deployment
  tier: expert
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: ml-systems
  default_profiles:
  - full
  - quick
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates A/B testing practices for ML models including experiment design,
    traffic splitting, metric selection, statistical analysis, and decision-making
    processes. Examines whether model comparisons in production are rigorous and
    lead to valid conclusions.
  why_it_matters: |
    A/B testing is the gold standard for evaluating model improvements in production.
    Poor A/B testing leads to wrong decisions about model performance, potentially
    deploying worse models or rejecting improvements. Valid experiments ensure
    model changes actually improve business outcomes.
  when_to_run:
  - Before launching A/B tests
  - When establishing experimentation practices
  - After inconclusive or questionable results
  - During experiment platform reviews
prerequisites:
  required_artifacts:
  - type: experiment_platform
    description: Access to A/B testing infrastructure
  - type: experiment_configs
    description: A/B test configurations
  - type: analysis_results
    description: Historical experiment results
  access_requirements:
  - Access to experimentation platform
  - Access to experiment configurations
  - Access to results and analysis
  - Access to traffic routing systems
discovery:
  file_patterns:
  - glob: '**/experiments/**/*.py'
    purpose: Find experiment code
  - glob: '**/ab_test/**'
    purpose: Find A/B test configurations
  - glob: '**/traffic_splitting/**'
    purpose: Find routing logic
knowledge_sources:
  guides:
  - id: ab-testing-guide
    name: A/B Testing for ML Guide
    url: https://exp-platform.com/Documents/
  - id: causal-inference
    name: Causal Inference in A/B Testing
    url: https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/
  standards:
  - id: ab-testing-best-practices
    name: A/B Testing Best Practices
    relevance: Statistical rigor in experimentation
tooling:
  analysis_tools:
  - tool: statsmodels
    purpose: Statistical analysis
  - tool: scipy.stats
    purpose: Statistical tests
  - tool: pyab
    purpose: A/B testing library
  - tool: eppo
    purpose: Feature flagging and experiments
signals:
  critical:
  - id: AB-CRIT-001
    signal: A/B test results invalid due to bias
    evidence_indicators:
    - Non-random assignment to variants
    - Selection bias in test population
    - Metric computation errors
    explanation: |
      Biased experiments lead to wrong conclusions, potentially deploying models that
      harm users or rejecting beneficial improvements.
    remediation: Audit randomization and implement proper experiment controls
  - id: AB-CRIT-002
    signal: Decisions made before statistical significance
    evidence_indicators:
    - Tests stopped early without adjustment
    - Multiple peeking without correction
    - Underpowered experiments
    explanation: |
      Premature decisions based on incomplete data lead to high false positive rates
      and unreliable model selection.
    remediation: Implement sequential testing or fixed-horizon analysis
  high:
  - id: AB-HIGH-001
    signal: No sample size calculation
    remediation: Perform power analysis before experiments
  - id: AB-HIGH-002
    signal: Wrong metrics for A/B tests
    remediation: Align A/B test metrics with business objectives
  - id: AB-HIGH-003
    signal: No guardrail metrics
    remediation: Implement guardrail metrics to catch regressions
  medium:
  - id: AB-MED-001
    signal: No novelty effect accounting
    remediation: Run tests long enough to account for novelty
  - id: AB-MED-002
    signal: Segment analysis not performed
    remediation: Analyze experiment results by user segments
  - id: AB-MED-003
    signal: No documentation of experiment rationale
    remediation: Document hypothesis and expected effects
  low:
  - id: AB-LOW-001
    signal: Experiment naming inconsistent
    remediation: Establish experiment naming conventions
  - id: AB-LOW-002
    signal: Historical experiments not archived
    remediation: Archive experiment configurations and results
  positive:
  - id: AB-POS-001
    signal: Rigorous statistical analysis with proper controls
  - id: AB-POS-002
    signal: Guardrail metrics protecting against regressions
  - id: AB-POS-003
    signal: Well-documented experiment rationale and results
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Review Experiment Design
    description: Assess experiment setup and randomization
    duration_estimate: 1 hour
  - id: '2'
    name: Validate Randomization
    description: Verify proper random assignment to variants
    duration_estimate: 45 minutes
  - id: '3'
    name: Check Statistical Analysis
    description: Review analysis methodology and rigor
    duration_estimate: 1 hour
  - id: '4'
    name: Assess Metric Selection
    description: Evaluate primary and guardrail metrics
    duration_estimate: 45 minutes
  - id: '5'
    name: Review Decision Process
    description: Examine how experiment results inform decisions
    duration_estimate: 30 minutes
  - id: '6'
    name: Analyze Historical Experiments
    description: Review past experiments for patterns
    duration_estimate: 45 minutes
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Experiment Design Analysis
    - Statistical Rigor Assessment
    - Decision Process Review
    - Recommendations
closeout_checklist:
- id: ab-001
  item: Randomization validated
  level: CRITICAL
  verification: automated
- id: ab-002
  item: Statistical methods reviewed
  level: HIGH
  verification: manual
- id: ab-003
  item: Decision process assessed
  level: MEDIUM
  verification: manual
governance:
  applicable_to:
    archetypes:
    - ml-systems
    - ai-applications
    - product-systems
  compliance_mappings:
  - framework: EU AI Act
    control: Testing
    description: Requirements for AI system validation
relationships:
  commonly_combined:
  - machine-learning-ai.model-deployment.model-serving
  - machine-learning-ai.model-monitoring.performance-monitoring
  depends_on:
  - machine-learning-ai.model-deployment.model-versioning
  feeds_into:
  - machine-learning-ai.model-deployment.progressive-rollout
