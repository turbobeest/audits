# ============================================================
# AUDIT: Harassment Prevention
# Category: 21 - Ethical & Societal
# Subcategory: Content Harm
# ============================================================

audit:
  id: "ethical-societal.content-harm.harassment-prevention"
  name: "Harassment Prevention Audit"
  version: "1.0.0"
  last_updated: "2025-01-19"
  status: "active"

  category: "ethical-societal"
  category_number: 21
  subcategory: "content-harm"

  tier: "expert"
  estimated_duration: "4-6 hours"

  completeness: "requires_discovery"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "critical"
  scope: "qualitative"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: false

description:
  what: |
    Evaluates platform systems for preventing and responding to harassment,
    bullying, and targeted abuse. Assesses detection capabilities, reporting
    mechanisms, response times, victim support, and consequences for
    perpetrators. Reviews both technical tools and human moderation.

  why_it_matters: |
    Harassment prevention is essential for platform safety:

    - Harassment drives users from platforms, especially marginalized groups
    - Can escalate to real-world harm and violence
    - EU DSA requires effective harassment response
    - Legal liability for failure to address known harassment
    - Platforms enabling harassment face reputational damage
    - Mental health impacts on harassment victims are severe

  when_to_run:
    - "For platforms with user-to-user interaction"
    - "During EU DSA compliance"
    - "After harassment incidents or campaigns"
    - "Annually for ongoing assessment"

prerequisites:
  required_artifacts:
    - type: "harassment_policies"
      description: "Anti-harassment policies and definitions"
    - type: "moderation_systems"
      description: "Documentation of harassment response systems"

  access_requirements:
    - "Policy documentation"
    - "Reporting system access"
    - "Interview access to trust & safety"

discovery:
  interviews:
    - role: "Trust & Safety Lead"
      questions:
        - "How do you define and categorize harassment?"
        - "What tools detect harassment behavior?"
        - "How quickly are harassment reports processed?"
        - "What protections exist for harassment victims?"
        - "How do you handle coordinated harassment campaigns?"
      purpose: "Understand harassment prevention systems"
      duration: "45 min"

    - role: "Community/Support Lead"
      questions:
        - "What support is provided to harassment victims?"
        - "How do users report harassment?"
        - "What feedback do victims receive?"
        - "What are the biggest gaps in protection?"
      purpose: "Understand victim experience"
      duration: "30 min"

  documents_to_review:
    - type: "Anti-Harassment Policy"
      purpose: "Review policy clarity and coverage"
      look_for:
        - "Clear harassment definitions"
        - "Categories of harassment"
        - "Enforcement actions"
        - "Appeal process"

    - type: "Response Metrics"
      purpose: "Assess response effectiveness"
      look_for:
        - "Report processing times"
        - "Action rates"
        - "Repeat offense rates"
        - "Victim satisfaction"

signals:
  critical:
    - id: "HARASS-CRIT-001"
      signal: "No harassment reporting or response system"
      evidence_indicators:
        - "No way for users to report harassment"
        - "Reports ignored or not processed"
        - "No consequences for harassers"
        - "Victims have no recourse"
      explanation: |
        Platforms without harassment response enable ongoing harm and face
        legal liability under regulations like EU DSA.
      remediation: |
        - Implement harassment reporting system
        - Create response workflow with SLAs
        - Define and enforce consequences
        - Provide victim support

    - id: "HARASS-CRIT-002"
      signal: "Coordinated harassment campaigns tolerated"
      evidence_indicators:
        - "Brigading not detected or addressed"
        - "Pile-on attacks continue unchecked"
        - "Doxxing not rapidly removed"
        - "Coordinated abuse not recognized"
      explanation: |
        Coordinated harassment causes severe harm and often targets
        marginalized individuals. Failure to address it enables campaigns.
      remediation: |
        - Implement coordinated harassment detection
        - Create rapid response for campaigns
        - Protect targets during active campaigns
        - Address participant accounts

  high:
    - id: "HARASS-HIGH-001"
      signal: "Slow response to harassment reports"
      evidence_indicators:
        - "Reports taking days to process"
        - "Harassment continues during review"
        - "No prioritization for severe harassment"
        - "Victims not protected while pending"
      explanation: |
        Slow response allows harassment to continue and escalate,
        causing extended harm to victims.
      remediation: |
        - Implement SLA-based processing
        - Prioritize severe harassment
        - Provide interim protection for victims
        - Scale capacity to meet demand

    - id: "HARASS-HIGH-002"
      signal: "Victim protection tools inadequate"
      evidence_indicators:
        - "Blocking doesn't prevent harassment"
        - "Harassers can see victim content"
        - "No protection from new accounts"
        - "Victims must see harassing content to report"
      explanation: |
        When victim tools are inadequate, harassment victims cannot
        protect themselves effectively.
      remediation: |
        - Improve blocking effectiveness
        - Add mute and filter options
        - Address block evasion
        - Allow third-party reporting

    - id: "HARASS-HIGH-003"
      signal: "No consequences for repeat harassers"
      evidence_indicators:
        - "Same users repeatedly reported"
        - "Warnings without escalation"
        - "Easy return after suspension"
        - "No account escalation system"
      explanation: |
        Without meaningful consequences, harassers continue behavior
        with impunity.
      remediation: |
        - Implement escalating consequences
        - Track repeat behavior
        - Make suspensions meaningful
        - Consider permanent removal for patterns

  medium:
    - id: "HARASS-MED-001"
      signal: "Harassment definitions unclear"
      evidence_indicators:
        - "Users unsure what constitutes harassment"
        - "Inconsistent enforcement"
        - "Context-dependent without guidance"
      remediation: |
        - Clarify harassment definitions
        - Provide examples and guidance
        - Train moderators on edge cases
        - Ensure consistent enforcement

  positive:
    - id: "HARASS-POS-001"
      signal: "Comprehensive harassment prevention"
      evidence_indicators:
        - "Clear policies consistently enforced"
        - "Rapid response to reports"
        - "Effective victim protection tools"
        - "Meaningful consequences for harassers"
        - "Coordinated harassment addressed"

procedure:
  context:
    cognitive_mode: "evaluative"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Policy Review"
      description: Review harassment policies for clarity and coverage.
      duration_estimate: "1 hour"
      expected_findings:
        - "Policy assessment"
        - "Definition clarity"

    - id: "2"
      name: "Response System Assessment"
      description: Evaluate harassment response effectiveness.
      duration_estimate: "1.5 hours"
      expected_findings:
        - "Response times"
        - "Action rates"

    - id: "3"
      name: "Victim Protection Review"
      description: Assess tools available to harassment victims.
      duration_estimate: "1 hour"
      expected_findings:
        - "Tool effectiveness"
        - "Protection gaps"

    - id: "4"
      name: "Stakeholder Interviews"
      description: Interview trust & safety and community teams.
      duration_estimate: "1.5 hours"
      expected_findings:
        - "Process understanding"
        - "Challenge identification"

    - id: "5"
      name: "Synthesize Findings"
      description: Compile findings and recommendations.
      duration_estimate: "30 min"
      expected_findings:
        - "Prioritized findings"
        - "Recommendations"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"
    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Policy Assessment"
        - "Response Effectiveness"
        - "Victim Protection"
        - "Recommendations"

  confidence_guidance:
    high: "Policies documented, metrics available, interviews completed"
    medium: "Partial access or metrics"
    low: "External assessment only"

offline:
  capability: "partial"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires comprehensive analysis"
    full:
      included: true
      priority: 90

closeout_checklist:
  - id: "harass-001"
    item: "Harassment policies reviewed"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Policy clarity and coverage assessed"
    expected: "Confirmed by reviewer"

  - id: "harass-002"
    item: "Response effectiveness assessed"
    level: "CRITICAL"
    verification: "manual"
    verification_notes: "Response times and action rates evaluated"
    expected: "Confirmed by reviewer"

  - id: "harass-003"
    item: "Victim protection tools reviewed"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Protection tools effectiveness evaluated"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["social", "content", "platform"]
    team_sizes: ["medium", "large"]

  compliance_frameworks:
    - framework: "EU DSA"
      controls: ["Article 14", "Article 16", "Article 20"]

relationships:
  commonly_combined:
    - "ethical-societal.content-harm.harmful-content-moderation"
    - "ethical-societal.content-harm.child-safety"
