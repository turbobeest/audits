audit:
  id: responsible-design.consent-transparency.algorithm-explainability
  name: Algorithm Explainability Audit
  version: 1.0.0
  last_updated: '2025-01-19'
  status: active
  category: ethical-societal
  category_number: 21
  subcategory: consent-transparency
  tier: expert
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: qualitative
  default_profiles:
  - full
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates whether algorithmic decisions affecting users can be
    meaningfully explained. Assesses whether users can understand why
    an algorithm made a particular decision about them, what factors
    influenced it, and how they might get a different outcome.

    Examines explanation mechanisms, meaningful information requirements,
    and the gap between technical explainability and user understanding.
  why_it_matters: |
    Algorithm explainability is increasingly required and ethically important:

    - GDPR Article 22 requires meaningful information about automated decisions
    - EU AI Act requires transparency for high-risk AI systems
    - ECOA requires reasons for credit denials
    - Users have ethical right to understand decisions affecting them
    - Explainability enables contestability and fairness verification
  when_to_run:
  - Before deploying automated decision systems
  - When implementing user-facing explanations
  - During regulatory compliance reviews
  - After complaints about unexplained decisions
prerequisites:
  required_artifacts:
  - type: algorithm_documentation
    description: Documentation of algorithmic decision systems
  - type: explanation_interfaces
    description: User-facing explanation mechanisms
  access_requirements:
  - Access to algorithmic systems
  - Explanation interface access
  - Interview access to ML and product teams
discovery:
  interviews:
  - role: ML/Data Science Lead
    questions:
    - What explainability techniques are implemented?
    - How are explanations generated for users?
    - What limitations exist in explaining decisions?
    - How do you balance accuracy with explainability?
    purpose: Understand technical explainability
    duration: 45 min
  - role: Product Manager
    questions:
    - What explanations do users receive?
    - What feedback have users given about explanations?
    - How can users contest algorithmic decisions?
    - What decisions are fully automated vs. human-reviewed?
    purpose: Understand user-facing explanations
    duration: 30 min
  - role: Legal/Compliance
    questions:
    - What explanation requirements apply to your systems?
    - How do you meet GDPR meaningful information requirements?
    - What documentation exists for automated decisions?
    purpose: Understand regulatory requirements
    duration: 30 min
  documents_to_review:
  - type: Explanation Interfaces
    purpose: Assess user-facing explanations
    look_for:
    - Explanation content and clarity
    - Factors disclosed
    - Actionable information
  - type: Technical Documentation
    purpose: Understand explainability implementation
    look_for:
    - XAI techniques used
    - Explanation fidelity
    - Known limitations
  file_patterns:
  - glob: '**/ETHICS.md'
    purpose: Ethics documentation
  - glob: '**/PRIVACY*.md'
    purpose: Privacy documentation
  - glob: '**/terms*.md'
    purpose: Terms of service
  - glob: '**/policy*.md'
    purpose: Policy documentation
  code_patterns:
  - pattern: consent|privacy|gdpr|ccpa|cookie
    type: regex
    scope: all
    purpose: Privacy/consent references
signals:
  critical:
  - id: EXPLAIN-CRIT-001
    signal: Automated decisions with no explanation mechanism
    evidence_indicators:
    - Decisions made without any explanation to users
    - No way for users to understand why decision was made
    - Opaque automated decisions affecting users
    explanation: |
      GDPR requires meaningful information about automated decisions.
      Providing no explanation may violate legal requirements and
      certainly violates ethical transparency principles.
    remediation: |
      - Implement explanation generation
      - Provide reasons for decisions
      - Disclose key factors considered
      - Enable decision contestability
  - id: EXPLAIN-CRIT-002
    signal: Explanations are misleading or inaccurate
    evidence_indicators:
    - Explanations don't reflect actual decision factors
    - Post-hoc rationalizations rather than true reasons
    - Simplified explanations that misrepresent model
    explanation: |
      Misleading explanations are worse than no explanation as they
      actively deceive users about why decisions were made.
    remediation: |
      - Ensure explanation fidelity
      - Use techniques with verified accuracy
      - Disclose explanation limitations
      - Avoid misleading simplifications
  high:
  - id: EXPLAIN-HIGH-001
    signal: Explanations not meaningful to users
    evidence_indicators:
    - Technical explanations users can't understand
    - Feature importance without context
    - Explanations don't enable user action
    - No indication how to get different outcome
    explanation: |
      GDPR requires meaningful information - explanations that users
      cannot understand or act on fail this standard.
    remediation: |
      - Design explanations for user comprehension
      - Provide contextual information
      - Include actionable guidance
      - Test with actual users
  - id: EXPLAIN-HIGH-002
    signal: No mechanism to contest decisions
    evidence_indicators:
    - No appeal process for algorithmic decisions
    - Human review not available
    - Users cannot provide additional information
    explanation: |
      Explainability should enable contestability. Without a way
      to contest, explanations have limited value.
    remediation: |
      - Implement decision appeal process
      - Provide human review option
      - Allow users to provide additional context
  medium:
  - id: EXPLAIN-MED-001
    signal: Explanations only available on request
    evidence_indicators:
    - Users must specifically request explanation
    - Explanation not provided proactively
    - Complex process to obtain explanation
    remediation: |
      - Provide explanations proactively
      - Make explanations easily accessible
      - Consider default explanation provision
  positive:
  - id: EXPLAIN-POS-001
    signal: Comprehensive explainability implementation
    evidence_indicators:
    - Clear user-facing explanations
    - Verified explanation fidelity
    - Actionable information provided
    - Appeal process available
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Automated Decision Inventory
    description: Identify all automated decisions affecting users.
    duration_estimate: 45 min
    expected_findings:
    - Decision system inventory
    - Impact assessment
  - id: '2'
    name: Explanation Mechanism Review
    description: Assess technical explainability implementation.
    duration_estimate: 1.5 hours
    expected_findings:
    - XAI techniques documented
    - Fidelity assessment
  - id: '3'
    name: User-Facing Explanation Testing
    description: Test explanations from user perspective.
    duration_estimate: 1 hour
    expected_findings:
    - Comprehensibility assessment
    - Actionability evaluation
  - id: '4'
    name: Contestability Assessment
    description: Review decision contestation mechanisms.
    duration_estimate: 45 min
    expected_findings:
    - Appeal process review
    - Human review availability
  - id: '5'
    name: Synthesize Findings
    description: Compile findings and recommendations.
    duration_estimate: 30 min
    expected_findings:
    - Prioritized findings
    - Recommendations
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Automated Decision Inventory
    - Explainability Assessment
    - Contestability Review
    - Recommendations
  confidence_guidance:
    high: Technical review, user testing, regulatory alignment verified
    medium: Technical review without user testing
    low: Documentation review only
offline:
  capability: partial
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive analysis
    full:
      included: true
      priority: 85
closeout_checklist:
- id: explain-001
  item: Automated decisions inventoried
  level: CRITICAL
  verification: manual
  verification_notes: All automated decisions affecting users documented
  expected: Confirmed by reviewer
- id: explain-002
  item: Explanation mechanisms assessed
  level: CRITICAL
  verification: manual
  verification_notes: User-facing explanations evaluated for clarity and fidelity
  expected: Confirmed by reviewer
- id: explain-003
  item: Contestability reviewed
  level: BLOCKING
  verification: manual
  verification_notes: Appeal and human review processes assessed
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - all
    domains:
    - Automated Decision Making
    - Machine Learning
    team_sizes:
    - small
    - medium
    - large
  compliance_frameworks:
  - framework: GDPR
    controls:
    - Article 22
    - Article 13(2)(f)
    - Article 14(2)(g)
  - framework: EU AI Act
    controls:
    - Transparency requirements
  - framework: ECOA
    controls:
    - Adverse action notices
relationships:
  commonly_combined:
  - responsible-design.algorithmic-fairness.bias-detection
  - responsible-design.consent-transparency.informed-consent
