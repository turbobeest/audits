audit:
  id: responsible-design.content-harm.harmful-content-moderation
  name: Harmful Content Moderation Audit
  version: 1.0.0
  last_updated: '2025-01-19'
  status: active
  category: ethical-societal
  category_number: 21
  subcategory: content-harm
  tier: expert
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: critical
  scope: qualitative
  default_profiles:
  - full
  blocks_phase: false
  parallelizable: false
description:
  what: |
    Evaluates the effectiveness and fairness of content moderation systems
    for harmful content including hate speech, violence, harassment,
    self-harm promotion, and illegal content. Assesses detection accuracy,
    enforcement consistency, appeal processes, and moderator wellbeing.
  why_it_matters: |
    Content moderation is critical for platform safety:

    - Inadequate moderation enables harm to users and communities
    - Over-moderation can silence legitimate speech
    - Inconsistent enforcement undermines trust
    - EU DSA requires effective content moderation
    - Moderator welfare is ethical obligation
    - Legal liability for certain content types
  when_to_run:
  - For platforms with user-generated content
  - During EU DSA compliance
  - After moderation failures or incidents
  - Annually for ongoing assessment
prerequisites:
  required_artifacts:
  - type: content_policies
    description: Content moderation policies
  - type: moderation_systems
    description: Documentation of moderation processes
  access_requirements:
  - Policy documentation
  - Moderation metrics access
  - Interview access to trust & safety
discovery:
  interviews:
  - role: Trust & Safety Lead
    questions:
    - What content categories are moderated?
    - How is harmful content detected?
    - What are response times for reports?
    - How is consistency ensured?
    - What appeal process exists?
    purpose: Understand moderation system
    duration: 45 min
  - role: Moderation Operations
    questions:
    - What volume of content is reviewed?
    - What training do moderators receive?
    - What support exists for moderator wellbeing?
    - What are the biggest challenges?
    purpose: Understand operational realities
    duration: 45 min
  documents_to_review:
  - type: Content Policy
    purpose: Review policy clarity and coverage
    look_for:
    - Policy completeness
    - Clear definitions
    - Enforcement actions
  - type: Moderation Metrics
    purpose: Assess effectiveness
    look_for:
    - Detection rates
    - Response times
    - Appeal outcomes
    - Consistency metrics
  file_patterns:
  - glob: '**/ETHICS.md'
    purpose: Ethics documentation
  - glob: '**/PRIVACY*.md'
    purpose: Privacy documentation
  - glob: '**/terms*.md'
    purpose: Terms of service
  - glob: '**/policy*.md'
    purpose: Policy documentation
  code_patterns:
  - pattern: consent|privacy|gdpr|ccpa|cookie
    type: regex
    scope: all
    purpose: Privacy/consent references
signals:
  critical:
  - id: MODERATE-CRIT-001
    signal: Illegal content not effectively addressed
    evidence_indicators:
    - CSAM not promptly removed
    - Terrorist content persists
    - Illegal content goes unreported to authorities
    - No proactive detection for illegal content
    explanation: |
      Failure to address illegal content creates legal liability
      and enables serious harm. This is a critical safety failure.
    remediation: |
      - Implement proactive illegal content detection
      - Ensure rapid removal (24 hours or less)
      - Report to appropriate authorities
      - Document compliance with legal requirements
  - id: MODERATE-CRIT-002
    signal: No moderation system for harmful content
    evidence_indicators:
    - No content policies defined
    - No moderation staff or systems
    - Reports not processed
    - Harmful content freely circulates
    explanation: |
      Absence of content moderation enables harm and fails
      regulatory requirements including EU DSA.
    remediation: |
      - Develop content policies
      - Implement moderation systems
      - Staff appropriately for volume
      - Create reporting mechanisms
  high:
  - id: MODERATE-HIGH-001
    signal: Slow response to harmful content reports
    evidence_indicators:
    - Reports taking days to process
    - Harmful content remains visible during review
    - No prioritization for severe harm
    - Backlog growing
    explanation: |
      Slow moderation allows harmful content to cause damage
      during extended exposure.
    remediation: |
      - Implement SLA-based prioritization
      - Prioritize severe harm reports
      - Scale capacity to meet demand
      - Consider proactive detection
  - id: MODERATE-HIGH-002
    signal: Inconsistent enforcement
    evidence_indicators:
    - Similar content treated differently
    - High appeal success rates
    - Policy interpretation varies
    - Bias in enforcement patterns
    explanation: |
      Inconsistent enforcement undermines trust and may indicate
      bias or inadequate training.
    remediation: |
      - Improve policy clarity
      - Enhance moderator training
      - Implement quality assurance
      - Monitor consistency metrics
  - id: MODERATE-HIGH-003
    signal: Moderator wellbeing not addressed
    evidence_indicators:
    - No psychological support for moderators
    - Excessive exposure to harmful content
    - High turnover in moderation team
    - No content exposure limits
    explanation: |
      Moderators exposed to harmful content face psychological harm.
      Failing to protect them is both unethical and operationally
      unsustainable.
    remediation: |
      - Provide psychological support
      - Implement exposure limits
      - Use automated filtering where possible
      - Monitor moderator wellbeing
  medium:
  - id: MODERATE-MED-001
    signal: No appeal process for moderation decisions
    evidence_indicators:
    - Users cannot appeal
    - Appeals not reviewed by different person
    - Appeal outcomes not tracked
    remediation: |
      - Implement appeal process
      - Independent review of appeals
      - Track and learn from appeals
  positive:
  - id: MODERATE-POS-001
    signal: Comprehensive, fair moderation system
    evidence_indicators:
    - Clear policies consistently enforced
    - Rapid response to reports
    - Fair appeal process
    - Moderator support in place
    - Regular effectiveness review
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Policy Review
    description: Review content policies for completeness and clarity.
    duration_estimate: 1 hour
    expected_findings:
    - Policy assessment
    - Coverage gaps
  - id: '2'
    name: Effectiveness Assessment
    description: Assess moderation effectiveness metrics.
    duration_estimate: 1.5 hours
    expected_findings:
    - Detection rates
    - Response times
    - Consistency
  - id: '3'
    name: Stakeholder Interviews
    description: Interview trust & safety and operations.
    duration_estimate: 1.5 hours
    expected_findings:
    - Process understanding
    - Challenges identified
  - id: '4'
    name: Synthesize Findings
    description: Compile findings and recommendations.
    duration_estimate: 30 min
    expected_findings:
    - Prioritized findings
    - Recommendations
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Policy Assessment
    - Effectiveness Review
    - Recommendations
  confidence_guidance:
    high: Metrics available, policies documented, interviews completed
    medium: Partial metrics or access
    low: External assessment only
offline:
  capability: partial
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive analysis
    full:
      included: true
      priority: 90
closeout_checklist:
- id: moderate-001
  item: Content policies reviewed
  level: CRITICAL
  verification: manual
  verification_notes: Policies assessed for coverage and clarity
  expected: Confirmed by reviewer
- id: moderate-002
  item: Moderation effectiveness assessed
  level: CRITICAL
  verification: manual
  verification_notes: Metrics reviewed for effectiveness
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - social
    - content
    - platform
    team_sizes:
    - medium
    - large
  compliance_frameworks:
  - framework: EU DSA
    controls:
    - Article 14
    - Article 16
    - Article 20
relationships:
  commonly_combined:
  - responsible-design.content-harm.misinformation-amplification
  - responsible-design.content-harm.harassment-prevention
