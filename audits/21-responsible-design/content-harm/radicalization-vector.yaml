audit:
  id: responsible-design.content-harm.radicalization-vector
  name: Radicalization Vector Audit
  version: 1.0.0
  last_updated: '2025-01-19'
  status: active
  category: ethical-societal
  category_number: 21
  subcategory: content-harm
  tier: expert
  estimated_duration: 5-7 hours  # median: 6h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: critical
  scope: qualitative
  default_profiles:
  - full
  blocks_phase: false
  parallelizable: false
description:
  what: |
    Evaluates whether platform systems act as vectors for radicalization by
    progressively recommending increasingly extreme content. Assesses whether
    recommendation algorithms create "rabbit holes" that lead users from
    mainstream to extreme content, and whether safeguards prevent this pathway.

    Examines algorithmic pathways, content adjacency, extremism detection,
    and circuit breakers that interrupt radicalization progressions.
  why_it_matters: |
    Platforms can inadvertently accelerate radicalization:

    - Recommendation systems optimizing for engagement may favor extreme content
    - Progressive exposure to extreme content normalizes it
    - Algorithmic radicalization has been linked to real-world violence
    - EU DSA requires systemic risk assessment for radicalization
    - Platforms face increasing scrutiny and liability
    - Social harm from radicalization is severe and lasting
  when_to_run:
  - For platforms with recommendation systems
  - When content includes political or ideological material
  - During EU DSA compliance
  - After radicalization-linked incidents
prerequisites:
  required_artifacts:
  - type: recommendation_systems
    description: Documentation of content recommendation algorithms
  - type: extremism_policies
    description: Policies on extreme content
  access_requirements:
  - Algorithm documentation
  - Content classification systems
  - Interview access to trust & safety
discovery:
  interviews:
  - role: Trust & Safety Lead
    questions:
    - How do you define and identify extremist content?
    - What prevents recommendation of extreme content?
    - Do you track progression patterns toward extremism?
    - What circuit breakers exist for radicalization pathways?
    purpose: Understand extremism prevention
    duration: 45 min
  - role: Recommendation System Lead
    questions:
    - How do recommendations handle borderline content?
    - Are there safeguards against progressive extremity?
    - How is content adjacency managed?
    - What prevents rabbit hole formation?
    purpose: Understand algorithmic safeguards
    duration: 45 min
  documents_to_review:
  - type: Extremism Policy
    purpose: Review extremism definitions and handling
    look_for:
    - Extremism definitions
    - Borderline content handling
    - Enforcement actions
  - type: Recommendation Documentation
    purpose: Review radicalization safeguards
    look_for:
    - Progressive exposure limits
    - Circuit breakers
    - Content adjacency rules
  file_patterns:
  - glob: '**/ETHICS.md'
    purpose: Ethics documentation
  - glob: '**/PRIVACY*.md'
    purpose: Privacy documentation
  - glob: '**/terms*.md'
    purpose: Terms of service
  - glob: '**/policy*.md'
    purpose: Policy documentation
  code_patterns:
  - pattern: consent|privacy|gdpr|ccpa|cookie
    type: regex
    scope: all
    purpose: Privacy/consent references
signals:
  critical:
  - id: RADICAL-CRIT-001
    signal: Algorithms recommend progressively extreme content
    evidence_indicators:
    - Users shown increasingly extreme content over time
    - Engagement with mild content leads to extreme recommendations
    - No limits on progressive extremity
    - Extremist content in related suggestions
    explanation: |
      When algorithms recommend progressively extreme content, they
      create radicalization pathways that can lead to real-world harm.
    remediation: |
      - Implement progressive exposure limits
      - Break recommendation chains toward extremism
      - Add content adjacency safeguards
      - Monitor pathway formation
  - id: RADICAL-CRIT-002
    signal: No identification or handling of extremist content
    evidence_indicators:
    - Extremist content not labeled or identified
    - No distinction from mainstream content
    - Extremist content recommended normally
    - No demotion of borderline content
    explanation: |
      Without identifying extremist content, platforms cannot prevent
      algorithmic radicalization or reduce harmful exposure.
    remediation: |
      - Develop extremism classification
      - Demote or exclude from recommendations
      - Monitor extremist content reach
      - Reduce algorithmic amplification
  high:
  - id: RADICAL-HIGH-001
    signal: Rabbit hole patterns not monitored
    evidence_indicators:
    - No tracking of user progression toward extremism
    - Content consumption patterns not analyzed
    - Accelerating extremity not detected
    - No early warning indicators
    explanation: |
      Without monitoring for radicalization patterns, platforms cannot
      intervene before users become deeply radicalized.
    remediation: |
      - Track content progression patterns
      - Identify accelerating extremity
      - Create early warning indicators
      - Implement intervention points
  - id: RADICAL-HIGH-002
    signal: No circuit breakers for radicalization
    evidence_indicators:
    - No interruption of extreme content sessions
    - Autoplay continues through extreme content
    - No friction for borderline content
    - Alternative perspectives not offered
    explanation: |
      Circuit breakers can interrupt radicalization pathways before
      deep immersion in extreme content.
    remediation: |
      - Implement session interruptions
      - Add friction before extreme content
      - Offer alternative perspectives
      - Break autoplay chains
  - id: RADICAL-HIGH-003
    signal: Extremist communities form and recruit
    evidence_indicators:
    - Groups promote extremist ideology
    - Recruitment activity not detected
    - Radicalization discussions visible
    - No enforcement on extremist groups
    explanation: |
      Extremist communities can accelerate radicalization through
      social reinforcement and active recruitment.
    remediation: |
      - Monitor for extremist group formation
      - Detect recruitment patterns
      - Enforce against radicalization activity
      - Remove extremist organizing spaces
  medium:
  - id: RADICAL-MED-001
    signal: Borderline content handling unclear
    evidence_indicators:
    - No clear definition of borderline content
    - Inconsistent treatment of near-extreme content
    - No graduated approach to borderline material
    remediation: |
      - Define borderline content clearly
      - Implement graduated handling
      - Reduce reach without removal
      - Monitor borderline content trends
  positive:
  - id: RADICAL-POS-001
    signal: Comprehensive radicalization prevention
    evidence_indicators:
    - Extremist content identified and demoted
    - Progressive exposure limits in place
    - Circuit breakers interrupt pathways
    - Radicalization patterns monitored
    - Alternative perspectives offered
procedure:
  context:
    cognitive_mode: critical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Recommendation Pathway Analysis
    description: Assess how recommendations handle extremity.
    duration_estimate: 1.5 hours
    expected_findings:
    - Pathway dynamics
    - Progressive exposure patterns
  - id: '2'
    name: Extremism Classification Review
    description: Review how extremist content is identified.
    duration_estimate: 1 hour
    expected_findings:
    - Classification coverage
    - Borderline handling
  - id: '3'
    name: Circuit Breaker Assessment
    description: Evaluate interventions against radicalization.
    duration_estimate: 1 hour
    expected_findings:
    - Intervention points
    - Effectiveness assessment
  - id: '4'
    name: Stakeholder Interviews
    description: Interview trust & safety and recommendation teams.
    duration_estimate: 1.5 hours
    expected_findings:
    - Process understanding
    - Gap identification
  - id: '5'
    name: Synthesize Findings
    description: Compile findings and recommendations.
    duration_estimate: 30 min
    expected_findings:
    - Prioritized findings
    - Recommendations
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Pathway Analysis
    - Classification Review
    - Circuit Breakers
    - Recommendations
  confidence_guidance:
    high: Algorithm reviewed, patterns analyzed, interviews completed
    medium: Partial visibility into systems
    low: External assessment only
offline:
  capability: partial
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive analysis
    full:
      included: true
      priority: 90
closeout_checklist:
- id: radical-001
  item: Recommendation pathways assessed
  level: CRITICAL
  verification: manual
  verification_notes: Progressive exposure dynamics evaluated
  expected: Confirmed by reviewer
- id: radical-002
  item: Extremism classification reviewed
  level: CRITICAL
  verification: manual
  verification_notes: Content classification coverage assessed
  expected: Confirmed by reviewer
- id: radical-003
  item: Circuit breakers evaluated
  level: BLOCKING
  verification: manual
  verification_notes: Intervention mechanisms assessed
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - social
    - content
    - platform
    team_sizes:
    - medium
    - large
  compliance_frameworks:
  - framework: EU DSA
    controls:
    - Article 34 (Systemic risks)
    - Article 35 (Risk mitigation)
relationships:
  commonly_combined:
  - responsible-design.content-harm.misinformation-amplification
  - responsible-design.content-harm.harmful-content-moderation
