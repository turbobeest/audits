audit:
  id: responsible-design.algorithmic-fairness.fairness-metric-selection
  name: Fairness Metric Selection Audit
  version: 1.0.0
  last_updated: '2025-01-19'
  status: active
  category: ethical-societal
  category_number: 21
  subcategory: algorithmic-fairness
  tier: phd
  estimated_duration: 3-5 hours  # median: 4h
  completeness: requires_discovery
  requires_runtime: false
  destructive: false
execution:
  automatable: manual
  severity: high
  scope: qualitative
  default_profiles:
  - full
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates the appropriateness of fairness metrics chosen for algorithmic
    systems. Assesses whether selected metrics align with the ethical context,
    legal requirements, and stakeholder values for the specific use case.

    Different fairness metrics (demographic parity, equalized odds, calibration,
    individual fairness) encode different ethical assumptions and cannot all
    be satisfied simultaneously (impossibility theorems). This audit ensures
    the chosen metrics match the normative requirements of the domain.
  why_it_matters: |
    Fairness metric selection is a normative choice with significant implications:

    - Choosing demographic parity may sacrifice accuracy for some groups
    - Equalized odds may permit different selection rates
    - Calibration may conflict with demographic parity
    - Wrong metric choice can mask discrimination or create new unfairness

    Organizations often default to a single metric without understanding
    tradeoffs, leading to:
    - False confidence in "fair" systems that discriminate
    - Regulatory non-compliance despite passing fairness checks
    - Stakeholder distrust when metric doesn't match their values
  when_to_run:
  - When designing ML fairness evaluation framework
  - Before deploying high-stakes algorithmic systems
  - When stakeholders question fairness criteria
  - After regulatory guidance changes
prerequisites:
  required_artifacts:
  - type: fairness_framework_documentation
    description: Documentation of current fairness metrics used
  - type: use_case_documentation
    description: Description of algorithmic system's purpose and context
  - type: stakeholder_identification
    description: List of affected stakeholders and their interests
  optional_artifacts:
  - type: regulatory_requirements
    description: Applicable regulations and their fairness interpretations
  access_requirements:
  - Access to ML team fairness documentation
  - Ability to interview stakeholders and ethicists
  - Access to legal/compliance interpretation of requirements
discovery:
  interviews:
  - role: ML/Data Science Lead
    questions:
    - What fairness metrics are currently used?
    - Why were these metrics chosen over alternatives?
    - What tradeoffs were considered?
    - Are you aware of impossibility theorems in fairness?
    purpose: Understand current metric selection rationale
    duration: 45 min
  - role: Ethics/Responsible AI Lead
    questions:
    - What ethical framework guides fairness decisions?
    - How are stakeholder values incorporated into metric selection?
    - What documentation exists for fairness metric choices?
    - Are different contexts evaluated differently?
    purpose: Assess ethical governance of metric selection
    duration: 45 min
  - role: Legal/Compliance
    questions:
    - What regulatory requirements inform fairness metrics?
    - How do chosen metrics align with legal standards?
    - What documentation is maintained for compliance?
    purpose: Verify regulatory alignment
    duration: 30 min
  - role: Affected Stakeholder Representatives
    questions:
    - What does fairness mean to you in this context?
    - What outcomes would you consider unfair?
    - Do current metrics capture your concerns?
    purpose: Validate metric alignment with stakeholder values
    duration: 30-45 min
  documents_to_review:
  - type: Fairness Framework Documentation
    purpose: Review current metric definitions and rationale
    look_for:
    - Specific metrics used (demographic parity, equalized odds, etc.)
    - Rationale for metric selection
    - Tradeoff analysis documentation
    - Context-specific considerations
  - type: Model Cards
    purpose: Assess fairness metric disclosure
    look_for:
    - Fairness metrics reported
    - Limitations acknowledged
    - Tradeoffs disclosed
  - type: Ethical Review Documentation
    purpose: Review ethical decision-making process
    look_for:
    - Stakeholder input documentation
    - Ethical framework application
    - Metric selection decision rationale
  file_patterns:
  - glob: '**/ETHICS.md'
    purpose: Ethics documentation
  - glob: '**/PRIVACY*.md'
    purpose: Privacy documentation
  - glob: '**/terms*.md'
    purpose: Terms of service
  - glob: '**/policy*.md'
    purpose: Policy documentation
  code_patterns:
  - pattern: consent|privacy|gdpr|ccpa|cookie
    type: regex
    scope: all
    purpose: Privacy/consent references
knowledge_sources:
  specifications:
  - id: impossibility-theorems
    name: Fairness Impossibility Theorems (Chouldechova, Kleinberg)
    url: https://arxiv.org/abs/1609.05807
    offline_cache: true
    priority: required
  guides:
  - id: fairmlbook
    name: 'Fairness and Machine Learning: Limitations and Opportunities'
    url: https://fairmlbook.org/
    offline_cache: true
    priority: required
  - id: google-fairness-indicators
    name: Google Fairness Indicators Guide
    url: https://www.tensorflow.org/responsible_ai/fairness_indicators/guide
    offline_cache: true
    priority: recommended
  papers:
  - id: chouldechova
    title: 'Fair prediction with disparate impact: A study of bias in recidivism prediction instruments'
    url: https://arxiv.org/abs/1610.07524
  - id: kleinberg
    title: Inherent Trade-Offs in the Fair Determination of Risk Scores
    url: https://arxiv.org/abs/1609.05807
  - id: corbett-davies
    title: 'The Measure and Mismeasure of Fairness: A Critical Review'
    url: https://arxiv.org/abs/1808.00023
  learning_resources:
  - id: mitchell-fairness-definitions
    title: 21 Fairness Definitions and Their Politics
    type: paper
    reference: https://www.fatml.org/resources/fairness-definitions-explained
    relevance: Comprehensive overview of fairness metrics and their implications
tooling:
  assessment_tools:
  - tool: Fairness Metric Comparison Matrix
    purpose: Systematically compare metric properties
    description: |
      Matrix comparing metrics across dimensions:
      - Group vs individual fairness
      - Compatibility with other metrics
      - Regulatory alignment
      - Stakeholder value alignment
    offline_capable: true
  - tool: Stakeholder Value Elicitation
    purpose: Gather stakeholder fairness preferences
    description: |
      Structured interview or survey process to understand
      what fairness means to different stakeholders
    offline_capable: true
signals:
  critical:
  - id: FAIRMETRIC-CRIT-001
    signal: Metric choice conflicts with regulatory requirements
    evidence_indicators:
    - Using demographic parity when regulations require equal error rates
    - Metric doesn't align with legal definition of fairness for domain
    - No analysis of regulatory fairness interpretation
    explanation: |
      Regulatory frameworks often imply specific fairness definitions.
      Using incompatible metrics creates compliance risk regardless
      of passing the chosen metric.
    remediation: |
      - Conduct regulatory analysis of fairness requirements
      - Map legal requirements to specific fairness metrics
      - Consult legal counsel on metric appropriateness
      - Consider multi-metric evaluation framework
  - id: FAIRMETRIC-CRIT-002
    signal: Single metric obscures known tradeoffs
    evidence_indicators:
    - Only one fairness metric evaluated
    - No acknowledgment of impossibility theorems
    - Metrics known to conflict not both evaluated
    - Calibration vs demographic parity tradeoff ignored
    explanation: |
      Impossibility theorems prove certain fairness metrics cannot
      be simultaneously satisfied. Single-metric evaluation can
      create false confidence while hiding discrimination.
    remediation: |
      - Implement multi-metric fairness evaluation
      - Document known tradeoffs between metrics
      - Explain why chosen metric is prioritized
      - Report performance on secondary metrics
  high:
  - id: FAIRMETRIC-HIGH-001
    signal: Metric selection lacks stakeholder input
    evidence_indicators:
    - No documentation of stakeholder value elicitation
    - Affected communities not consulted on fairness definition
    - Metric chosen by technical team alone
    explanation: |
      Fairness is a normative concept that should reflect stakeholder
      values, not just technical convenience. Technical teams may
      choose metrics that don't align with affected community values.
    remediation: |
      - Conduct stakeholder engagement on fairness definition
      - Document how stakeholder input shaped metric selection
      - Create feedback mechanism for ongoing input
      - Consider participatory fairness definition process
  - id: FAIRMETRIC-HIGH-002
    signal: Metric inappropriate for domain context
    evidence_indicators:
    - Using demographic parity for risk assessment (may be inappropriate)
    - Using calibration when equal selection rates matter
    - No analysis of metric-context fit
    explanation: |
      Different contexts call for different fairness metrics.
      Risk assessment may require calibration; opportunity allocation
      may require demographic parity. Context mismatch leads to
      ethically inappropriate systems.
    remediation: |
      - Analyze context to determine appropriate fairness conception
      - Document rationale for metric-context alignment
      - Consider domain-specific fairness guidance
      - Engage domain experts in metric selection
  medium:
  - id: FAIRMETRIC-MED-001
    signal: No documentation of metric selection rationale
    evidence_indicators:
    - Metrics used without documented justification
    - No tradeoff analysis recorded
    - Metric inherited without re-evaluation
    remediation: |
      - Document rationale for current metrics
      - Conduct tradeoff analysis and record findings
      - Establish periodic metric re-evaluation process
  - id: FAIRMETRIC-MED-002
    signal: Metrics not disclosed to affected users
    evidence_indicators:
    - Model cards don't specify fairness metrics
    - Users cannot understand fairness criteria
    - Transparency about fairness approach lacking
    remediation: |
      - Add fairness metrics to model documentation
      - Create user-accessible fairness explanations
      - Disclose known limitations and tradeoffs
  positive:
  - id: FAIRMETRIC-POS-001
    signal: Multi-metric fairness framework with documented tradeoffs
    evidence_indicators:
    - Multiple complementary metrics evaluated
    - Tradeoffs explicitly documented
    - Rationale for prioritization clear
    - Impossibility constraints acknowledged
  - id: FAIRMETRIC-POS-002
    signal: Stakeholder-informed metric selection
    evidence_indicators:
    - Documented stakeholder engagement process
    - Affected community input shapes metrics
    - Ongoing feedback mechanism exists
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  preparation:
  - step: Gather current fairness framework documentation
  - step: Identify all fairness metrics in use
  - step: Map use cases to their fairness requirements
  - step: Schedule stakeholder interviews
  steps:
  - id: '1'
    name: Inventory Current Metrics
    description: |
      Document all fairness metrics currently used across algorithmic
      systems, their definitions, and implementation.
    duration_estimate: 30-60 min
    expected_findings:
    - Complete list of fairness metrics in use
    - Definitions and implementation details
  - id: '2'
    name: Analyze Metric Properties
    description: |
      For each metric, analyze its properties including compatibility
      with other metrics, regulatory alignment, and ethical assumptions.
    duration_estimate: 1-2 hours
    questions:
    - What ethical assumptions does this metric encode?
    - What tradeoffs does this metric accept?
    - Which other metrics conflict with this one?
    expected_findings:
    - Property analysis for each metric
    - Incompatibility mapping
  - id: '3'
    name: Regulatory Requirements Analysis
    description: |
      Map regulatory requirements to fairness metrics and assess
      whether current metrics satisfy legal obligations.
    duration_estimate: 1 hour
    questions:
    - What does regulation require for this domain?
    - Do current metrics align with regulatory interpretation?
    expected_findings:
    - Regulatory-metric alignment assessment
    - Compliance gaps identified
  - id: '4'
    name: Stakeholder Value Elicitation
    description: |
      Interview stakeholders to understand their fairness values
      and assess whether current metrics capture their concerns.
    duration_estimate: 1-2 hours
    expected_findings:
    - Stakeholder fairness definitions
    - Value-metric alignment assessment
  - id: '5'
    name: Synthesize Findings
    description: |
      Compile analysis into recommendations for metric selection
      and documentation improvements.
    duration_estimate: 1 hour
    expected_findings:
    - Metric appropriateness assessment
    - Recommendations for improvement
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: metric_analysis_matrix
    format: structured
    description: Comparison of metrics across relevant dimensions
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Current Metrics Assessment
    - Regulatory Alignment Analysis
    - Stakeholder Value Alignment
    - Recommendations
  confidence_guidance:
    high: Multiple stakeholders agree, regulatory guidance clear, documented rationale
    medium: Some stakeholder disagreement, regulatory interpretation uncertain
    low: Limited stakeholder input, novel domain without clear precedent
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: fairmlbook
      priority: required
    - source_id: impossibility-theorems
      priority: required
  degradation:
  - feature: Current regulatory guidance lookup
    impact: Cannot verify latest regulatory interpretations
    mitigation: Cache key regulatory documents
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive stakeholder engagement
    security:
      included: false
      reason: Not security-focused
    production:
      included: false
      reason: Design-phase audit
    full:
      included: true
      priority: 80
closeout_checklist:
- id: fairmetric-001
  item: All fairness metrics in use documented
  level: CRITICAL
  verification: manual
  verification_notes: Reviewer confirms complete inventory of fairness metrics
  expected: Confirmed by reviewer
- id: fairmetric-002
  item: Metric properties and tradeoffs analyzed
  level: BLOCKING
  verification: manual
  verification_notes: Tradeoff analysis exists for each metric
  expected: Confirmed by reviewer
- id: fairmetric-003
  item: Regulatory alignment assessed
  level: BLOCKING
  verification: manual
  verification_notes: Regulatory requirements mapped to chosen metrics
  expected: Confirmed by reviewer
- id: fairmetric-004
  item: Stakeholder input documented
  level: WARNING
  verification: manual
  verification_notes: Stakeholder fairness values captured and considered
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - all
    domains:
    - Machine Learning
    - Automated Decision Making
    team_sizes:
    - small
    - medium
    - large
  compliance_frameworks:
  - framework: EU AI Act
    controls:
    - Article 9 (Risk Management)
    - Article 13 (Transparency)
  - framework: NIST AI RMF
    controls:
    - MAP 1.5
    - MEASURE 2.6
relationships:
  commonly_combined:
  - responsible-design.algorithmic-fairness.bias-detection
  - responsible-design.algorithmic-fairness.disparate-impact
  - responsible-design.consent-transparency.algorithm-explainability
