audit:
  id: responsible-design.algorithmic-fairness.historical-bias-training-data
  name: Historical Bias in Training Data Audit
  version: 1.0.0
  last_updated: '2025-01-19'
  status: active
  category: ethical-societal
  category_number: 21
  subcategory: algorithmic-fairness
  tier: phd
  estimated_duration: 4-6 hours  # median: 5h
  completeness: requires_discovery
  requires_runtime: false
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: qualitative
  default_profiles:
  - full
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Assesses training data for historical biases that may be learned and
    perpetuated by machine learning models. Examines whether training labels,
    data collection processes, or historical patterns encode past discrimination
    that could be amplified by algorithmic systems.

    Examples include hiring models trained on historically biased hiring decisions,
    criminal justice algorithms trained on racially disparate arrest data, and
    lending models trained on redlining-era decisions.
  why_it_matters: |
    Machine learning models learn from data - if that data reflects historical
    discrimination, models will perpetuate and potentially amplify it:

    - Models trained on biased hiring data will continue discriminating
    - Criminal justice algorithms trained on biased arrests predict biased outcomes
    - Loan models trained on redlining data encode geographic discrimination
    - Healthcare algorithms trained during access disparities underserve minorities

    Unlike explicit discrimination, historical bias creates discrimination
    through apparently objective data-driven processes, making it harder to
    detect and contest.
  when_to_run:
  - Before training new ML models
  - When onboarding new data sources
  - After discovering unexplained disparate impact
  - During responsible AI assessments
  - When expanding to populations not in training data
prerequisites:
  required_artifacts:
  - type: training_data_documentation
    description: Documentation of training data sources, collection methods, and time periods
  - type: label_documentation
    description: Documentation of how training labels were created
  - type: domain_context
    description: Understanding of historical discrimination patterns in the domain
  optional_artifacts:
  - type: data_provenance
    description: Detailed lineage of data sources
  - type: historical_analysis
    description: Prior analysis of bias in this data or domain
  access_requirements:
  - Access to training data documentation
  - Access to label creation process documentation
  - Ability to interview data and domain experts
  - Historical context on domain discrimination
discovery:
  interviews:
  - role: ML/Data Science Lead
    questions:
    - How was training data collected and from what time period?
    - How were training labels created?
    - What historical biases might exist in this data?
    - Has the data been audited for historical discrimination patterns?
    purpose: Understand data provenance and bias awareness
    duration: 45 min
  - role: Domain Expert
    questions:
    - What historical discrimination existed in this domain?
    - What time periods were particularly problematic?
    - What data collection methods might have encoded bias?
    - What outcomes reflect bias vs. legitimate differences?
    purpose: Domain-specific historical context
    duration: 45 min
  - role: Data Engineering Lead
    questions:
    - What are the sources for training data?
    - What time periods are represented?
    - What data cleaning or filtering was applied?
    - Were any populations or time periods excluded?
    purpose: Technical understanding of data pipeline
    duration: 30 min
  documents_to_review:
  - type: Data Sheets / Data Cards
    purpose: Review documented data provenance
    look_for:
    - Data collection methodology
    - Time period covered
    - Known limitations
    - Demographic composition
  - type: Label Documentation
    purpose: Understand label creation process
    look_for:
    - Who created labels (humans vs. existing systems)
    - Label definition and criteria
    - Quality control processes
    - Historical decision basis
  - type: Historical Domain Research
    purpose: Understand domain discrimination history
    look_for:
    - Documented discrimination patterns
    - Regulatory actions or lawsuits
    - Academic research on bias
  file_patterns:
  - glob: '**/ETHICS.md'
    purpose: Ethics documentation
  - glob: '**/PRIVACY*.md'
    purpose: Privacy documentation
  - glob: '**/terms*.md'
    purpose: Terms of service
  - glob: '**/policy*.md'
    purpose: Policy documentation
  code_patterns:
  - pattern: consent|privacy|gdpr|ccpa|cookie
    type: regex
    scope: all
    purpose: Privacy/consent references
knowledge_sources:
  specifications:
  - id: datasheets-for-datasets
    name: Datasheets for Datasets
    url: https://arxiv.org/abs/1803.09010
    offline_cache: true
    priority: required
  guides:
  - id: fairmlbook-data
    name: Fairness and Machine Learning - Chapter on Data
    url: https://fairmlbook.org/classification.html
    offline_cache: true
    priority: required
  papers:
  - id: obermeyer-health
    title: Dissecting racial bias in an algorithm used to manage population health
    url: https://www.science.org/doi/10.1126/science.aax2342
  - id: propublica-compas
    title: Machine Bias (ProPublica COMPAS Investigation)
    url: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
  learning_resources:
  - id: data-feminism
    title: Data Feminism (D'Ignazio & Klein)
    type: book
    reference: MIT Press
    relevance: Framework for understanding data bias
tooling:
  assessment_tools:
  - tool: Historical Bias Assessment Framework
    purpose: Systematic evaluation of historical bias risks
    description: |
      Structured assessment of:
      - Data collection time period and methods
      - Label creation process and decision-makers
      - Domain historical discrimination context
      - Outcome definition appropriateness
    offline_capable: true
  - tool: Label Audit Protocol
    purpose: Assess labels for historical discrimination encoding
    description: |
      Evaluate whether labels reflect:
      - Legitimate outcomes vs. biased decisions
      - Historical access disparities
      - Systemic discrimination patterns
    offline_capable: true
  infrastructure_tools:
  - tool: Data profiling tools
    purpose: Analyze data composition and patterns
    description: |
      Profile training data for demographic patterns,
      time period distribution, and outcome rates by group
signals:
  critical:
  - id: HISTBIAS-CRIT-001
    signal: Labels derived from historically biased decisions
    evidence_indicators:
    - Hiring labels based on historical hiring decisions
    - Arrest/conviction labels used for risk prediction
    - Loan approval labels from redlining era
    - Performance labels from biased evaluation processes
    explanation: |
      When labels encode past discrimination, models learn to reproduce
      that discrimination. This is particularly problematic when historical
      decisions were made by biased humans or during periods of documented
      discrimination.
    remediation: |
      - Critically evaluate whether labels reflect bias vs. merit
      - Consider alternative label definitions
      - Use causal inference to debias labels
      - Implement human review for high-stakes decisions
      - Document label limitations prominently
  - id: HISTBIAS-CRIT-002
    signal: Training data from period of documented discrimination
    evidence_indicators:
    - Data collected during legally segregated era
    - Data from period of documented industry discrimination
    - Data predates anti-discrimination enforcement
    - Time period with known systemic bias
    explanation: |
      Data from periods of documented discrimination necessarily
      encodes that discrimination. Using such data without adjustment
      perpetuates historical wrongs.
    remediation: |
      - Exclude or heavily weight against biased time periods
      - Apply statistical debiasing techniques
      - Supplement with more recent, less biased data
      - Document time period limitations
  high:
  - id: HISTBIAS-HIGH-001
    signal: Outcomes reflect access disparities, not true differences
    evidence_indicators:
    - Healthcare outcomes reflect insurance access, not health
    - Educational outcomes reflect school quality, not ability
    - Employment outcomes reflect network access, not merit
    - Confounding of outcome with opportunity
    explanation: |
      When outcomes reflect historical access disparities rather than
      underlying characteristics, models trained on these outcomes
      perpetuate the access disparity as if it were natural.
    remediation: |
      - Identify causal pathways from access to outcomes
      - Consider alternative outcome definitions
      - Control for access-related confounders
      - Document known confounding
  - id: HISTBIAS-HIGH-002
    signal: Data collection process encoded discrimination
    evidence_indicators:
    - Biased sampling excluded certain populations
    - Data collected primarily in privileged contexts
    - Collection methods created demographic skew
    - Historical data accessibility varied by group
    explanation: |
      Biased data collection creates training data that doesn't
      represent affected populations, leading to models that
      perform poorly for underrepresented groups.
    remediation: |
      - Audit data collection for demographic bias
      - Supplement with more representative data
      - Apply importance weighting for underrepresentation
      - Document collection limitations
  - id: HISTBIAS-HIGH-003
    signal: Human labelers may have encoded bias
    evidence_indicators:
    - Labels created by homogeneous labeling team
    - No bias awareness training for labelers
    - Subjective labeling criteria
    - Labels for subjective constructs (quality, risk, potential)
    explanation: |
      Human labelers bring their own biases to subjective labeling
      tasks. Without diversity and bias awareness, these biases
      are encoded in training data.
    remediation: |
      - Diversify labeling teams
      - Implement bias awareness training
      - Use multiple labelers and measure agreement
      - Define objective labeling criteria where possible
  medium:
  - id: HISTBIAS-MED-001
    signal: No documentation of data historical context
    evidence_indicators:
    - Data provenance not documented
    - Time period of data collection unknown
    - Label creation process undocumented
    - Historical bias assessment not performed
    remediation: |
      - Create comprehensive data documentation
      - Document time periods and collection methods
      - Perform historical bias assessment
      - Create datasheets for datasets
  - id: HISTBIAS-MED-002
    signal: Feedback loops may amplify historical bias
    evidence_indicators:
    - Model predictions influence future training data
    - Decisions based on model feed back into labels
    - No mechanism to break feedback loops
    remediation: |
      - Identify feedback loop pathways
      - Implement mechanisms to break loops
      - Monitor for bias amplification over time
      - Consider human-in-the-loop interventions
  positive:
  - id: HISTBIAS-POS-001
    signal: Comprehensive historical bias assessment completed
    evidence_indicators:
    - Data provenance fully documented
    - Historical context analyzed
    - Known biases documented
    - Mitigation strategies implemented
  - id: HISTBIAS-POS-002
    signal: Label debiasing techniques applied
    evidence_indicators:
    - Causal debiasing methods used
    - Human review incorporated
    - Alternative labels considered
    - Label limitations documented
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  preparation:
  - step: Gather training data documentation
  - step: Research historical discrimination in domain
  - step: Identify label creation process
  - step: Schedule interviews with data and domain experts
  steps:
  - id: '1'
    name: Data Provenance Analysis
    description: |
      Document the sources, collection methods, and time periods
      for all training data.
    duration_estimate: 1 hour
    questions:
    - Where did this data come from?
    - When was it collected?
    - How was it collected?
    - Who was included and excluded?
    expected_findings:
    - Complete data provenance
    - Time period documentation
  - id: '2'
    name: Historical Context Research
    description: |
      Research historical discrimination in the domain to understand
      what bias may exist in the data.
    duration_estimate: 1-2 hours
    questions:
    - What discrimination occurred in this domain historically?
    - What time periods were particularly problematic?
    - What data would have been affected?
    expected_findings:
    - Historical discrimination timeline
    - Data overlap with problematic periods
  - id: '3'
    name: Label Analysis
    description: |
      Evaluate how training labels were created and whether
      they may encode historical bias.
    duration_estimate: 1-2 hours
    questions:
    - How were labels created?
    - Do labels reflect biased historical decisions?
    - Do outcomes reflect access disparities?
    expected_findings:
    - Label creation process documentation
    - Historical bias risk assessment
  - id: '4'
    name: Expert Interviews
    description: |
      Interview domain and data experts to understand historical
      bias risks specific to this context.
    duration_estimate: 1.5 hours
    expected_findings:
    - Expert perspectives on historical bias
    - Context-specific risks identified
  - id: '5'
    name: Synthesize Findings
    description: |
      Compile historical bias assessment into prioritized
      recommendations.
    duration_estimate: 30 min
    expected_findings:
    - Historical bias risk assessment
    - Remediation recommendations
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: historical_bias_assessment
    format: structured
    description: Comprehensive assessment of historical bias in training data
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Data Provenance
    - Historical Context
    - Label Analysis
    - Identified Biases
    - Recommendations
  confidence_guidance:
    high: Documented historical discrimination, clear data overlap, expert consensus
    medium: Historical patterns suggest bias, partial documentation
    low: Theoretical bias risk without specific evidence
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: datasheets-for-datasets
      priority: required
    - source_id: fairmlbook-data
      priority: required
  degradation:
  - feature: Historical research
    impact: Limited access to historical context
    mitigation: Cache domain-specific historical analyses
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive historical analysis
    security:
      included: false
      reason: Not security-focused
    full:
      included: true
      priority: 80
closeout_checklist:
- id: histbias-001
  item: Data provenance documented
  level: CRITICAL
  verification: manual
  verification_notes: Reviewer confirms data sources, time periods, and collection methods documented
  expected: Confirmed by reviewer
- id: histbias-002
  item: Historical context analyzed
  level: CRITICAL
  verification: manual
  verification_notes: Historical discrimination in domain researched and documented
  expected: Confirmed by reviewer
- id: histbias-003
  item: Label creation process reviewed
  level: BLOCKING
  verification: manual
  verification_notes: Label provenance and potential bias assessed
  expected: Confirmed by reviewer
- id: histbias-004
  item: Remediation recommendations provided
  level: WARNING
  verification: manual
  verification_notes: Each identified bias has documented remediation path
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - all
    domains:
    - Machine Learning
    - Automated Decision Making
    team_sizes:
    - small
    - medium
    - large
  compliance_frameworks:
  - framework: EU AI Act
    controls:
    - Article 10 (data governance)
  - framework: NIST AI RMF
    controls:
    - MAP 2.3
    - MEASURE 2.6
relationships:
  commonly_combined:
  - responsible-design.algorithmic-fairness.bias-detection
  - responsible-design.algorithmic-fairness.disparate-impact
  - responsible-design.consent-transparency.data-usage-transparency
