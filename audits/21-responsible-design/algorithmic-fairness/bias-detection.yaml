audit:
  id: responsible-design.algorithmic-fairness.bias-detection
  name: Bias Detection Audit
  version: 1.0.0
  last_updated: '2025-01-19'
  status: active
  category: ethical-societal
  category_number: 21
  subcategory: algorithmic-fairness
  tier: phd
  estimated_duration: 4-8 hours  # median: 6h
  completeness: requires_discovery
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: critical
  scope: qualitative
  default_profiles:
  - full
  - security
  blocks_phase: true
  parallelizable: false
description:
  what: |
    Assesses algorithmic systems for potential bias against protected groups
    and marginalized populations. Evaluates whether AI/ML models, recommendation
    systems, scoring algorithms, and automated decision-making processes
    produce discriminatory outcomes based on race, gender, age, disability,
    religion, national origin, or other protected characteristics.

    Examines training data composition, feature selection, model outputs,
    and downstream impacts across demographic groups.
  why_it_matters: |
    Algorithmic bias can cause significant harm to individuals and communities:
    - Discriminatory hiring decisions affecting employment opportunities
    - Biased loan/credit decisions perpetuating economic inequality
    - Healthcare algorithms that underserve minority populations
    - Criminal justice systems with disparate racial impacts
    - Housing algorithms that enforce digital redlining

    Beyond ethical concerns, algorithmic bias exposes organizations to:
    - Legal liability under civil rights laws
    - Regulatory penalties (EU AI Act, ECOA, Fair Housing Act)
    - Reputational damage and loss of public trust
    - Class action lawsuits from affected populations
  when_to_run:
  - Before deploying any ML model affecting people
  - When expanding algorithms to new populations
  - After incidents revealing potential bias
  - Quarterly for production ML systems
  - When regulatory requirements mandate bias audits
prerequisites:
  required_artifacts:
  - type: model_documentation
    description: Documentation of ML models including intended use, training data, and features
  - type: training_data_access
    description: Access to training datasets or representative samples with demographic information
  - type: model_access
    description: Ability to query models or access prediction outputs
  optional_artifacts:
  - type: historical_decisions
    description: Historical data of algorithmic decisions and outcomes
  - type: demographic_data
    description: Demographic breakdown of affected populations
  access_requirements:
  - Access to ML model inference APIs or model artifacts
  - Read access to training data repositories
  - Access to outcome/impact data if available
  - Ability to conduct interviews with data scientists and ML engineers
discovery:
  interviews:
  - role: ML/Data Science Team Lead
    questions:
    - What protected characteristics exist in your training data?
    - How do you currently test for bias before deployment?
    - Have you observed disparate outcomes across demographic groups?
    - What fairness metrics do you track in production?
    purpose: Understand current bias awareness and mitigation practices
    duration: 45 min
  - role: Product Manager
    questions:
    - Who are the populations affected by this algorithm?
    - Have you received complaints about unfair treatment?
    - What is the impact of incorrect predictions on users?
    - How are algorithmic decisions explained to users?
    purpose: Identify user-facing impacts and feedback patterns
    duration: 30 min
  - role: Legal/Compliance Team
    questions:
    - What regulations govern this algorithmic decision-making?
    - Have there been any bias-related incidents or complaints?
    - What documentation is required for regulatory compliance?
    purpose: Understand regulatory requirements and incident history
    duration: 30 min
  documents_to_review:
  - type: Model Cards
    purpose: Review standardized model documentation for bias disclosures
    look_for:
    - Intended use and out-of-scope uses
    - Training data composition and demographic breakdown
    - Evaluation results across demographic groups
    - Known limitations and failure modes
  - type: Data Sheets
    purpose: Assess training data provenance and composition
    look_for:
    - Data collection methodology
    - Demographic representation in training data
    - Historical biases embedded in labels
    - Data cleaning and preprocessing decisions
  - type: Fairness Testing Reports
    purpose: Review existing bias testing results
    look_for:
    - Metrics used (demographic parity, equal opportunity, etc.)
    - Performance gaps across groups
    - Intersectional analysis results
  metrics_queries:
  - system: ML Model Monitoring
    query: Prediction distribution by demographic group over time
    purpose: Identify disparate prediction rates
    threshold: Ratio between groups should be > 0.8 (80% rule)
  - system: Outcome Tracking
    query: True positive/negative rates by demographic group
    purpose: Identify disparate error rates
    threshold: Error rate ratio should be within 20% across groups
  file_patterns:
  - glob: '**/ETHICS.md'
    purpose: Ethics documentation
  - glob: '**/PRIVACY*.md'
    purpose: Privacy documentation
  - glob: '**/terms*.md'
    purpose: Terms of service
  - glob: '**/policy*.md'
    purpose: Policy documentation
  code_patterns:
  - pattern: consent|privacy|gdpr|ccpa|cookie
    type: regex
    scope: all
    purpose: Privacy/consent references
knowledge_sources:
  specifications:
  - id: eu-ai-act
    name: EU Artificial Intelligence Act
    url: https://eur-lex.europa.eu/eli/reg/2024/1689/oj
    offline_cache: true
    priority: required
  - id: nist-ai-rmf
    name: NIST AI Risk Management Framework
    url: https://www.nist.gov/itl/ai-risk-management-framework
    offline_cache: true
    priority: required
  - id: ieee-algorithmic-bias
    name: IEEE Standard on Algorithmic Bias
    url: https://standards.ieee.org/standard/7003-2021.html
    offline_cache: true
    priority: recommended
  guides:
  - id: brookings-bias-mitigation
    name: 'Brookings: Algorithmic Bias Detection and Mitigation'
    url: https://www.brookings.edu/articles/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/
    offline_cache: true
    priority: recommended
  papers:
  - id: fairmlbook
    title: 'Fairness and Machine Learning: Limitations and Opportunities'
    url: https://fairmlbook.org/
  - id: model-cards
    title: Model Cards for Model Reporting
    url: https://arxiv.org/abs/1810.03993
  learning_resources:
  - id: google-mlcc-fairness
    title: Google ML Crash Course - Fairness
    type: course
    reference: https://developers.google.com/machine-learning/crash-course/fairness
    relevance: Practical introduction to fairness in ML
tooling:
  assessment_tools:
  - tool: Aequitas
    purpose: Open source bias and fairness audit toolkit
    install: pip install aequitas
    description: |
      Provides bias metrics, disparity analysis, and fairness
      evaluation across protected groups.
    offline_capable: true
  - tool: What-If Tool (WIT)
    purpose: Visual interface for ML model fairness exploration
    install: pip install witwidget
    description: |
      Graphical tool for probing ML model behavior across
      demographic groups and identifying bias patterns.
    offline_capable: true
  - tool: Fairlearn
    purpose: Microsoft's fairness assessment and mitigation toolkit
    install: pip install fairlearn
    description: |
      Comprehensive library for assessing and improving
      fairness of ML systems.
    offline_capable: true
  - tool: AI Fairness 360 (AIF360)
    purpose: IBM's comprehensive bias detection toolkit
    install: pip install aif360
    description: |
      70+ fairness metrics, bias mitigation algorithms,
      and explanation capabilities.
    offline_capable: true
  infrastructure_tools:
  - tool: SHAP
    purpose: Explain model predictions and identify feature bias
    install: pip install shap
    command: shap.summary_plot(shap_values, X)
    offline_capable: true
  - tool: LIME
    purpose: Local interpretable model explanations
    install: pip install lime
    offline_capable: true
signals:
  critical:
  - id: BIAS-CRIT-001
    signal: Significant disparate impact on protected group
    evidence_indicators:
    - Selection rate for protected group is less than 80% of majority group
    - Four-fifths rule violation in hiring or lending algorithms
    - Statistical significance in outcome disparities (p < 0.05)
    - Adverse impact ratio below 0.8 for any protected group
    explanation: |
      Disparate impact occurs when a facially neutral policy or algorithm
      disproportionately affects members of protected groups. The 80% rule
      (four-fifths rule) from EEOC guidelines is a common threshold.
      Significant disparate impact creates legal liability and real harm.
    remediation: |
      - Immediately investigate root causes in training data and features
      - Consider removing or transforming proxy variables
      - Apply bias mitigation techniques (reweighting, resampling)
      - Implement fairness constraints in model training
      - Consider alternative models with better fairness-accuracy tradeoffs
      - Document remediation efforts for regulatory compliance
  - id: BIAS-CRIT-002
    signal: Algorithm uses protected characteristics directly
    evidence_indicators:
    - Race, gender, religion used as direct features
    - Explicit demographic variables in model inputs
    - Protected characteristics accessible to model
    explanation: |
      Direct use of protected characteristics in algorithmic decisions
      is illegal in many contexts (credit, housing, employment) and
      ethically problematic in most others.
    remediation: |
      - Remove protected characteristics from model features
      - Audit for highly correlated proxy variables
      - Implement access controls preventing demographic data in models
      - Document legitimate use cases if legally permitted
  high:
  - id: BIAS-HIGH-001
    signal: Proxy variables correlate strongly with protected characteristics
    evidence_indicators:
    - Zip code highly correlated with race (correlation > 0.7)
    - Name features that encode ethnicity or gender
    - Credit score as proxy for race in non-lending contexts
    - Features that encode historical discrimination
    explanation: |
      Even without explicit protected characteristics, algorithms can
      discriminate through proxy variables that correlate with race,
      gender, or other protected attributes.
    remediation: |
      - Audit feature correlations with protected characteristics
      - Consider removing or transforming proxy variables
      - Use causal modeling to identify discriminatory pathways
      - Apply fair representation learning techniques
  - id: BIAS-HIGH-002
    signal: Training data underrepresents protected groups
    evidence_indicators:
    - Protected group comprises <10% of training data vs >20% of population
    - Model performance significantly worse for minority groups
    - Data collection excluded certain demographic groups
    explanation: |
      Underrepresentation in training data leads to models that
      perform poorly for minority groups, perpetuating exclusion
      and producing discriminatory outcomes.
    remediation: |
      - Collect additional representative training data
      - Apply resampling or reweighting techniques
      - Use synthetic data augmentation for underrepresented groups
      - Evaluate and report performance by demographic group
  - id: BIAS-HIGH-003
    signal: Labels embed historical discrimination
    evidence_indicators:
    - Training labels derived from historically biased decisions
    - Human-labeled data from potentially biased annotators
    - Outcome labels reflect systemic discrimination (e.g., arrests vs. actual crime)
    explanation: |
      When training labels reflect historical discrimination, models
      learn to perpetuate and amplify those biases. This is particularly
      problematic in criminal justice, lending, and hiring contexts.
    remediation: |
      - Critically examine label provenance and potential biases
      - Consider alternative label definitions
      - Use causal inference to debias labels
      - Implement human review for high-stakes decisions
  medium:
  - id: BIAS-MED-001
    signal: No fairness testing before deployment
    evidence_indicators:
    - Model documentation lacks fairness evaluation
    - No demographic breakdown of model performance
    - Fairness metrics not tracked in production
    remediation: |
      - Implement fairness testing in ML pipeline
      - Add demographic performance metrics to model evaluation
      - Establish fairness criteria as deployment gates
      - Track fairness metrics in production monitoring
  - id: BIAS-MED-002
    signal: Insufficient intersectional analysis
    evidence_indicators:
    - Only single-axis fairness analysis (race OR gender)
    - No evaluation for intersectional groups (e.g., Black women)
    - Small sample sizes for intersectional groups
    remediation: |
      - Extend fairness analysis to intersectional groups
      - Collect sufficient data for intersectional evaluation
      - Report confidence intervals for small group analyses
  positive:
  - id: BIAS-POS-001
    signal: Comprehensive fairness testing framework in place
    evidence_indicators:
    - Multiple fairness metrics evaluated pre-deployment
    - Model cards document demographic performance
    - Fairness criteria as deployment gates
    - Regular fairness audits of production models
  - id: BIAS-POS-002
    signal: Proactive bias mitigation practices
    evidence_indicators:
    - Diverse training data collection strategies
    - Bias mitigation techniques applied during training
    - Human review for high-stakes decisions
    - Feedback mechanisms for affected users
procedure:
  context:
    cognitive_mode: evaluative
    ensemble_role: auditor
  preparation:
  - step: Gather model documentation (model cards, datasheets)
  - step: Identify protected characteristics relevant to the domain
  - step: Obtain access to training data or representative samples
  - step: Schedule interviews with ML team and stakeholders
  - step: Set up bias detection tooling (Aequitas, Fairlearn, etc.)
  steps:
  - id: '1'
    name: Document Review
    description: |
      Review existing model documentation for bias-related information.
      Assess whether fairness considerations are documented and what
      testing has been performed.
    duration_estimate: 1-2 hours
    questions:
    - What fairness metrics are documented?
    - Is demographic performance broken down?
    - What limitations are disclosed?
    - What bias mitigation was applied?
    expected_findings:
    - Current state of fairness documentation
    - Gaps in bias testing and reporting
  - id: '2'
    name: Training Data Analysis
    description: |
      Analyze training data composition for demographic representation
      and potential sources of bias. Examine labels for historical
      discrimination patterns.
    duration_estimate: 2-3 hours
    questions:
    - What is the demographic composition of training data?
    - Are protected groups adequately represented?
    - What is the provenance of training labels?
    - Are there historical biases encoded in labels?
    expected_findings:
    - Demographic breakdown of training data
    - Potential sources of bias in data and labels
  - id: '3'
    name: Feature Analysis
    description: |
      Audit model features for direct use of protected characteristics
      or proxy variables that correlate with protected attributes.
    duration_estimate: 1-2 hours
    commands:
    - purpose: Check for protected characteristic features
      command: Review model feature list and config files
    - purpose: Calculate proxy correlations
      command: Compute correlation matrix between features and demographic variables
    expected_findings:
    - List of potentially problematic features
    - Proxy variable correlation analysis
  - id: '4'
    name: Quantitative Bias Testing
    description: |
      Run comprehensive bias metrics using fairness toolkits.
      Calculate disparate impact ratios, equalized odds,
      calibration by group.
    duration_estimate: 2-3 hours
    commands:
    - purpose: Calculate demographic parity
      command: Compute selection rate by protected group
    - purpose: Calculate equalized odds
      command: Compute TPR and FPR by protected group
    - purpose: Calculate calibration
      command: Compute predicted probability vs. outcome by group
    expected_findings:
    - Fairness metrics across protected groups
    - Specific disparities identified
  - id: '5'
    name: Stakeholder Interviews
    description: |
      Interview ML team, product managers, and compliance to understand
      current practices, known issues, and regulatory requirements.
    duration_estimate: 2-3 hours
    questions:
    - What bias testing is currently performed?
    - Have disparate outcomes been observed?
    - What regulatory requirements apply?
    expected_findings:
    - Current organizational practices
    - Known issues and incident history
  - id: '6'
    name: Synthesize Findings
    description: |
      Compile quantitative and qualitative findings into actionable
      recommendations with severity ratings and remediation guidance.
    duration_estimate: 1 hour
    expected_findings:
    - Prioritized list of bias findings
    - Remediation recommendations
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: bias_metrics_report
    format: structured
    description: Quantitative fairness metrics by protected group
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Data and Feature Analysis
    - Quantitative Fairness Metrics
    - Critical Findings
    - Recommendations
    - Regulatory Implications
  confidence_guidance:
    high: Statistical analysis confirms disparity with p < 0.05, multiple methods agree
    medium: Metrics indicate potential bias but sample sizes limit confidence
    low: Circumstantial indicators, insufficient data for statistical conclusions
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: nist-ai-rmf
      priority: required
    - source_id: brookings-bias-mitigation
      priority: recommended
  degradation:
  - feature: External regulatory reference lookup
    impact: Cannot verify current regulatory requirements
    mitigation: Cache key regulatory documents before offline work
profiles:
  membership:
    quick:
      included: false
      reason: Requires comprehensive analysis - cannot be abbreviated
    security:
      included: true
      priority: 70
      reason: Bias creates legal and reputational security risks
    production:
      included: false
      reason: Pre-production audit focus
    full:
      included: true
      priority: 90
closeout_checklist:
- id: bias-001
  item: Training data demographic composition documented
  level: CRITICAL
  verification: manual
  verification_notes: Reviewer confirms demographic breakdown exists for all protected groups relevant
    to domain
  expected: Confirmed by reviewer
- id: bias-002
  item: Disparate impact analysis completed
  level: CRITICAL
  verification: manual
  verification_notes: Reviewer confirms four-fifths rule analysis exists for all protected groups
  expected: Confirmed by reviewer
- id: bias-003
  item: Feature audit for protected characteristics completed
  level: BLOCKING
  verification: manual
  verification_notes: Reviewer confirms feature list audited for direct and proxy use of protected attributes
  expected: Confirmed by reviewer
- id: bias-004
  item: ML team interview completed
  level: BLOCKING
  verification: manual
  verification_notes: Interview notes exist for ML/data science team lead
  expected: Confirmed by reviewer
- id: bias-005
  item: Bias metrics report generated
  level: BLOCKING
  verification: manual
  verification_notes: Quantitative fairness metrics document exists with results by protected group
  expected: Confirmed by reviewer
- id: bias-006
  item: Remediation recommendations documented
  level: WARNING
  verification: manual
  verification_notes: For each critical/high finding, remediation path is documented
  expected: Confirmed by reviewer
governance:
  applicable_to:
    archetypes:
    - all
    domains:
    - Machine Learning
    - Automated Decision Making
    - Recommendation Systems
    - Scoring/Rating Systems
    team_sizes:
    - small
    - medium
    - large
  compliance_frameworks:
  - framework: EU AI Act
    controls:
    - Article 9
    - Article 10
    - Annex IV
  - framework: ECOA
    controls:
    - Fair lending requirements
  - framework: Fair Housing Act
    controls:
    - Disparate impact provisions
  - framework: EEOC Guidelines
    controls:
    - Four-fifths rule
    - Uniform Guidelines
  - framework: NIST AI RMF
    controls:
    - MAP 1.5
    - MEASURE 2.6
    - MANAGE 3.1
relationships:
  commonly_combined:
  - responsible-design.algorithmic-fairness.disparate-impact
  - responsible-design.algorithmic-fairness.fairness-metric-selection
  - responsible-design.consent-transparency.algorithm-explainability
