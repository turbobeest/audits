# ============================================================
# AUDIT: Benchmark Testing
# ============================================================
# Evaluates performance benchmarking practices to measure and
# compare system performance against defined standards.
# ============================================================

audit:
  id: "testing-quality-assurance.performance-testing.benchmark-testing"
  name: "Benchmark Testing"
  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 10
  subcategory: "performance-testing"

  tier: "expert"
  estimated_duration: "3 hours"

  completeness: "complete"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "testing"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Assesses performance benchmarking practices including microbenchmarks,
    macro benchmarks, and comparative analysis. Reviews benchmark methodology,
    statistical rigor, reproducibility, and proper interpretation of results.
    Evaluates whether benchmarks accurately represent production workloads
    and enable meaningful performance comparisons.

  why_it_matters: |
    Poor benchmarking leads to incorrect performance conclusions, wasted
    optimization effort, and missed actual bottlenecks. Without statistically
    valid benchmarks, teams cannot reliably compare alternatives, detect
    regressions, or validate optimizations. This wastes engineering time
    and may degrade production performance.

  when_to_run:
    - "During performance optimization efforts"
    - "When evaluating technology alternatives"
    - "Before and after major refactoring"
    - "Regular performance validation cycles"

prerequisites:
  required_artifacts:
    - type: "benchmark-suite"
      description: "Benchmark test implementations"
    - type: "performance-targets"
      description: "Defined performance goals and thresholds"

  access_requirements:
    - "Isolated benchmark environment"
    - "Historical benchmark results"
    - "Profiling tools access"

discovery:
  file_patterns:
    - glob: "**/benchmark/**"
      purpose: "Benchmark test directories"
    - glob: "**/bench/**"
      purpose: "Benchmark shorthand directories"
    - glob: "**/*benchmark*.{js,ts,py,java,go,rs}"
      purpose: "Benchmark test files"
    - glob: "**/perf/**"
      purpose: "Performance test directories"

  code_patterns:
    - pattern: "@Benchmark|benchmark\\(|bench_"
      type: "regex"
      scope: "source"
      purpose: "Benchmark annotations and functions"
    - pattern: "BenchmarkDotNet|JMH|hyperfine|criterion"
      type: "keyword"
      scope: "source"
      purpose: "Benchmark framework usage"
    - pattern: "iterations|warmup|samples"
      type: "keyword"
      scope: "config"
      purpose: "Benchmark configuration parameters"

knowledge_sources:
  guides:
    - id: "jmh-docs"
      name: "Java Microbenchmark Harness Documentation"
      url: "https://openjdk.java.net/projects/code-tools/jmh/"
      offline_cache: true
    - id: "criterion-docs"
      name: "Criterion.rs Benchmarking Guide"
      url: "https://bheisler.github.io/criterion.rs/book/"
      offline_cache: true

  papers:
    - id: "benchmark-methodology"
      title: "How Not to Measure Latency"
      url: "https://www.youtube.com/watch?v=lJ8ydIuPFeU"

  learning_resources:
    - id: "perf-analysis"
      title: "Systems Performance: Enterprise and the Cloud"
      type: "book"
      reference: "Brendan Gregg, Pearson"

tooling:
  static_analysis:
    - tool: "JMH"
      purpose: "Java microbenchmarking"
      offline_capable: true
    - tool: "BenchmarkDotNet"
      purpose: ".NET benchmarking"
      offline_capable: true
    - tool: "criterion"
      purpose: "Rust benchmarking"
      offline_capable: true
    - tool: "hyperfine"
      purpose: "Command-line benchmarking"
      offline_capable: true

  monitoring_queries:
    - system: "Custom"
      query: "benchmark_duration_ns / iterations"
      purpose: "Per-iteration timing"
    - system: "Custom"
      query: "stddev(benchmark_results) / mean(benchmark_results)"
      purpose: "Coefficient of variation"

signals:
  critical:
    - id: "BENCH-CRIT-001"
      signal: "No benchmarking infrastructure exists"
      evidence_pattern: "Absence of benchmark directories or benchmark test files"
      explanation: |
        Without benchmarks, performance claims are unsubstantiated. Teams cannot
        detect regressions, validate optimizations, or make informed technology
        choices. Performance becomes guesswork.
      remediation: "Implement benchmark suite for critical code paths and algorithms"

    - id: "BENCH-CRIT-002"
      signal: "Benchmarks lack warmup periods"
      evidence_pattern: "Missing warmup configuration in benchmark setup"
      explanation: |
        JIT compilation, caching, and lazy initialization skew results without
        proper warmup. Cold-start measurements don't reflect steady-state
        performance, leading to incorrect conclusions.
      remediation: "Configure appropriate warmup iterations for all benchmarks"

  high:
    - id: "BENCH-HIGH-001"
      signal: "Benchmarks use insufficient iterations"
      evidence_pattern: "Single iteration or very low iteration counts"
      explanation: |
        Single measurements have high variance and are unreliable. Statistical
        validity requires multiple iterations to account for noise and calculate
        confidence intervals.
      remediation: "Use statistically significant iteration counts with variance analysis"

    - id: "BENCH-HIGH-002"
      signal: "Dead code elimination not prevented"
      evidence_pattern: "Benchmark results unused, JIT may eliminate computation"
      explanation: |
        Compilers optimize away unused computations. If benchmark results aren't
        consumed, the measured code may not actually execute, producing
        meaningless timing results.
      remediation: "Ensure benchmark results are consumed using blackhole/doNotOptimize patterns"

  medium:
    - id: "BENCH-MED-001"
      signal: "Benchmark environment not controlled"
      evidence_pattern: "No environment isolation or configuration"
      remediation: "Run benchmarks in isolated, controlled environments with fixed configurations"

    - id: "BENCH-MED-002"
      signal: "Results not compared against baselines"
      evidence_pattern: "Benchmarks run without baseline comparison"
      remediation: "Store baseline results and automatically compare new runs"

    - id: "BENCH-MED-003"
      signal: "Missing statistical analysis"
      evidence_pattern: "No confidence intervals, variance, or outlier handling"
      remediation: "Include statistical analysis with confidence intervals in benchmark reports"

  low:
    - id: "BENCH-LOW-001"
      signal: "Benchmark results not version controlled"
      remediation: "Store benchmark results alongside code for historical tracking"

  positive:
    - id: "BENCH-POS-001"
      signal: "Comprehensive benchmark suite with proper methodology"
    - id: "BENCH-POS-002"
      signal: "Statistical rigor with confidence intervals and variance analysis"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Identify benchmark infrastructure"
      description: |
        Search for benchmark suites, performance tests, and benchmarking
        framework usage.
      duration_estimate: "25 min"
      commands:
        - purpose: "Find benchmark directories and files"
          command: "find . -type f -name '*benchmark*' -o -type d -name 'bench*' 2>/dev/null"
        - purpose: "Check for benchmark frameworks"
          command: "grep -r 'JMH\\|BenchmarkDotNet\\|criterion\\|hyperfine\\|@Benchmark' . 2>/dev/null | head -20"
      expected_findings:
        - "Benchmark test files"
        - "Framework configurations"

    - id: "2"
      name: "Review benchmark methodology"
      description: |
        Examine benchmark configurations for proper warmup, iteration counts,
        and statistical rigor.
      duration_estimate: "40 min"
      commands:
        - purpose: "Check for warmup configuration"
          command: "grep -r 'warmup\\|Warmup\\|warm_up' --include='*.java' --include='*.rs' --include='*.py' 2>/dev/null"
        - purpose: "Check iteration configuration"
          command: "grep -r 'iterations\\|Iterations\\|samples' --include='*.java' --include='*.rs' --include='*.py' 2>/dev/null"
      expected_findings:
        - "Warmup period configuration"
        - "Iteration count settings"

    - id: "3"
      name: "Verify result consumption"
      description: |
        Check that benchmark results are properly consumed to prevent
        dead code elimination.
      duration_estimate: "30 min"
      commands:
        - purpose: "Check for blackhole/consume patterns"
          command: "grep -r 'Blackhole\\|blackhole\\|doNotOptimize\\|consume' --include='*.java' --include='*.rs' 2>/dev/null"
      expected_findings:
        - "Result consumption patterns"
        - "Dead code elimination prevention"

    - id: "4"
      name: "Review statistical analysis"
      description: |
        Verify benchmarks include proper statistical analysis with
        confidence intervals and variance handling.
      duration_estimate: "25 min"
      commands:
        - purpose: "Check for statistical metrics"
          command: "grep -r 'confidence\\|stddev\\|variance\\|percentile' --include='*.java' --include='*.rs' --include='*.py' 2>/dev/null"
      expected_findings:
        - "Statistical analysis configuration"
        - "Confidence interval reporting"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"
    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Benchmark Methodology Assessment"
        - "Statistical Validity Analysis"
        - "Recommendations"

  confidence_guidance:
    high: "Benchmarks reviewed, methodology verified, results validated"
    medium: "Configuration reviewed but execution not observed"
    low: "Limited visibility into benchmarking practices"

offline:
  capability: "full"
  cache_manifest:
    knowledge:
      - source_id: "jmh-docs"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires detailed methodology analysis"
    full:
      included: true
      priority: 3

closeout_checklist:
  - id: "benchmark-testing-001"
    item: "Benchmark suite exists"
    level: "CRITICAL"
    verification: "find . -type f -name '*benchmark*' -o -type d -name 'bench*' 2>/dev/null | head -1 | grep -q . && echo PASS || echo FAIL"
    expected: "PASS"

  - id: "benchmark-testing-002"
    item: "Benchmarks include warmup periods"
    level: "BLOCKING"
    verification: "grep -r 'warmup\\|Warmup' --include='*.java' --include='*.rs' 2>/dev/null | grep -q . && echo PASS || echo FAIL"
    expected: "PASS"

  - id: "benchmark-testing-003"
    item: "Results properly consumed to prevent DCE"
    level: "WARNING"
    verification: "grep -r 'Blackhole\\|doNotOptimize\\|consume' 2>/dev/null | grep -q . && echo PASS || echo FAIL"
    expected: "PASS"

governance:
  applicable_to:
    archetypes: ["library", "api-service", "performance-critical"]

  compliance_frameworks:
    - framework: "Internal"
      controls: ["PERF-001"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.performance-testing.performance-baseline"
    - "testing-quality-assurance.performance-testing.performance-regression"
