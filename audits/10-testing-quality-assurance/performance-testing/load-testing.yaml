audit:
  id: testing-quality-assurance.performance-testing.load-testing
  name: Load Testing
  version: 1.0.0
  last_updated: '2026-01-19'
  status: active
  category: testing-quality-assurance
  category_number: 10
  subcategory: performance-testing
  tier: expert
  estimated_duration: 4 hours
  completeness: complete
  requires_runtime: true
  destructive: false
execution:
  automatable: partial
  severity: high
  scope: testing
  default_profiles:
  - full
  - production
  blocks_phase: false
  parallelizable: true
description:
  what: |
    Evaluates load testing practices and infrastructure to ensure the system
    can handle expected traffic volumes. Reviews load testing tools configuration,
    test scenarios, baseline measurements, and performance metrics collection.
    Examines whether realistic load patterns are simulated and results properly
    analyzed.
  why_it_matters: |
    Inadequate load testing leads to production outages during traffic spikes,
    degraded user experience, and missed SLAs. Systems that haven't been
    properly load tested often fail at critical business moments like product
    launches, marketing campaigns, or seasonal peaks. This costs revenue,
    damages reputation, and erodes customer trust.
  when_to_run:
  - Before major releases or launches
  - After significant architecture changes
  - Quarterly performance validation
  - When onboarding new critical services
prerequisites:
  required_artifacts:
  - type: load-test-configuration
    description: Load testing tool configurations (k6, JMeter, Gatling, Locust)
  - type: performance-requirements
    description: Documented performance SLAs and requirements
  access_requirements:
  - Load testing environment access
  - Monitoring and metrics dashboards
  - Historical performance data
discovery:
  file_patterns:
  - glob: '**/k6/**/*.js'
    purpose: k6 load test scripts
  - glob: '**/locust/**/*.py'
    purpose: Locust load test definitions
  - glob: '**/*.jmx'
    purpose: JMeter test plans
  - glob: '**/gatling/**/*.scala'
    purpose: Gatling simulation files
  - glob: '**/load-test*/**'
    purpose: Load testing directories
  - glob: '**/performance-test*/**'
    purpose: Performance test configurations
  code_patterns:
  - pattern: vus|virtualUsers|concurrent
    type: keyword
    scope: config
    purpose: Virtual user configuration
  - pattern: ramp-up|rampUp|stages
    type: keyword
    scope: config
    purpose: Load ramping patterns
  - pattern: threshold|sla|target
    type: keyword
    scope: config
    purpose: Performance thresholds
knowledge_sources:
  specifications:
  - id: rfc7231
    name: HTTP/1.1 Semantics and Content
    url: https://tools.ietf.org/html/rfc7231
    offline_cache: true
    priority: recommended
  guides:
  - id: k6-docs
    name: k6 Load Testing Documentation
    url: https://k6.io/docs/
    offline_cache: true
  - id: gatling-docs
    name: Gatling Performance Testing Guide
    url: https://gatling.io/docs/
    offline_cache: true
  learning_resources:
  - id: perf-testing-book
    title: Performance Testing Guidance for Web Applications
    type: book
    reference: Microsoft patterns & practices
tooling:
  static_analysis:
  - tool: k6
    purpose: Modern load testing with JavaScript
    offline_capable: true
  - tool: JMeter
    purpose: Apache load testing framework
    offline_capable: true
  - tool: Gatling
    purpose: Scala-based load testing
    offline_capable: true
  - tool: Locust
    purpose: Python-based distributed load testing
    offline_capable: true
  monitoring_queries:
  - system: Prometheus
    query: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
    purpose: P95 latency during load tests
  - system: Prometheus
    query: rate(http_requests_total[5m])
    purpose: Request rate throughput
signals:
  critical:
  - id: LOAD-CRIT-001
    signal: No load testing infrastructure exists
    explanation: |
      Without load testing infrastructure, there is no way to validate system
      performance before production deployment. This creates blind spots for
      capacity planning and risks major outages during traffic spikes.
    remediation: Implement load testing framework with baseline scenarios for critical paths
    evidence_description: Absence of load test files (*.jmx, k6/*.js, locust/*.py)
  - id: LOAD-CRIT-002
    signal: Load tests never executed in CI/CD pipeline
    evidence_pattern: No load test execution in CI configuration files
    explanation: |
      Load tests that exist but are never run provide false confidence.
      Performance regressions go undetected until they impact production users.
    remediation: Integrate load tests into deployment pipeline with automated execution
  high:
  - id: LOAD-HIGH-001
    signal: No performance thresholds defined
    evidence_pattern: Load test scripts without threshold/assert/check statements
    explanation: |
      Load tests without pass/fail criteria cannot detect regressions.
      Tests may run but never flag performance degradation.
    remediation: Define explicit thresholds for latency percentiles, error rates, and throughput
  - id: LOAD-HIGH-002
    signal: Load tests use unrealistic traffic patterns
    evidence_pattern: Constant load without ramp-up, think time, or variation
    explanation: |
      Unrealistic load patterns don't expose issues that occur under real traffic.
      Missing think times, ramp-up periods, or traffic variations miss edge cases.
    remediation: Model realistic user behavior with think times, ramp patterns, and traffic variation
  - id: LOAD-HIGH-003
    signal: Production traffic volume not reflected in tests
    evidence_pattern: VU counts significantly below production concurrent users
    explanation: |
      Testing at 10% of production capacity won't reveal scaling bottlenecks.
      Performance may be acceptable at test scale but fail at production scale.
    remediation: Scale load tests to match or exceed peak production traffic
  medium:
  - id: LOAD-MED-001
    signal: Load test environment differs significantly from production
    evidence_pattern: Smaller instance types, missing components in test environment
    remediation: Create production-like test environment or use production with traffic shadowing
  - id: LOAD-MED-002
    signal: No baseline performance metrics documented
    evidence_pattern: Missing performance baseline documentation
    remediation: Establish and document baseline performance metrics for comparison
  low:
  - id: LOAD-LOW-001
    signal: Load test results not archived for trend analysis
    remediation: Store load test results with timestamps for historical comparison
  positive:
  - id: LOAD-POS-001
    signal: Comprehensive load testing with realistic scenarios and thresholds
  - id: LOAD-POS-002
    signal: Automated load tests integrated into CI/CD pipeline
procedure:
  context:
    cognitive_mode: critical
    ensemble_role: auditor
  steps:
  - id: '1'
    name: Identify load testing infrastructure
    description: |
      Search for load testing tools and configurations in the codebase.
      Look for k6, JMeter, Gatling, Locust, or custom load testing solutions.
    duration_estimate: 30 min
    commands:
    - purpose: Find load testing configuration files
      command: find . -type f \( -name '*.jmx' -o -name 'k6*.js' -o -path '*locust*' -o -path '*gatling*'
        \) 2>/dev/null
    - purpose: Check for load test npm scripts
      command: grep -r 'load-test\|loadtest\|k6\|artillery' package.json 2>/dev/null
    expected_findings:
    - Load testing tool configuration files
    - Test scenario definitions
  - id: '2'
    name: Review load test scenarios
    description: |
      Examine load test scripts for realistic user journeys, proper ramping,
      think times, and representative traffic patterns.
    duration_estimate: 45 min
    commands:
    - purpose: Check for virtual user configuration
      command: grep -r 'vus\|virtual.*users\|concurrent' --include='*.js' --include='*.py' --include='*.scala'
        2>/dev/null
    - purpose: Look for ramping patterns
      command: grep -r 'ramp\|stages\|duration' --include='*.js' --include='*.py' 2>/dev/null
    expected_findings:
    - Virtual user counts
    - Ramp-up configurations
    - Test duration settings
  - id: '3'
    name: Verify performance thresholds
    description: |
      Check that load tests define explicit pass/fail criteria based on
      latency, throughput, and error rate requirements.
    duration_estimate: 30 min
    commands:
    - purpose: Find threshold definitions
      command: grep -r 'threshold\|assert\|check\|expect' --include='*.js' --include='*.py' 2>/dev/null
        | head -50
    expected_findings:
    - Defined performance thresholds
    - SLA-based pass/fail criteria
  - id: '4'
    name: Check CI/CD integration
    description: |
      Verify load tests are integrated into the deployment pipeline
      and executed automatically.
    duration_estimate: 20 min
    commands:
    - purpose: Check GitHub Actions for load tests
      command: grep -r 'k6\|load.*test\|performance.*test' .github/workflows/ 2>/dev/null
    - purpose: Check other CI configs
      command: grep -r 'k6\|load.*test' .gitlab-ci.yml Jenkinsfile .circleci/ 2>/dev/null
    expected_findings:
    - CI pipeline load test stages
    - Automated performance gates
output:
  deliverables:
  - type: finding_list
    format: structured
  - type: summary
    format: prose
    sections:
    - Executive Summary
    - Load Testing Coverage Analysis
    - Threshold and SLA Compliance
    - Recommendations
  confidence_guidance:
    high: Load test configurations reviewed, execution verified, results analyzed
    medium: Configuration reviewed but execution not observed
    low: Limited visibility into load testing practices
offline:
  capability: partial
  cache_manifest:
    knowledge:
    - source_id: k6-docs
      priority: recommended
profiles:
  membership:
    quick:
      included: false
      reason: Requires detailed analysis of test configurations and results
    full:
      included: true
      priority: 2
    production:
      included: true
      priority: 1
closeout_checklist:
- id: load-testing-001
  item: Load testing infrastructure exists
  level: CRITICAL
  verification: find . -type f \( -name '*.jmx' -o -name 'k6*.js' -o -path '*locust*' \) 2>/dev/null |
    head -1 | grep -q . && echo PASS || echo FAIL
  expected: PASS
- id: load-testing-002
  item: Performance thresholds are defined
  level: BLOCKING
  verification: grep -r 'threshold' --include='*.js' --include='*.py' 2>/dev/null | grep -q . && echo
    PASS || echo FAIL
  expected: PASS
- id: load-testing-003
  item: Load tests integrated into CI/CD
  level: WARNING
  verification: grep -r 'load.*test\|k6\|performance' .github/workflows/ .gitlab-ci.yml 2>/dev/null |
    grep -q . && echo PASS || echo FAIL
  expected: PASS
governance:
  applicable_to:
    archetypes:
    - web-application
    - api-service
    - microservices
  compliance_frameworks:
  - framework: SOC2
    controls:
    - CC7.1
    - CC7.2
relationships:
  commonly_combined:
  - testing-quality-assurance.performance-testing.stress-testing
  - testing-quality-assurance.performance-testing.performance-baseline
  - testing-quality-assurance.performance-testing.scalability-testing
