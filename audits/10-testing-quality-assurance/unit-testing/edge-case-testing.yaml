# ============================================================
# AUDIT: Edge Case Testing
# ============================================================
# Evaluates coverage of edge cases, boundary conditions, and
# error scenarios in unit tests.
# ============================================================

audit:
  id: "testing-quality-assurance.unit-testing.edge-case-testing"
  name: "Edge Case Testing"
  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 10
  subcategory: "unit-testing"

  tier: "expert"
  estimated_duration: "2 hours"

  completeness: "complete"
  requires_runtime: false
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "testing"

  default_profiles:
    - "full"
    - "quality"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Assesses whether unit tests cover edge cases including boundary
    conditions, null/undefined inputs, empty collections, maximum/minimum
    values, error conditions, concurrent access, and unusual but valid
    input combinations.

  why_it_matters: |
    Edge cases are where bugs hide. Happy-path-only testing misses the
    inputs that cause crashes, security vulnerabilities, and data corruption.
    Thorough edge case testing catches issues before they reach production
    and affect users with unusual but valid usage patterns.

  when_to_run:
    - "Test quality assessment"
    - "Security review"
    - "Before releases"
    - "After bug fixes (to verify root cause coverage)"

prerequisites:
  required_artifacts:
    - type: "test_suite"
      description: "Unit test files to analyze"
    - type: "source_code"
      description: "Source code to understand edge case scenarios"

  access_requirements:
    - "Source code repository access"

discovery:
  file_patterns:
    - glob: "**/*.test.{js,ts,jsx,tsx}"
      purpose: "JavaScript/TypeScript unit tests"
    - glob: "**/*.spec.{js,ts,jsx,tsx}"
      purpose: "Spec-style unit tests"
    - glob: "**/test_*.py"
      purpose: "Python unit tests"

  code_patterns:
    - pattern: "null|undefined|nil|None"
      type: "keyword"
      scope: "source"
      purpose: "Identify null handling tests"
    - pattern: "empty|\\[\\]|\\{\\}|''"
      type: "regex"
      scope: "source"
      purpose: "Identify empty value tests"
    - pattern: "throw|reject|error|exception"
      type: "keyword"
      scope: "source"
      purpose: "Identify error handling tests"
    - pattern: "MAX_|MIN_|LIMIT|boundary"
      type: "regex"
      scope: "source"
      purpose: "Identify boundary tests"

knowledge_sources:
  specifications:
    - id: "xunit-patterns"
      name: "xUnit Test Patterns"
      url: "http://xunitpatterns.com/"
      offline_cache: true
      priority: "recommended"

  guides:
    - id: "boundary-testing"
      name: "Boundary Value Analysis"
      url: "https://www.guru99.com/equivalence-partitioning-boundary-value-analysis.html"
      offline_cache: true
    - id: "tdd-by-example"
      name: "Kent Beck - Test Driven Development"
      url: "https://www.oreilly.com/library/view/test-driven-development/0321146530/"
      offline_cache: true

  learning_resources:
    - id: "testing-book"
      title: "Software Testing: A Craftsman's Approach"
      type: "book"
      reference: "ISBN 978-1466560680"

tooling:
  static_analysis:
    - tool: "property-based-testing"
      purpose: "Generate edge cases automatically"
      offline_capable: true
    - tool: "mutation-testing"
      purpose: "Verify edge cases are covered"
      offline_capable: true

  scripts:
    - id: "edge-case-analyzer"
      language: "bash"
      purpose: "Analyze edge case coverage"
      source: "inline"
      code: |
        #!/bin/bash
        echo "=== Edge Case Analysis ==="
        echo "Null/undefined tests:"
        grep -r -c 'null\|undefined\|nil' --include='*.test.*' . 2>/dev/null | awk -F: '$2 > 0 {sum += $2} END {print sum}'
        echo ""
        echo "Empty value tests:"
        grep -r -c "\\[\\]\|''\|\\\"\\\"" --include='*.test.*' . 2>/dev/null | awk -F: '$2 > 0 {sum += $2} END {print sum}'
        echo ""
        echo "Error/throw tests:"
        grep -r -c 'toThrow\|rejects\|error\|Error' --include='*.test.*' . 2>/dev/null | awk -F: '$2 > 0 {sum += $2} END {print sum}'
        echo ""
        echo "Boundary tests (MAX/MIN/LIMIT):"
        grep -r -c 'MAX_\|MIN_\|LIMIT\|boundary' --include='*.test.*' . 2>/dev/null | awk -F: '$2 > 0 {sum += $2} END {print sum}'

signals:
  critical:
    - id: "EDGE-CRIT-001"
      signal: "No null/undefined input tests"
      evidence_pattern: "No tests for null inputs in functions accepting optional params"
      explanation: |
        Null pointer exceptions are among the most common runtime errors.
        Not testing null inputs means crashes in production.
      remediation: "Add tests for null/undefined inputs to all public functions"

    - id: "EDGE-CRIT-002"
      signal: "Error paths not tested"
      evidence_pattern: "No toThrow assertions for functions that can fail"
      explanation: |
        Untested error paths lead to unhandled exceptions and poor
        error messages in production.
      remediation: "Add error scenario tests for all failure modes"

  high:
    - id: "EDGE-HIGH-001"
      signal: "Boundary conditions not tested"
      evidence_pattern: "Functions with limits lack boundary tests"
      explanation: |
        Off-by-one errors and boundary condition bugs are common.
        Testing at boundaries catches these before production.
      remediation: "Add tests for minimum, maximum, and boundary values"

    - id: "EDGE-HIGH-002"
      signal: "Empty collection handling not tested"
      evidence_pattern: "Array/object functions lack empty input tests"
      explanation: |
        Empty collections often cause unexpected behavior or errors.
        Every function handling collections should be tested with empty input.
      remediation: "Add empty array/object tests for all collection functions"

  medium:
    - id: "EDGE-MED-001"
      signal: "Type coercion edge cases not tested"
      evidence_pattern: "No tests for string/number boundary inputs"
      remediation: "Add tests for type edge cases (0, '', false)"

    - id: "EDGE-MED-002"
      signal: "Unicode and special character handling not tested"
      evidence_pattern: "String functions lack unicode tests"
      remediation: "Add tests for unicode, emoji, and special characters"

  low:
    - id: "EDGE-LOW-001"
      signal: "Timezone edge cases not tested"
      evidence_pattern: "Date functions lack timezone tests"

  positive:
    - id: "EDGE-POS-001"
      signal: "Comprehensive edge case coverage"
      evidence_pattern: "Null, empty, boundary, and error tests present"
    - id: "EDGE-POS-002"
      signal: "Property-based testing in use"
      evidence_pattern: "fast-check, hypothesis, or similar in use"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Analyze null/undefined testing"
      description: |
        Check whether functions that accept optional or nullable
        parameters are tested with null/undefined inputs.
      duration_estimate: "20 min"
      commands:
        - purpose: "Find null tests"
          command: "grep -r -c 'null\\|undefined' --include='*.test.*' . 2>/dev/null | grep -v node_modules | head -20"
        - purpose: "Sample null test patterns"
          command: "grep -r -B2 -A5 'null\\|undefined' --include='*.test.*' . 2>/dev/null | head -40"
      expected_findings:
        - "Null testing coverage"
        - "Nullable parameter handling"

    - id: "2"
      name: "Check boundary testing"
      description: |
        Identify functions with numerical boundaries and verify
        boundary conditions are tested.
      duration_estimate: "25 min"
      commands:
        - purpose: "Find boundary-related tests"
          command: "grep -r 'MAX\\|MIN\\|LIMIT\\|boundary\\|off.by' --include='*.test.*' . 2>/dev/null | head -20"
        - purpose: "Find number limit tests"
          command: "grep -r 'Number.MAX\\|Number.MIN\\|Infinity' --include='*.test.*' . 2>/dev/null | head -10"
      expected_findings:
        - "Boundary test coverage"
        - "Numeric limit handling"

    - id: "3"
      name: "Evaluate error testing"
      description: |
        Check whether error conditions are tested with specific
        error types and messages.
      duration_estimate: "25 min"
      commands:
        - purpose: "Find error tests"
          command: "grep -r 'toThrow\\|rejects\\|catch' --include='*.test.*' . 2>/dev/null | head -30"
        - purpose: "Check error specificity"
          command: "grep -r 'toThrow(' --include='*.test.*' . 2>/dev/null | grep -v 'toThrow()' | head -15"
      expected_findings:
        - "Error testing coverage"
        - "Error assertion specificity"

    - id: "4"
      name: "Check empty collection testing"
      description: |
        Verify that functions handling collections are tested
        with empty arrays and objects.
      duration_estimate: "15 min"
      commands:
        - purpose: "Find empty collection tests"
          command: "grep -r -E '\\[\\]|\\{\\}|empty' --include='*.test.*' . 2>/dev/null | head -20"
      expected_findings:
        - "Empty collection test coverage"
        - "Edge case patterns"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"

    - type: "edge_case_report"
      format: "table"
      sections:
        - "Null Testing Analysis"
        - "Boundary Testing Analysis"
        - "Error Testing Analysis"
        - "Empty Collection Testing"

    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Edge Case Coverage Assessment"
        - "Gap Analysis"
        - "Recommendations"

  confidence_guidance:
    high: "Comprehensive source and test analysis"
    medium: "Pattern-based analysis"
    low: "Based on grep patterns only"

offline:
  capability: "full"

  cache_manifest:
    knowledge:
      - source_id: "boundary-testing"
        priority: "required"

profiles:
  membership:
    quick:
      included: false
      reason: "Edge case analysis requires thorough review"
    full:
      included: true
      priority: 1

closeout_checklist:
  - id: "edge-001"
    item: "Null/undefined testing assessed"
    level: "CRITICAL"
    verification: "grep -r 'null\\|undefined' --include='*.test.*' . 2>/dev/null | wc -l"
    expected: "PASS (count documented)"

  - id: "edge-002"
    item: "Boundary testing assessed"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Document boundary test coverage"
    expected: "Confirmed by reviewer"

  - id: "edge-003"
    item: "Error testing assessed"
    level: "BLOCKING"
    verification: "grep -r 'toThrow' --include='*.test.*' . 2>/dev/null | wc -l"
    expected: "PASS (count documented)"

governance:
  applicable_to:
    archetypes: ["all"]

  compliance_frameworks:
    - framework: "ISO 25010"
      controls: ["Reliability", "Fault Tolerance"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.unit-testing.unit-test-coverage"
    - "testing-quality-assurance.unit-testing.assertion-quality"
    - "security-trust.input-validation.sql-injection"
