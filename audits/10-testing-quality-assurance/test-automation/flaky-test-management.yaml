# ============================================================
# AUDIT: Flaky Test Management
# ============================================================
# Evaluates practices for identifying, tracking, and resolving
# intermittently failing tests.
# ============================================================

audit:
  id: "testing-quality-assurance.test-automation.flaky-test-management"
  name: "Flaky Test Management"
  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 10
  subcategory: "test-automation"

  tier: "expert"
  estimated_duration: "3 hours"

  completeness: "complete"
  requires_runtime: true
  destructive: false

execution:
  automatable: "partial"
  severity: "high"
  scope: "testing"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Examines flaky test management practices including detection mechanisms,
    tracking systems, quarantine strategies, and remediation processes.
    Reviews how intermittently failing tests are identified, isolated, and
    fixed to maintain test suite reliability.

  why_it_matters: |
    Flaky tests erode trust in the test suite. When tests fail randomly,
    teams learn to ignore failures, real bugs slip through, and debugging
    becomes a guessing game. Effective flaky test management preserves
    the test suite as a reliable quality gate.

  when_to_run:
    - "When test failures increase"
    - "After parallelization changes"
    - "During test suite health audits"
    - "When CI pipeline reliability drops"

prerequisites:
  required_artifacts:
    - type: "test-results-history"
      description: "Historical test execution results"
    - type: "flaky-test-tracking"
      description: "Flaky test tracking system or database"

  access_requirements:
    - "Test result history access"
    - "CI/CD execution logs"
    - "Test analytics dashboards"

discovery:
  file_patterns:
    - glob: "**/*.flaky.test.*"
      purpose: "Flaky test markers"
    - glob: "**/quarantine/**"
      purpose: "Quarantined tests"
    - glob: "**/.flaky*"
      purpose: "Flaky test configuration"

  code_patterns:
    - pattern: "flaky|retry|unstable|quarantine"
      type: "keyword"
      scope: "source"
      purpose: "Flaky test markers"
    - pattern: "@flaky|@retry|skip.*flaky"
      type: "regex"
      scope: "source"
      purpose: "Flaky test decorators"
    - pattern: "rerun|retries|maxRetries"
      type: "keyword"
      scope: "config"
      purpose: "Retry configuration"

knowledge_sources:
  guides:
    - id: "google-flaky"
      name: "Google Testing Blog - Flaky Tests"
      url: "https://testing.googleblog.com/2020/12/test-flakiness-one-of-main-challenges.html"
      offline_cache: true

  papers:
    - id: "flaky-research"
      title: "An Empirical Analysis of Flaky Tests"
      url: "https://mir.cs.illinois.edu/marinov/publications/LuoETAL14FlakyTestsAnalysis.pdf"

tooling:
  static_analysis:
    - tool: "pytest-rerunfailures"
      purpose: "Automatic retry for flaky tests"
      offline_capable: true
    - tool: "jest-circus"
      purpose: "Jest retry handling"
      offline_capable: true
    - tool: "flaky-test-detector"
      purpose: "Statistical flaky test detection"
      offline_capable: true

  monitoring_queries:
    - system: "TestAnalytics"
      query: "SELECT test_name, COUNT(*) as runs, SUM(failed) as failures FROM test_results WHERE timestamp > now() - 30d GROUP BY test_name HAVING failures > 0 AND failures < runs"
      purpose: "Identify intermittently failing tests"

signals:
  critical:
    - id: "FLAKY-CRIT-001"
      signal: "Flaky tests blocking CI without investigation"
      evidence_pattern: "Repeated CI failures on same tests without fixes"
      explanation: |
        Flaky tests that repeatedly block CI without investigation waste
        developer time and may mask real failures. Teams start ignoring
        all failures, destroying the test suite's value as a quality gate.
      remediation: "Investigate and fix or quarantine flaky tests immediately"

    - id: "FLAKY-CRIT-002"
      signal: "No flaky test detection mechanism"
      evidence_pattern: "No statistical analysis or tracking of test reliability"
      explanation: |
        Without detection, flaky tests accumulate unnoticed. Teams don't
        know which tests are unreliable, making debugging failed builds
        a frustrating guessing game.
      remediation: "Implement statistical flaky test detection"

  high:
    - id: "FLAKY-HIGH-001"
      signal: "Retry masking flaky tests without tracking"
      evidence_pattern: "Automatic retries without logging or alerting"
      explanation: |
        Retries that silently pass hide underlying problems. Flaky tests
        indicate real issues like race conditions or resource leaks that
        may affect production. Silent retries accumulate technical debt.
      remediation: "Track retried tests and alert on flakiness patterns"

    - id: "FLAKY-HIGH-002"
      signal: "High flaky test count (>5% of suite)"
      evidence_pattern: "More than 5% of tests show intermittent failures"
      explanation: |
        High flakiness rates indicate systemic issues with test isolation,
        infrastructure, or code quality. The test suite becomes unreliable
        for regression detection.
      remediation: "Prioritize flaky test reduction as technical debt"

  medium:
    - id: "FLAKY-MED-001"
      signal: "No quarantine process for flaky tests"
      evidence_pattern: "Flaky tests remain in main suite without isolation"
      remediation: "Implement quarantine process to isolate flaky tests while fixing"

    - id: "FLAKY-MED-002"
      signal: "Flaky test root causes not documented"
      evidence_pattern: "Fixes applied without documenting underlying issues"
      remediation: "Document root causes to prevent similar flakiness patterns"

  low:
    - id: "FLAKY-LOW-001"
      signal: "Flaky test metrics not tracked over time"
      remediation: "Track flaky test trends for quality monitoring"

  positive:
    - id: "FLAKY-POS-001"
      signal: "Proactive flaky test detection with tracking"
    - id: "FLAKY-POS-002"
      signal: "Low flakiness rate with effective remediation process"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Identify flaky test markers"
      description: |
        Search for flaky test annotations, quarantine configurations,
        and retry settings.
      duration_estimate: "25 min"
      commands:
        - purpose: "Find flaky test markers"
          command: "grep -r '@flaky\\|flaky\\|unstable\\|quarantine' . --include='*.py' --include='*.js' --include='*.ts' 2>/dev/null | head -30"
        - purpose: "Check retry configuration"
          command: "grep -r 'retry\\|rerun\\|maxRetries' . --include='*.yaml' --include='jest*.js' --include='pytest.ini' 2>/dev/null"
      expected_findings:
        - "Flaky test markers"
        - "Retry configurations"

    - id: "2"
      name: "Review flaky test tracking"
      description: |
        Examine tracking mechanisms for identifying and monitoring
        flaky tests.
      duration_estimate: "35 min"
      commands:
        - purpose: "Check for flaky test tracking"
          command: "find . -type f \\( -name '*flaky*' -o -name '*quarantine*' \\) 2>/dev/null"
        - purpose: "Check CI for flaky handling"
          command: "grep -r 'flaky\\|retry\\|rerun' .github/workflows/ 2>/dev/null"
      expected_findings:
        - "Tracking system configuration"
        - "CI flaky handling"

    - id: "3"
      name: "Assess flakiness rates"
      description: |
        Review test result history to estimate the percentage of
        flaky tests in the suite.
      duration_estimate: "30 min"
      questions:
        - "What percentage of the test suite is marked as flaky?"
        - "How often do retries succeed on first failure?"
        - "What are the common root causes of flakiness?"
      expected_findings:
        - "Flakiness rate estimate"
        - "Common failure patterns"

    - id: "4"
      name: "Review remediation process"
      description: |
        Verify that a process exists for investigating and fixing
        flaky tests.
      duration_estimate: "20 min"
      questions:
        - "Is there a documented process for handling flaky tests?"
        - "Are flaky tests triaged and assigned for fix?"
        - "Is there a time limit for quarantined tests?"
      expected_findings:
        - "Remediation process documentation"
        - "SLA for flaky test resolution"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"
    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Flaky Test Detection Assessment"
        - "Tracking and Remediation Analysis"
        - "Recommendations"

  confidence_guidance:
    high: "Detection verified, tracking confirmed, remediation process documented"
    medium: "Some tracking visible but process unclear"
    low: "Limited visibility into flaky test management"

offline:
  capability: "full"
  cache_manifest:
    knowledge:
      - source_id: "google-flaky"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires historical analysis"
    full:
      included: true
      priority: 2

closeout_checklist:
  - id: "flaky-test-001"
    item: "Flaky test detection mechanism exists"
    level: "CRITICAL"
    verification: "grep -r 'flaky\\|retry\\|rerun' . --include='*.yaml' --include='*.ini' 2>/dev/null | grep -q . && echo PASS || echo FAIL"
    expected: "PASS"

  - id: "flaky-test-002"
    item: "Flaky tests tracked and monitored"
    level: "BLOCKING"
    verification: "manual"
    verification_notes: "Reviewer confirms flaky test tracking system exists"
    expected: "Confirmed by reviewer"

  - id: "flaky-test-003"
    item: "Flakiness rate below 5%"
    level: "WARNING"
    verification: "manual"
    verification_notes: "Reviewer confirms flaky test percentage is acceptable"
    expected: "Confirmed by reviewer"

governance:
  applicable_to:
    archetypes: ["all"]

  compliance_frameworks:
    - framework: "Internal"
      controls: ["QA-001"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.test-automation.test-parallelization"
    - "testing-quality-assurance.test-automation.ci-integration"
    - "testing-quality-assurance.test-automation.automated-test-maintenance"
