# ============================================================
# AUDIT: Test Reporting
# ============================================================
# Evaluates test result reporting practices for visibility,
# actionability, and stakeholder communication.
# ============================================================

audit:
  id: "testing-quality-assurance.test-automation.test-reporting"
  name: "Test Reporting"
  version: "1.0.0"
  last_updated: "2026-01-19"
  status: "active"

  category: "testing-quality-assurance"
  category_number: 10
  subcategory: "test-automation"

  tier: "expert"
  estimated_duration: "2 hours"

  completeness: "complete"
  requires_runtime: false
  destructive: false

execution:
  automatable: "yes"
  severity: "high"
  scope: "testing"

  default_profiles:
    - "full"

  blocks_phase: false
  parallelizable: true

description:
  what: |
    Examines test reporting practices including result aggregation,
    visualization, trend analysis, and stakeholder communication.
    Reviews report formats, delivery mechanisms, and actionability
    of test result information.

  why_it_matters: |
    Test results are only valuable if they're visible and actionable.
    Poor reporting means failures go unnoticed, trends aren't identified,
    and quality decisions lack data. Good reporting transforms raw test
    results into insights that drive quality improvement.

  when_to_run:
    - "During CI/CD setup"
    - "After reporting tool changes"
    - "When stakeholders request better visibility"
    - "Quality improvement initiatives"

prerequisites:
  required_artifacts:
    - type: "test-reports"
      description: "Test result reports and artifacts"
    - type: "ci-configuration"
      description: "CI/CD reporting configuration"

  access_requirements:
    - "Test reporting dashboards"
    - "CI/CD artifact storage"
    - "Notification system access"

discovery:
  file_patterns:
    - glob: "**/*report*/**"
      purpose: "Test report directories"
    - glob: "**/*junit*.xml"
      purpose: "JUnit format reports"
    - glob: "**/coverage/**"
      purpose: "Coverage reports"

  code_patterns:
    - pattern: "reporter|junit|html.*report|allure"
      type: "keyword"
      scope: "config"
      purpose: "Report configuration"
    - pattern: "upload.*artifact|store.*result|publish"
      type: "regex"
      scope: "config"
      purpose: "Report publishing"
    - pattern: "coverage|lcov|cobertura"
      type: "keyword"
      scope: "config"
      purpose: "Coverage reporting"

knowledge_sources:
  guides:
    - id: "allure-docs"
      name: "Allure Test Report Framework"
      url: "https://docs.qameta.io/allure/"
      offline_cache: true
    - id: "github-artifacts"
      name: "GitHub Actions Artifacts"
      url: "https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts"
      offline_cache: true

tooling:
  static_analysis:
    - tool: "Allure"
      purpose: "Test reporting framework"
      offline_capable: true
    - tool: "ReportPortal"
      purpose: "Test result aggregation"
      offline_capable: false
    - tool: "Codecov"
      purpose: "Coverage reporting"
      offline_capable: false

  scripts:
    - id: "generate-report"
      language: "bash"
      purpose: "Generate test reports"
      source: "inline"
      code: |
        #!/bin/bash
        # Generate comprehensive test report
        allure generate ./allure-results -o ./allure-report --clean
        echo "Report generated at ./allure-report/index.html"

signals:
  critical:
    - id: "REPORT-CRIT-001"
      signal: "No test result reporting configured"
      evidence_pattern: "Test execution without result storage or reporting"
      explanation: |
        Without reporting, test results disappear after execution. Failures
        aren't tracked, trends aren't visible, and historical data is lost.
        The test suite provides no lasting value beyond immediate feedback.
      remediation: "Configure test result storage and reporting in CI/CD"

    - id: "REPORT-CRIT-002"
      signal: "Test failures not notified to responsible parties"
      evidence_pattern: "No notification configuration for test failures"
      explanation: |
        Unreported failures go unaddressed. Responsible developers don't
        know their code broke tests, and issues accumulate until someone
        notices the degradation.
      remediation: "Configure failure notifications to code owners and teams"

  high:
    - id: "REPORT-HIGH-001"
      signal: "Reports not accessible to stakeholders"
      evidence_pattern: "Reports stored but not published or shared"
      explanation: |
        Reports that aren't easily accessible aren't used. Quality
        information remains siloed, preventing data-driven decisions
        and stakeholder confidence.
      remediation: "Publish reports to accessible dashboards and share links"

    - id: "REPORT-HIGH-002"
      signal: "No historical trend analysis"
      evidence_pattern: "Only latest results visible, no trending"
      explanation: |
        Without trends, gradual quality degradation goes unnoticed.
        Historical data reveals patterns like increasing test times or
        failure rates that single-run reports miss.
      remediation: "Implement historical result storage and trend visualization"

  medium:
    - id: "REPORT-MED-001"
      signal: "Coverage reports not generated"
      evidence_pattern: "Tests run without coverage collection"
      remediation: "Configure coverage collection and reporting"

    - id: "REPORT-MED-002"
      signal: "Reports lack actionable details"
      evidence_pattern: "Pass/fail only without failure details or logs"
      remediation: "Include stack traces, logs, and screenshots in reports"

  low:
    - id: "REPORT-LOW-001"
      signal: "Report format inconsistent across projects"
      remediation: "Standardize report format across projects"

  positive:
    - id: "REPORT-POS-001"
      signal: "Comprehensive reporting with historical trends"
    - id: "REPORT-POS-002"
      signal: "Automated notifications with actionable details"

procedure:
  context:
    cognitive_mode: "critical"
    ensemble_role: "auditor"

  steps:
    - id: "1"
      name: "Identify reporting configuration"
      description: |
        Search for test reporting configurations in test runners and
        CI/CD pipelines.
      duration_estimate: "20 min"
      commands:
        - purpose: "Find report configuration"
          command: "grep -r 'reporter\\|junit\\|allure\\|html-report' . --include='jest*.js' --include='pytest.ini' --include='*.yaml' 2>/dev/null | head -20"
        - purpose: "Check CI for reporting"
          command: "grep -r 'artifact\\|report\\|coverage' .github/workflows/ 2>/dev/null | head -20"
      expected_findings:
        - "Reporter configuration"
        - "CI artifact uploads"

    - id: "2"
      name: "Review report content"
      description: |
        Examine report configurations for completeness including
        failure details, logs, and screenshots.
      duration_estimate: "25 min"
      commands:
        - purpose: "Check for detailed reporting"
          command: "grep -r 'verbose\\|screenshot\\|trace\\|log' . --include='*.yaml' --include='jest*.js' 2>/dev/null"
      expected_findings:
        - "Detail level configuration"
        - "Artifact capture settings"

    - id: "3"
      name: "Verify notification configuration"
      description: |
        Check that test failures trigger notifications to appropriate
        parties.
      duration_estimate: "20 min"
      commands:
        - purpose: "Check for notifications"
          command: "grep -r 'slack\\|email\\|notify\\|webhook' .github/workflows/ 2>/dev/null"
      expected_findings:
        - "Notification configuration"
        - "Failure alerting"

    - id: "4"
      name: "Assess accessibility"
      description: |
        Verify that reports are published and accessible to
        stakeholders.
      duration_estimate: "15 min"
      commands:
        - purpose: "Check for publishing"
          command: "grep -r 'publish\\|deploy\\|pages\\|upload' .github/workflows/ 2>/dev/null"
      expected_findings:
        - "Report publishing configuration"
        - "Accessibility settings"

output:
  deliverables:
    - type: "finding_list"
      format: "structured"
    - type: "summary"
      format: "prose"
      sections:
        - "Executive Summary"
        - "Reporting Coverage Analysis"
        - "Accessibility Assessment"
        - "Recommendations"

  confidence_guidance:
    high: "Reporting verified, notifications confirmed, accessibility validated"
    medium: "Configuration reviewed but reports not inspected"
    low: "Limited visibility into reporting practices"

offline:
  capability: "full"
  cache_manifest:
    knowledge:
      - source_id: "allure-docs"
        priority: "recommended"

profiles:
  membership:
    quick:
      included: false
      reason: "Requires detailed reporting analysis"
    full:
      included: true
      priority: 3

closeout_checklist:
  - id: "test-reporting-001"
    item: "Test reporting configured"
    level: "CRITICAL"
    verification: "grep -r 'reporter\\|junit\\|allure' . --include='*.yaml' --include='jest*.js' 2>/dev/null | grep -q . && echo PASS || echo FAIL"
    expected: "PASS"

  - id: "test-reporting-002"
    item: "Results stored as artifacts"
    level: "BLOCKING"
    verification: "grep -r 'artifact\\|upload' .github/workflows/ 2>/dev/null | grep -q . && echo PASS || echo FAIL"
    expected: "PASS"

  - id: "test-reporting-003"
    item: "Failure notifications configured"
    level: "WARNING"
    verification: "grep -r 'notify\\|slack\\|email' .github/workflows/ 2>/dev/null | grep -q . && echo PASS || echo FAIL"
    expected: "PASS"

governance:
  applicable_to:
    archetypes: ["all"]

  compliance_frameworks:
    - framework: "Internal"
      controls: ["QA-002"]

relationships:
  commonly_combined:
    - "testing-quality-assurance.test-automation.ci-integration"
    - "testing-quality-assurance.test-automation.flaky-test-management"
